DANIAR
NOTE: SPARK_PREPEND_CLASSES is set, placing locally compiled Spark classes ahead of assembly.
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
16/08/14 07:54:42 INFO SparkContext: Running Spark version 1.6.1
16/08/14 07:54:43 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
16/08/14 07:54:43 INFO SecurityManager: Changing view acls to: daniar
16/08/14 07:54:43 INFO SecurityManager: Changing modify acls to: daniar
16/08/14 07:54:43 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(daniar); users with modify permissions: Set(daniar)
16/08/14 07:54:44 INFO Utils: Successfully started service 'sparkDriver' on port 47530.
16/08/14 07:54:44 INFO Slf4jLogger: Slf4jLogger started
16/08/14 07:54:44 INFO Remoting: Starting remoting
16/08/14 07:54:44 INFO Remoting: Remoting started; listening on addresses :[akka.tcp://sparkDriverActorSystem@155.98.38.95:50860]
16/08/14 07:54:44 INFO Utils: Successfully started service 'sparkDriverActorSystem' on port 50860.
16/08/14 07:54:44 INFO SparkEnv: Registering MapOutputTracker
16/08/14 07:54:45 INFO SparkEnv: Registering BlockManagerMaster
16/08/14 07:54:45 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-d810b145-3362-443b-870a-80522f9d119c
16/08/14 07:54:45 INFO MemoryStore: MemoryStore started with capacity 511.1 MB
16/08/14 07:54:45 INFO SparkEnv: Registering OutputCommitCoordinator
16/08/14 07:54:45 INFO Server: jetty-8.y.z-SNAPSHOT
16/08/14 07:54:45 INFO AbstractConnector: Started SelectChannelConnector@0.0.0.0:4040
16/08/14 07:54:45 INFO Utils: Successfully started service 'SparkUI' on port 4040.
16/08/14 07:54:45 INFO SparkUI: Started SparkUI at http://155.98.38.95:4040
16/08/14 07:54:45 INFO HttpFileServer: HTTP File server directory is /tmp/spark-5881480f-b42d-4c94-8d9f-adf1b6e75a72/httpd-b399b3b3-e104-4a34-adb7-4c58888dacd8
16/08/14 07:54:45 INFO HttpServer: Starting HTTP Server
16/08/14 07:54:45 INFO Server: jetty-8.y.z-SNAPSHOT
16/08/14 07:54:45 INFO AbstractConnector: Started SocketConnector@0.0.0.0:45196
16/08/14 07:54:45 INFO Utils: Successfully started service 'HTTP file server' on port 45196.
16/08/14 07:54:45 INFO Utils: Copying /proj/cs331-uc/daniar/SPARK/spark-1.6.1/sort_in_node.py to /tmp/spark-5881480f-b42d-4c94-8d9f-adf1b6e75a72/userFiles-4d8b0429-112f-4124-b1df-5c420d612661/sort_in_node.py
16/08/14 07:54:45 INFO SparkContext: Added file file:/proj/cs331-uc/daniar/SPARK/spark-1.6.1/sort_in_node.py at http://155.98.38.95:45196/files/sort_in_node.py with timestamp 1471182885707
16/08/14 07:54:45 INFO SparkDeploySchedulerBackend: DaniarrronStartCoarse
16/08/14 07:54:46 INFO AppClient$ClientEndpoint: Connecting to master spark://node-1.testspark.cs331-uc.emulab.net:7077...
16/08/14 07:54:46 INFO SparkDeploySchedulerBackend: Connected to Spark cluster with app ID app-20160814075446-0004
16/08/14 07:54:46 INFO TaskSchedulerImpl: Starting speculative execution thread
16/08/14 07:54:46 INFO AppClient$ClientEndpoint: Executor added: app-20160814075446-0004/0 on worker-20160814073210-155.98.38.111-53075 (155.98.38.111:53075) with 4 cores
16/08/14 07:54:46 INFO SparkDeploySchedulerBackend: Granted executor ID app-20160814075446-0004/0 on hostPort 155.98.38.111:53075 with 4 cores, 1024.0 MB RAM
16/08/14 07:54:46 INFO AppClient$ClientEndpoint: Executor added: app-20160814075446-0004/1 on worker-20160814073212-155.98.38.102-46872 (155.98.38.102:46872) with 4 cores
16/08/14 07:54:46 INFO SparkDeploySchedulerBackend: Granted executor ID app-20160814075446-0004/1 on hostPort 155.98.38.102:46872 with 4 cores, 1024.0 MB RAM
16/08/14 07:54:46 INFO AppClient$ClientEndpoint: Executor added: app-20160814075446-0004/2 on worker-20160814073211-155.98.38.93-41007 (155.98.38.93:41007) with 4 cores
16/08/14 07:54:46 INFO SparkDeploySchedulerBackend: Granted executor ID app-20160814075446-0004/2 on hostPort 155.98.38.93:41007 with 4 cores, 1024.0 MB RAM
16/08/14 07:54:46 INFO AppClient$ClientEndpoint: Executor added: app-20160814075446-0004/3 on worker-20160814073207-155.98.38.115-46343 (155.98.38.115:46343) with 4 cores
16/08/14 07:54:46 INFO SparkDeploySchedulerBackend: Granted executor ID app-20160814075446-0004/3 on hostPort 155.98.38.115:46343 with 4 cores, 1024.0 MB RAM
16/08/14 07:54:46 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 56470.
16/08/14 07:54:46 INFO NettyBlockTransferService: Server created on 56470
16/08/14 07:54:46 INFO BlockManagerMaster: Trying to register BlockManager
16/08/14 07:54:46 INFO BlockManagerMasterEndpoint: Registering block manager 155.98.38.95:56470 with 511.1 MB RAM, BlockManagerId(driver, 155.98.38.95, 56470)
16/08/14 07:54:46 INFO BlockManagerMaster: Registered BlockManager
16/08/14 07:54:46 INFO AppClient$ClientEndpoint: Executor updated: app-20160814075446-0004/2 is now RUNNING
16/08/14 07:54:46 INFO AppClient$ClientEndpoint: Executor updated: app-20160814075446-0004/3 is now RUNNING
16/08/14 07:54:46 INFO AppClient$ClientEndpoint: Executor updated: app-20160814075446-0004/1 is now RUNNING
16/08/14 07:54:46 INFO AppClient$ClientEndpoint: Executor updated: app-20160814075446-0004/0 is now RUNNING
16/08/14 07:54:46 INFO EventLoggingListener: Logging events to file:/proj/cs331-uc/daniar/SPARK/spark-1.6.1/../generated_driver_log/app-20160814075446-0004
16/08/14 07:54:46 INFO SparkDeploySchedulerBackend: DANIAR DO THE HACK in SparkDeploySchedulerBackend.scala
16/08/14 07:54:46 INFO SparkDeploySchedulerBackend: DANIAR DO THE HACK in SparkDeploySchedulerBackend.scala
16/08/14 07:54:46 INFO SparkDeploySchedulerBackend: DANIAR DO THE HACK in SparkDeploySchedulerBackend.scala
16/08/14 07:54:47 INFO SparkDeploySchedulerBackend: DANIAR DO THE HACK in SparkDeploySchedulerBackend.scala
16/08/14 07:54:47 INFO SparkDeploySchedulerBackend: DANIAR DO THE HACK in SparkDeploySchedulerBackend.scala
16/08/14 07:54:47 INFO SparkDeploySchedulerBackend: DANIAR DO THE HACK in SparkDeploySchedulerBackend.scala
16/08/14 07:54:47 INFO SparkDeploySchedulerBackend: DANIAR DO THE HACK in SparkDeploySchedulerBackend.scala
16/08/14 07:54:47 INFO SparkDeploySchedulerBackend: DANIAR DO THE HACK in SparkDeploySchedulerBackend.scala
16/08/14 07:54:47 INFO SparkDeploySchedulerBackend: DANIAR DO THE HACK in SparkDeploySchedulerBackend.scala
16/08/14 07:54:47 INFO SparkDeploySchedulerBackend: DANIAR DO THE HACK in SparkDeploySchedulerBackend.scala
16/08/14 07:54:47 INFO SparkDeploySchedulerBackend: DANIAR DO THE HACK in SparkDeploySchedulerBackend.scala
16/08/14 07:54:47 INFO SparkDeploySchedulerBackend: DANIAR DO THE HACK in SparkDeploySchedulerBackend.scala
16/08/14 07:54:47 INFO SparkDeploySchedulerBackend: DANIAR DO THE HACK in SparkDeploySchedulerBackend.scala
16/08/14 07:54:48 INFO SparkDeploySchedulerBackend: DANIAR DO THE HACK in SparkDeploySchedulerBackend.scala
16/08/14 07:54:48 INFO SparkDeploySchedulerBackend: DANIAR DO THE HACK in SparkDeploySchedulerBackend.scala
16/08/14 07:54:48 INFO SparkDeploySchedulerBackend: DANIAR DO THE HACK in SparkDeploySchedulerBackend.scala
16/08/14 07:54:48 INFO SparkDeploySchedulerBackend: DANIAR DO THE HACK in SparkDeploySchedulerBackend.scala
16/08/14 07:54:48 INFO SparkDeploySchedulerBackend: DANIAR DO THE HACK in SparkDeploySchedulerBackend.scala
16/08/14 07:54:48 INFO SparkDeploySchedulerBackend: DANIAR DO THE HACK in SparkDeploySchedulerBackend.scala
16/08/14 07:54:48 INFO SparkDeploySchedulerBackend: DANIAR DO THE HACK in SparkDeploySchedulerBackend.scala
16/08/14 07:54:48 INFO SparkDeploySchedulerBackend: DANIAR DO THE HACK in SparkDeploySchedulerBackend.scala
16/08/14 07:54:48 INFO SparkDeploySchedulerBackend: DANIAR DO THE HACK in SparkDeploySchedulerBackend.scala
16/08/14 07:54:48 INFO SparkDeploySchedulerBackend:  CoarseGrainedSchedulerBackendScala: Daniarrrr --- RegisterExecutor null
16/08/14 07:54:48 INFO SparkDeploySchedulerBackend: CoarseGrainedSchedulerBackend: martin -- execAdr is null pc511.emulab.net:52110
16/08/14 07:54:48 INFO SparkDeploySchedulerBackend: Registered executor NettyRpcEndpointRef(null) (pc511.emulab.net:52110) with ID 0
16/08/14 07:54:48 INFO SparkDeploySchedulerBackend: DANIAR DO THE HACK in SparkDeploySchedulerBackend.scala
16/08/14 07:54:48 INFO SparkDeploySchedulerBackend:  CoarseGrainedSchedulerBackendScala: Daniarrrr --- RegisterExecutor null
16/08/14 07:54:48 INFO SparkDeploySchedulerBackend: CoarseGrainedSchedulerBackend: martin -- execAdr is null pc515.emulab.net:36890
16/08/14 07:54:48 INFO SparkDeploySchedulerBackend: Registered executor NettyRpcEndpointRef(null) (pc515.emulab.net:36890) with ID 3
16/08/14 07:54:48 INFO SparkDeploySchedulerBackend:  CoarseGrainedSchedulerBackendScala: Daniarrrr --- RegisterExecutor null
16/08/14 07:54:48 INFO SparkDeploySchedulerBackend: CoarseGrainedSchedulerBackend: martin -- execAdr is null pc502.emulab.net:40410
16/08/14 07:54:48 INFO SparkDeploySchedulerBackend: Registered executor NettyRpcEndpointRef(null) (pc502.emulab.net:40410) with ID 1
16/08/14 07:54:48 INFO BlockManagerMasterEndpoint: Registering block manager pc511.emulab.net:34605 with 511.1 MB RAM, BlockManagerId(0, pc511.emulab.net, 34605)
16/08/14 07:54:48 INFO BlockManagerMasterEndpoint: Registering block manager pc515.emulab.net:35947 with 511.1 MB RAM, BlockManagerId(3, pc515.emulab.net, 35947)
16/08/14 07:54:49 INFO SparkDeploySchedulerBackend: DANIAR DO THE HACK in SparkDeploySchedulerBackend.scala
16/08/14 07:54:49 INFO BlockManagerMasterEndpoint: Registering block manager pc502.emulab.net:39143 with 511.1 MB RAM, BlockManagerId(1, pc502.emulab.net, 39143)
16/08/14 07:54:49 INFO SparkDeploySchedulerBackend: DANIAR DO THE HACK in SparkDeploySchedulerBackend.scala
16/08/14 07:54:49 INFO SparkDeploySchedulerBackend: DANIAR DO THE HACK in SparkDeploySchedulerBackend.scala
16/08/14 07:54:49 INFO SparkDeploySchedulerBackend: DANIAR DO THE HACK in SparkDeploySchedulerBackend.scala
16/08/14 07:54:49 INFO SparkDeploySchedulerBackend: DANIAR DO THE HACK in SparkDeploySchedulerBackend.scala
16/08/14 07:54:49 INFO SparkDeploySchedulerBackend: DANIAR DO THE HACK in SparkDeploySchedulerBackend.scala
16/08/14 07:54:49 INFO SparkDeploySchedulerBackend: DANIAR DO THE HACK in SparkDeploySchedulerBackend.scala
16/08/14 07:54:49 INFO SparkDeploySchedulerBackend: DANIAR DO THE HACK in SparkDeploySchedulerBackend.scala
16/08/14 07:54:49 INFO SparkDeploySchedulerBackend: DANIAR DO THE HACK in SparkDeploySchedulerBackend.scala
16/08/14 07:54:49 INFO SparkDeploySchedulerBackend: DANIAR DO THE HACK in SparkDeploySchedulerBackend.scala
16/08/14 07:54:50 INFO SparkDeploySchedulerBackend: DANIAR DO THE HACK in SparkDeploySchedulerBackend.scala
16/08/14 07:54:50 INFO SparkDeploySchedulerBackend:  CoarseGrainedSchedulerBackendScala: Daniarrrr --- RegisterExecutor null
16/08/14 07:54:50 INFO SparkDeploySchedulerBackend: CoarseGrainedSchedulerBackend: martin -- execAdr is null pc493.emulab.net:40269
16/08/14 07:54:50 INFO SparkDeploySchedulerBackend: Registered executor NettyRpcEndpointRef(null) (pc493.emulab.net:40269) with ID 2
16/08/14 07:54:50 INFO SparkDeploySchedulerBackend: DANIAR DO THE HACK in SparkDeploySchedulerBackend.scala
16/08/14 07:54:50 INFO SparkDeploySchedulerBackend: SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.0
16/08/14 07:54:50 INFO BlockManagerMasterEndpoint: Registering block manager pc493.emulab.net:43443 with 511.1 MB RAM, BlockManagerId(2, pc493.emulab.net, 43443)
16/08/14 07:54:50 INFO MemoryStore: DANIAR HERE AT MEMORY STORE putIteratornon-empty iterator
16/08/14 07:54:50 INFO MemoryStore: DANIAR HERE AT MEMORY STORE currentTaskAttemptId
16/08/14 07:54:50 INFO MemoryStore: DANIAR HERE AT MEMORY STORE currentTaskAttemptId
16/08/14 07:54:50 INFO MemoryStore: DANIAR HERE AT MEMORY STORE putArray [Ljava.lang.Object;@20f2e91d  blockID broadcast_0
16/08/14 07:54:50 INFO MemoryStore: DANIAR HERE AT MEMORY STORE currentTaskAttemptId
16/08/14 07:54:50 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 232.0 KB, free 232.0 KB)
16/08/14 07:54:50 INFO MemoryStore: DANIAR HERE AT MEMORY STORE currentTaskAttemptId
16/08/14 07:54:50 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 21.8 KB, free 253.8 KB)
16/08/14 07:54:50 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 155.98.38.95:56470 (size: 21.8 KB, free: 511.1 MB)
16/08/14 07:54:50 INFO SparkContext: Created broadcast 0 from textFile at NativeMethodAccessorImpl.java:-2
16/08/14 07:54:50 INFO MemoryStore: DANIAR HERE AT MEMORY STORE getValues
16/08/14 07:54:50 INFO MemoryStore: HEUUU MemoryEntry([Ljava.lang.Object;@20f2e91d,237544,true)
16/08/14 07:54:50 INFO FileInputFormat: Total input paths to process : 1
16/08/14 07:54:51 INFO SparkContext: Starting job: sortByKey at /proj/cs331-uc/daniar/SPARK/spark-1.6.1/sort_in_node.py:12
16/08/14 07:54:51 INFO DAGScheduler: Got job 0 (sortByKey at /proj/cs331-uc/daniar/SPARK/spark-1.6.1/sort_in_node.py:12) with 2 output partitions
16/08/14 07:54:51 INFO DAGScheduler: Final stage: ResultStage 0 (sortByKey at /proj/cs331-uc/daniar/SPARK/spark-1.6.1/sort_in_node.py:12)
16/08/14 07:54:51 INFO DAGScheduler: Parents of final stage: List()
16/08/14 07:54:51 INFO DAGScheduler: Missing parents: List()
16/08/14 07:54:51 INFO DAGScheduler: Submitting ResultStage 0 (PythonRDD[2] at sortByKey at /proj/cs331-uc/daniar/SPARK/spark-1.6.1/sort_in_node.py:12), which has no missing parents
16/08/14 07:54:51 INFO MemoryStore: DANIAR HERE AT MEMORY STORE putIteratornon-empty iterator
16/08/14 07:54:51 INFO MemoryStore: DANIAR HERE AT MEMORY STORE currentTaskAttemptId
16/08/14 07:54:51 INFO MemoryStore: DANIAR HERE AT MEMORY STORE currentTaskAttemptId
16/08/14 07:54:51 INFO MemoryStore: DANIAR HERE AT MEMORY STORE putArray [Ljava.lang.Object;@405a63f2  blockID broadcast_1
16/08/14 07:54:51 INFO MemoryStore: DANIAR HERE AT MEMORY STORE currentTaskAttemptId
16/08/14 07:54:51 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 6.1 KB, free 259.9 KB)
16/08/14 07:54:51 INFO MemoryStore: DANIAR HERE AT MEMORY STORE currentTaskAttemptId
16/08/14 07:54:51 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 3.7 KB, free 263.6 KB)
16/08/14 07:54:51 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 155.98.38.95:56470 (size: 3.7 KB, free: 511.1 MB)
16/08/14 07:54:51 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1007
16/08/14 07:54:51 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 0 (PythonRDD[2] at sortByKey at /proj/cs331-uc/daniar/SPARK/spark-1.6.1/sort_in_node.py:12)
16/08/14 07:54:51 INFO TaskSchedulerImpl: Adding task set 0.0 with 2 tasks
16/08/14 07:54:51 INFO TaskSetManager: SAPIIIIIIIIIIIII  TaskSet 0.0
16/08/14 07:54:51 INFO TaskSchedulerImpl: DANIAR: FINISH SUBMIT TASK SET
16/08/14 07:54:51 INFO TaskSetManager: Daniar: START Important! Starting task 0.0 in stage 0.0 (TID 0, pc511.emulab.net, partition 0,PROCESS_LOCAL, 2215 bytes)
16/08/14 07:54:51 INFO DAGScheduler: DANIAR DAGScheduler.scala  org.apache.spark.scheduler.TaskInfo@42908f20    ResultTask(0, 0)
16/08/14 07:54:51 INFO TaskSetManager: Daniar: END Important! Starting task 0.0 in stage 0.0 
16/08/14 07:54:51 INFO TaskSchedulerImpl: DANIAR: TASK LAUNCHED taskSet.name [stage] = TaskSet_0  daniar_counter = 0
16/08/14 07:54:51 INFO TaskSetManager: Daniar: START Important! Starting task 1.0 in stage 0.0 (TID 1, pc502.emulab.net, partition 1,PROCESS_LOCAL, 2215 bytes)
16/08/14 07:54:51 INFO DAGScheduler: DANIAR DAGScheduler.scala  org.apache.spark.scheduler.TaskInfo@163c2c4a    ResultTask(0, 1)
16/08/14 07:54:51 INFO TaskSetManager: Daniar: END Important! Starting task 1.0 in stage 0.0 
16/08/14 07:54:51 INFO TaskSchedulerImpl: DANIAR: TASK LAUNCHED taskSet.name [stage] = TaskSet_0  daniar_counter = 1
16/08/14 07:54:51 INFO SparkDeploySchedulerBackend: Daniarrr:  Before assigning any tasks it should be no random assignment [should be printed once]
16/08/14 07:54:51 INFO SparkDeploySchedulerBackend: Daniarrr:  Before assigning any tasks it should be no random assignment [should be printed once]
16/08/14 07:54:51 INFO MemoryStore: DANIAR HERE AT MEMORY STORE getBytes MemoryEntry(java.nio.HeapByteBuffer[pos=0 lim=3800 cap=3800],3800,false)
16/08/14 07:54:51 INFO MemoryStore: DANIAR HERE AT MEMORY STORE getBytes MemoryEntry(java.nio.HeapByteBuffer[pos=0 lim=3800 cap=3800],3800,false)
16/08/14 07:54:51 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on pc502.emulab.net:39143 (size: 3.7 KB, free: 511.1 MB)
16/08/14 07:54:51 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on pc511.emulab.net:34605 (size: 3.7 KB, free: 511.1 MB)
16/08/14 07:54:52 INFO MemoryStore: DANIAR HERE AT MEMORY STORE getBytes MemoryEntry(java.nio.HeapByteBuffer[pos=0 lim=22319 cap=22319],22319,false)
16/08/14 07:54:52 INFO MemoryStore: DANIAR HERE AT MEMORY STORE getBytes MemoryEntry(java.nio.HeapByteBuffer[pos=0 lim=22319 cap=22319],22319,false)
16/08/14 07:54:52 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on pc511.emulab.net:34605 (size: 21.8 KB, free: 511.1 MB)
16/08/14 07:54:52 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on pc502.emulab.net:39143 (size: 21.8 KB, free: 511.1 MB)
16/08/14 07:55:03 INFO SparkDeploySchedulerBackend: daniarrr -- makeOffers0
16/08/14 07:55:03 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 12009 ms on pc511.emulab.net (1/2)
16/08/14 07:55:03 INFO SparkDeploySchedulerBackend: daniarrr -- makeOffers1
16/08/14 07:55:03 INFO TaskSetManager: Finished task 1.0 in stage 0.0 (TID 1) in 12413 ms on pc502.emulab.net (2/2)
16/08/14 07:55:03 INFO DAGScheduler: ResultStage 0 (sortByKey at /proj/cs331-uc/daniar/SPARK/spark-1.6.1/sort_in_node.py:12) finished in 12.473 s
16/08/14 07:55:03 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 
16/08/14 07:55:03 INFO DAGScheduler: Job 0 finished: sortByKey at /proj/cs331-uc/daniar/SPARK/spark-1.6.1/sort_in_node.py:12, took 12.661053 s
16/08/14 07:55:03 INFO SparkContext: Starting job: sortByKey at /proj/cs331-uc/daniar/SPARK/spark-1.6.1/sort_in_node.py:12
16/08/14 07:55:03 INFO DAGScheduler: Got job 1 (sortByKey at /proj/cs331-uc/daniar/SPARK/spark-1.6.1/sort_in_node.py:12) with 2 output partitions
16/08/14 07:55:03 INFO DAGScheduler: Final stage: ResultStage 1 (sortByKey at /proj/cs331-uc/daniar/SPARK/spark-1.6.1/sort_in_node.py:12)
16/08/14 07:55:03 INFO DAGScheduler: Parents of final stage: List()
16/08/14 07:55:03 INFO DAGScheduler: Missing parents: List()
16/08/14 07:55:03 INFO DAGScheduler: Submitting ResultStage 1 (PythonRDD[3] at sortByKey at /proj/cs331-uc/daniar/SPARK/spark-1.6.1/sort_in_node.py:12), which has no missing parents
16/08/14 07:55:03 INFO MemoryStore: DANIAR HERE AT MEMORY STORE putIteratornon-empty iterator
16/08/14 07:55:03 INFO MemoryStore: DANIAR HERE AT MEMORY STORE currentTaskAttemptId
16/08/14 07:55:03 INFO MemoryStore: DANIAR HERE AT MEMORY STORE currentTaskAttemptId
16/08/14 07:55:03 INFO MemoryStore: DANIAR HERE AT MEMORY STORE putArray [Ljava.lang.Object;@1b4abed7  blockID broadcast_2
16/08/14 07:55:03 INFO MemoryStore: DANIAR HERE AT MEMORY STORE currentTaskAttemptId
16/08/14 07:55:03 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 5.9 KB, free 269.5 KB)
16/08/14 07:55:03 INFO MemoryStore: DANIAR HERE AT MEMORY STORE currentTaskAttemptId
16/08/14 07:55:03 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 3.7 KB, free 273.2 KB)
16/08/14 07:55:03 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 155.98.38.95:56470 (size: 3.7 KB, free: 511.1 MB)
16/08/14 07:55:03 INFO SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:1007
16/08/14 07:55:03 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 1 (PythonRDD[3] at sortByKey at /proj/cs331-uc/daniar/SPARK/spark-1.6.1/sort_in_node.py:12)
16/08/14 07:55:03 INFO TaskSchedulerImpl: Adding task set 1.0 with 2 tasks
16/08/14 07:55:03 INFO TaskSetManager: SAPIIIIIIIIIIIII  TaskSet 1.0
16/08/14 07:55:03 INFO TaskSchedulerImpl: DANIAR: FINISH SUBMIT TASK SET
16/08/14 07:55:03 INFO TaskSetManager: Daniar: START Important! Starting task 0.0 in stage 1.0 (TID 2, pc511.emulab.net, partition 0,PROCESS_LOCAL, 2215 bytes)
16/08/14 07:55:03 INFO DAGScheduler: DANIAR DAGScheduler.scala  org.apache.spark.scheduler.TaskInfo@60add4f9    ResultTask(1, 0)
16/08/14 07:55:03 INFO TaskSetManager: Daniar: END Important! Starting task 0.0 in stage 1.0 
16/08/14 07:55:03 INFO TaskSchedulerImpl: DANIAR: TASK LAUNCHED taskSet.name [stage] = TaskSet_1  daniar_counter = 2
16/08/14 07:55:03 INFO TaskSetManager: Daniar: START Important! Starting task 1.0 in stage 1.0 (TID 3, pc502.emulab.net, partition 1,PROCESS_LOCAL, 2215 bytes)
16/08/14 07:55:03 INFO DAGScheduler: DANIAR DAGScheduler.scala  org.apache.spark.scheduler.TaskInfo@517f7fa0    ResultTask(1, 1)
16/08/14 07:55:03 INFO TaskSetManager: Daniar: END Important! Starting task 1.0 in stage 1.0 
16/08/14 07:55:03 INFO TaskSchedulerImpl: DANIAR: TASK LAUNCHED taskSet.name [stage] = TaskSet_1  daniar_counter = 3
16/08/14 07:55:03 INFO SparkDeploySchedulerBackend: Daniarrr:  Before assigning any tasks it should be no random assignment [should be printed once]
16/08/14 07:55:03 INFO SparkDeploySchedulerBackend: Daniarrr:  Before assigning any tasks it should be no random assignment [should be printed once]
16/08/14 07:55:03 INFO MemoryStore: DANIAR HERE AT MEMORY STORE getBytes MemoryEntry(java.nio.HeapByteBuffer[pos=0 lim=3786 cap=3786],3786,false)
16/08/14 07:55:03 INFO MemoryStore: DANIAR HERE AT MEMORY STORE getBytes MemoryEntry(java.nio.HeapByteBuffer[pos=0 lim=3786 cap=3786],3786,false)
16/08/14 07:55:03 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on pc511.emulab.net:34605 (size: 3.7 KB, free: 511.1 MB)
16/08/14 07:55:03 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on pc502.emulab.net:39143 (size: 3.7 KB, free: 511.1 MB)
16/08/14 07:55:15 INFO SparkDeploySchedulerBackend: daniarrr -- makeOffers0
16/08/14 07:55:15 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 2) in 11817 ms on pc511.emulab.net (1/2)
16/08/14 07:55:15 INFO SparkDeploySchedulerBackend: daniarrr -- makeOffers1
16/08/14 07:55:15 INFO TaskSetManager: Finished task 1.0 in stage 1.0 (TID 3) in 12048 ms on pc502.emulab.net (2/2)
16/08/14 07:55:15 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool 
16/08/14 07:55:15 INFO DAGScheduler: ResultStage 1 (sortByKey at /proj/cs331-uc/daniar/SPARK/spark-1.6.1/sort_in_node.py:12) finished in 12.046 s
16/08/14 07:55:15 INFO DAGScheduler: Job 1 finished: sortByKey at /proj/cs331-uc/daniar/SPARK/spark-1.6.1/sort_in_node.py:12, took 12.067466 s
16/08/14 07:55:15 INFO deprecation: mapred.tip.id is deprecated. Instead, use mapreduce.task.id
16/08/14 07:55:15 INFO deprecation: mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id
16/08/14 07:55:15 INFO deprecation: mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap
16/08/14 07:55:15 INFO deprecation: mapred.task.partition is deprecated. Instead, use mapreduce.task.partition
16/08/14 07:55:15 INFO deprecation: mapred.job.id is deprecated. Instead, use mapreduce.job.id
16/08/14 07:55:16 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
16/08/14 07:55:16 INFO SparkContext: Starting job: saveAsTextFile at NativeMethodAccessorImpl.java:-2
16/08/14 07:55:16 INFO DAGScheduler: Registering RDD 5 (sortByKey at /proj/cs331-uc/daniar/SPARK/spark-1.6.1/sort_in_node.py:12)
16/08/14 07:55:16 INFO DAGScheduler: Got job 2 (saveAsTextFile at NativeMethodAccessorImpl.java:-2) with 2 output partitions
16/08/14 07:55:16 INFO DAGScheduler: Final stage: ResultStage 3 (saveAsTextFile at NativeMethodAccessorImpl.java:-2)
16/08/14 07:55:16 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 2)
16/08/14 07:55:16 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 2)
16/08/14 07:55:16 INFO DAGScheduler: Submitting ShuffleMapStage 2 (PairwiseRDD[5] at sortByKey at /proj/cs331-uc/daniar/SPARK/spark-1.6.1/sort_in_node.py:12), which has no missing parents
16/08/14 07:55:16 INFO MemoryStore: DANIAR HERE AT MEMORY STORE putIteratornon-empty iterator
16/08/14 07:55:16 INFO MemoryStore: DANIAR HERE AT MEMORY STORE currentTaskAttemptId
16/08/14 07:55:16 INFO MemoryStore: DANIAR HERE AT MEMORY STORE currentTaskAttemptId
16/08/14 07:55:16 INFO MemoryStore: DANIAR HERE AT MEMORY STORE putArray [Ljava.lang.Object;@7794641f  blockID broadcast_3
16/08/14 07:55:16 INFO MemoryStore: DANIAR HERE AT MEMORY STORE currentTaskAttemptId
16/08/14 07:55:16 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 7.5 KB, free 280.6 KB)
16/08/14 07:55:16 INFO MemoryStore: DANIAR HERE AT MEMORY STORE currentTaskAttemptId
16/08/14 07:55:16 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 4.7 KB, free 285.3 KB)
16/08/14 07:55:16 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 155.98.38.95:56470 (size: 4.7 KB, free: 511.1 MB)
16/08/14 07:55:16 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1007
16/08/14 07:55:16 INFO DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 2 (PairwiseRDD[5] at sortByKey at /proj/cs331-uc/daniar/SPARK/spark-1.6.1/sort_in_node.py:12)
16/08/14 07:55:16 INFO TaskSchedulerImpl: Adding task set 2.0 with 2 tasks
16/08/14 07:55:16 INFO TaskSetManager: SAPIIIIIIIIIIIII  TaskSet 2.0
16/08/14 07:55:16 INFO TaskSchedulerImpl: DANIAR: FINISH SUBMIT TASK SET
16/08/14 07:55:16 INFO TaskSetManager: Daniar: START Important! Starting task 0.0 in stage 2.0 (TID 4, pc493.emulab.net, partition 0,PROCESS_LOCAL, 2204 bytes)
16/08/14 07:55:16 INFO DAGScheduler: DANIAR DAGScheduler.scala  org.apache.spark.scheduler.TaskInfo@5fd57ac6    ShuffleMapTask(2, 0)
16/08/14 07:55:16 INFO TaskSetManager: Daniar: END Important! Starting task 0.0 in stage 2.0 
16/08/14 07:55:16 INFO TaskSchedulerImpl: DANIAR: TASK LAUNCHED taskSet.name [stage] = TaskSet_2  daniar_counter = 0
16/08/14 07:55:16 INFO TaskSetManager: Daniar: START Important! Starting task 1.0 in stage 2.0 (TID 5, pc515.emulab.net, partition 1,PROCESS_LOCAL, 2204 bytes)
16/08/14 07:55:16 INFO DAGScheduler: DANIAR DAGScheduler.scala  org.apache.spark.scheduler.TaskInfo@26588341    ShuffleMapTask(2, 1)
16/08/14 07:55:16 INFO TaskSetManager: Daniar: END Important! Starting task 1.0 in stage 2.0 
16/08/14 07:55:16 INFO TaskSchedulerImpl: DANIAR: TASK LAUNCHED taskSet.name [stage] = TaskSet_2  daniar_counter = 1
16/08/14 07:55:16 INFO SparkDeploySchedulerBackend: Daniarrr:  Before assigning any tasks it should be no random assignment [should be printed once]
16/08/14 07:55:16 INFO SparkDeploySchedulerBackend: Daniarrr:  Before assigning any tasks it should be no random assignment [should be printed once]
16/08/14 07:55:16 INFO MemoryStore: DANIAR HERE AT MEMORY STORE getBytes MemoryEntry(java.nio.HeapByteBuffer[pos=0 lim=4812 cap=4812],4812,false)
16/08/14 07:55:16 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on pc515.emulab.net:35947 (size: 4.7 KB, free: 511.1 MB)
16/08/14 07:55:16 INFO MemoryStore: DANIAR HERE AT MEMORY STORE getBytes MemoryEntry(java.nio.HeapByteBuffer[pos=0 lim=4812 cap=4812],4812,false)
16/08/14 07:55:16 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on pc493.emulab.net:43443 (size: 4.7 KB, free: 511.1 MB)
16/08/14 07:55:17 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on pc515.emulab.net:35947 (size: 21.8 KB, free: 511.1 MB)
16/08/14 07:55:17 INFO MemoryStore: DANIAR HERE AT MEMORY STORE getBytes MemoryEntry(java.nio.HeapByteBuffer[pos=0 lim=22319 cap=22319],22319,false)
16/08/14 07:55:17 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on pc493.emulab.net:43443 (size: 21.8 KB, free: 511.1 MB)
16/08/14 07:55:37 INFO SparkDeploySchedulerBackend: daniarrr -- makeOffers3
16/08/14 07:55:37 INFO TaskSetManager: Finished task 1.0 in stage 2.0 (TID 5) in 20937 ms on pc515.emulab.net (1/2)
16/08/14 07:55:46 INFO SparkDeploySchedulerBackend: daniarrr -- makeOffers2
16/08/14 07:55:46 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 4) in 30245 ms on pc493.emulab.net (2/2)
16/08/14 07:55:46 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool 
16/08/14 07:55:46 INFO DAGScheduler: ShuffleMapStage 2 (sortByKey at /proj/cs331-uc/daniar/SPARK/spark-1.6.1/sort_in_node.py:12) finished in 30.246 s
16/08/14 07:55:46 INFO DAGScheduler: looking for newly runnable stages
16/08/14 07:55:46 INFO DAGScheduler: running: Set()
16/08/14 07:55:46 INFO DAGScheduler: waiting: Set(ResultStage 3)
16/08/14 07:55:46 INFO DAGScheduler: failed: Set()
16/08/14 07:55:46 INFO DAGScheduler: Submitting ResultStage 3 (MapPartitionsRDD[10] at saveAsTextFile at NativeMethodAccessorImpl.java:-2), which has no missing parents
16/08/14 07:55:46 INFO MemoryStore: DANIAR HERE AT MEMORY STORE putIteratornon-empty iterator
16/08/14 07:55:46 INFO MemoryStore: DANIAR HERE AT MEMORY STORE currentTaskAttemptId
16/08/14 07:55:46 INFO MemoryStore: DANIAR HERE AT MEMORY STORE currentTaskAttemptId
16/08/14 07:55:46 INFO MemoryStore: DANIAR HERE AT MEMORY STORE putArray [Ljava.lang.Object;@76e738c6  blockID broadcast_4
16/08/14 07:55:46 INFO MemoryStore: DANIAR HERE AT MEMORY STORE currentTaskAttemptId
16/08/14 07:55:46 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 75.0 KB, free 360.4 KB)
16/08/14 07:55:46 INFO MemoryStore: DANIAR HERE AT MEMORY STORE currentTaskAttemptId
16/08/14 07:55:46 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 27.2 KB, free 387.5 KB)
16/08/14 07:55:46 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 155.98.38.95:56470 (size: 27.2 KB, free: 511.1 MB)
16/08/14 07:55:46 INFO SparkContext: Created broadcast 4 from broadcast at DAGScheduler.scala:1007
16/08/14 07:55:46 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 3 (MapPartitionsRDD[10] at saveAsTextFile at NativeMethodAccessorImpl.java:-2)
16/08/14 07:55:46 INFO TaskSchedulerImpl: Adding task set 3.0 with 2 tasks
16/08/14 07:55:46 INFO TaskSetManager: SAPIIIIIIIIIIIII  TaskSet 3.0
16/08/14 07:55:46 INFO TaskSchedulerImpl: DANIAR: FINISH SUBMIT TASK SET
16/08/14 07:55:46 INFO TaskSetManager: Daniar: START Important! Starting task 0.0 in stage 3.0 (TID 6, pc493.emulab.net, partition 0,NODE_LOCAL, 1951 bytes)
16/08/14 07:55:46 INFO DAGScheduler: DANIAR DAGScheduler.scala  org.apache.spark.scheduler.TaskInfo@766ff162    ResultTask(3, 0)
16/08/14 07:55:46 INFO TaskSetManager: Daniar: END Important! Starting task 0.0 in stage 3.0 
16/08/14 07:55:46 INFO TaskSchedulerImpl: DANIAR: TASK LAUNCHED taskSet.name [stage] = TaskSet_3  daniar_counter = 2
16/08/14 07:55:46 INFO TaskSetManager: Daniar: START Important! Starting task 1.0 in stage 3.0 (TID 7, pc515.emulab.net, partition 1,NODE_LOCAL, 1951 bytes)
16/08/14 07:55:46 INFO DAGScheduler: DANIAR DAGScheduler.scala  org.apache.spark.scheduler.TaskInfo@3071f2bb    ResultTask(3, 1)
16/08/14 07:55:46 INFO TaskSetManager: Daniar: END Important! Starting task 1.0 in stage 3.0 
16/08/14 07:55:46 INFO TaskSchedulerImpl: DANIAR: TASK LAUNCHED taskSet.name [stage] = TaskSet_3  daniar_counter = 3
16/08/14 07:55:46 INFO SparkDeploySchedulerBackend: Daniarrr:  Before assigning any tasks it should be no random assignment [should be printed once]
16/08/14 07:55:46 INFO SparkDeploySchedulerBackend: Daniarrr:  Before assigning any tasks it should be no random assignment [should be printed once]
16/08/14 07:55:46 INFO MemoryStore: DANIAR HERE AT MEMORY STORE getBytes MemoryEntry(java.nio.HeapByteBuffer[pos=0 lim=27818 cap=27818],27818,false)
16/08/14 07:55:46 INFO MemoryStore: DANIAR HERE AT MEMORY STORE getBytes MemoryEntry(java.nio.HeapByteBuffer[pos=0 lim=27818 cap=27818],27818,false)
16/08/14 07:55:46 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on pc493.emulab.net:43443 (size: 27.2 KB, free: 511.1 MB)
16/08/14 07:55:46 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on pc515.emulab.net:35947 (size: 27.2 KB, free: 511.1 MB)
16/08/14 07:55:46 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 0 to pc515.emulab.net:36890
16/08/14 07:55:46 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 0 to pc493.emulab.net:40269
16/08/14 07:55:46 INFO MapOutputTrackerMaster: Size of output statuses for shuffle 0 is 166 bytes
16/08/14 07:55:46 INFO MapOutputTrackerMaster: Size of output statuses for shuffle 0 is 166 bytes
16/08/14 07:56:20 INFO SparkDeploySchedulerBackend: daniarrr -- makeOffers3
16/08/14 07:56:20 INFO TaskSetManager: Finished task 1.0 in stage 3.0 (TID 7) in 33590 ms on pc515.emulab.net (1/2)
16/08/14 07:56:37 INFO TaskSetManager: Marking task 0 in stage 3.0 (on pc493.emulab.net) as speculatable because it ran more than 50385 ms
16/08/14 07:56:43 INFO SparkDeploySchedulerBackend: daniarrr -- makeOffers2
16/08/14 07:56:43 INFO DAGScheduler: ResultStage 3 (saveAsTextFile at NativeMethodAccessorImpl.java:-2) finished in 57.369 s
16/08/14 07:56:43 INFO TaskSetManager: Finished task 0.0 in stage 3.0 (TID 6) in 57368 ms on pc493.emulab.net (2/2)
16/08/14 07:56:43 INFO TaskSchedulerImpl: Removed TaskSet 3.0, whose tasks have all completed, from pool 
16/08/14 07:56:43 INFO DAGScheduler: Job 2 finished: saveAsTextFile at NativeMethodAccessorImpl.java:-2, took 87.832705 s
16/08/14 07:56:43 INFO SparkContext: Invoking stop() from shutdown hook
16/08/14 07:56:43 INFO ContextHandler: stopped o.e.j.s.ServletContextHandler{/metrics/json,null}
16/08/14 07:56:43 INFO ContextHandler: stopped o.e.j.s.ServletContextHandler{/stages/stage/kill,null}
16/08/14 07:56:43 INFO ContextHandler: stopped o.e.j.s.ServletContextHandler{/api,null}
16/08/14 07:56:43 INFO ContextHandler: stopped o.e.j.s.ServletContextHandler{/,null}
16/08/14 07:56:43 INFO ContextHandler: stopped o.e.j.s.ServletContextHandler{/static,null}
16/08/14 07:56:43 INFO ContextHandler: stopped o.e.j.s.ServletContextHandler{/executors/threadDump/json,null}
16/08/14 07:56:43 INFO ContextHandler: stopped o.e.j.s.ServletContextHandler{/executors/threadDump,null}
16/08/14 07:56:43 INFO ContextHandler: stopped o.e.j.s.ServletContextHandler{/executors/json,null}
16/08/14 07:56:43 INFO ContextHandler: stopped o.e.j.s.ServletContextHandler{/executors,null}
16/08/14 07:56:43 INFO ContextHandler: stopped o.e.j.s.ServletContextHandler{/environment/json,null}
16/08/14 07:56:43 INFO ContextHandler: stopped o.e.j.s.ServletContextHandler{/environment,null}
16/08/14 07:56:43 INFO ContextHandler: stopped o.e.j.s.ServletContextHandler{/storage/rdd/json,null}
16/08/14 07:56:43 INFO ContextHandler: stopped o.e.j.s.ServletContextHandler{/storage/rdd,null}
16/08/14 07:56:43 INFO ContextHandler: stopped o.e.j.s.ServletContextHandler{/storage/json,null}
16/08/14 07:56:43 INFO ContextHandler: stopped o.e.j.s.ServletContextHandler{/storage,null}
16/08/14 07:56:43 INFO ContextHandler: stopped o.e.j.s.ServletContextHandler{/stages/pool/json,null}
16/08/14 07:56:43 INFO ContextHandler: stopped o.e.j.s.ServletContextHandler{/stages/pool,null}
16/08/14 07:56:43 INFO ContextHandler: stopped o.e.j.s.ServletContextHandler{/stages/stage/json,null}
16/08/14 07:56:43 INFO ContextHandler: stopped o.e.j.s.ServletContextHandler{/stages/stage,null}
16/08/14 07:56:43 INFO ContextHandler: stopped o.e.j.s.ServletContextHandler{/stages/json,null}
16/08/14 07:56:43 INFO ContextHandler: stopped o.e.j.s.ServletContextHandler{/stages,null}
16/08/14 07:56:43 INFO ContextHandler: stopped o.e.j.s.ServletContextHandler{/jobs/job/json,null}
16/08/14 07:56:43 INFO ContextHandler: stopped o.e.j.s.ServletContextHandler{/jobs/job,null}
16/08/14 07:56:43 INFO ContextHandler: stopped o.e.j.s.ServletContextHandler{/jobs/json,null}
16/08/14 07:56:43 INFO ContextHandler: stopped o.e.j.s.ServletContextHandler{/jobs,null}
16/08/14 07:56:44 WARN QueuedThreadPool: 4 threads could not be stopped
16/08/14 07:56:44 INFO SparkUI: Stopped Spark web UI at http://155.98.38.95:4040
16/08/14 07:56:44 INFO SparkDeploySchedulerBackend: Shutting down all executors
16/08/14 07:56:44 INFO SparkDeploySchedulerBackend: Asking each executor to shut down
16/08/14 07:56:44 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
16/08/14 07:56:44 INFO MemoryStore: MemoryStore cleared
16/08/14 07:56:44 INFO BlockManager: BlockManager stopped
16/08/14 07:56:44 INFO BlockManagerMaster: BlockManagerMaster stopped
16/08/14 07:56:44 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
16/08/14 07:56:44 INFO RemoteActorRefProvider$RemotingTerminator: Shutting down remote daemon.
16/08/14 07:56:44 INFO RemoteActorRefProvider$RemotingTerminator: Remote daemon shut down; proceeding with flushing remote transports.
16/08/14 07:56:44 INFO SparkContext: Successfully stopped SparkContext
16/08/14 07:56:44 INFO ShutdownHookManager: Shutdown hook called
16/08/14 07:56:44 INFO ShutdownHookManager: Deleting directory /tmp/spark-5881480f-b42d-4c94-8d9f-adf1b6e75a72
16/08/14 07:56:44 INFO ShutdownHookManager: Deleting directory /tmp/spark-5881480f-b42d-4c94-8d9f-adf1b6e75a72/pyspark-9eb8c712-97dc-42df-9619-7beb2ce0f01c
16/08/14 07:56:44 INFO ShutdownHookManager: Deleting directory /tmp/spark-5881480f-b42d-4c94-8d9f-adf1b6e75a72/httpd-b399b3b3-e104-4a34-adb7-4c58888dacd8
