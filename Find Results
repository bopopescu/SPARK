Searching 6416 files for "onBlockFetchSuccess" (case sensitive)

0 matches

Searching 17282 files for "onBlockFetchSuccess" (case sensitive)

/home/daniar/documents/SPARK/spark-1.6.1/assembly/target/scala-2.10/spark-assembly-1.6.1-hadoop2.7.2.jar:
    File too large, skipping

/home/daniar/documents/SPARK/spark-1.6.1/core/src/main/scala/org/apache/spark/network/BlockTransferService.scala:
   58     *
   59     * Note that this API takes a sequence so the implementation can batch requests, and does not
   60:    * return a future so the underlying implementation can invoke onBlockFetchSuccess as soon as
   61     * the data of a block is fetched, rather than waiting for all blocks to be fetched.
   62     */
   ..
   92            result.failure(exception)
   93          }
   94:         override def onBlockFetchSuccess(blockId: String, data: ManagedBuffer): Unit = {
   95            val ret = ByteBuffer.allocate(data.size.toInt)
   96            ret.put(data.nioByteBuffer())

/home/daniar/documents/SPARK/spark-1.6.1/core/src/main/scala/org/apache/spark/storage/ShuffleBlockFetcherIterator.scala:
  166      shuffleClient.fetchBlocks(address.host, address.port, address.executorId, blockIds.toArray,
  167        new BlockFetchingListener {
  168:         override def onBlockFetchSuccess(blockId: String, buf: ManagedBuffer): Unit = {
  169            // Only add the buffer to results queue if the iterator is not zombie,
  170            // i.e. cleanup() has not been called yet.

/home/daniar/documents/SPARK/spark-1.6.1/core/src/test/scala/org/apache/spark/network/netty/NettyBlockTransferSecuritySuite.scala:
  144          }
  145  
  146:         override def onBlockFetchSuccess(blockId: String, data: ManagedBuffer): Unit = {
  147            promise.success(data.retain())
  148          }

/home/daniar/documents/SPARK/spark-1.6.1/core/src/test/scala/org/apache/spark/storage/ShuffleBlockFetcherIteratorSuite.scala:
   51          for (blockId <- blocks) {
   52            if (data.contains(BlockId(blockId))) {
   53:             listener.onBlockFetchSuccess(blockId, data(BlockId(blockId)))
   54            } else {
   55              listener.onBlockFetchFailure(blockId, new BlockNotFoundException(blockId))
   ..
  152          future {
  153            // Return the first two blocks, and wait till task completion before returning the 3rd one
  154:           listener.onBlockFetchSuccess(
  155              ShuffleBlockId(0, 0, 0).toString, blocks(ShuffleBlockId(0, 0, 0)))
  156:           listener.onBlockFetchSuccess(
  157              ShuffleBlockId(0, 1, 0).toString, blocks(ShuffleBlockId(0, 1, 0)))
  158            sem.acquire()
  159:           listener.onBlockFetchSuccess(
  160              ShuffleBlockId(0, 2, 0).toString, blocks(ShuffleBlockId(0, 2, 0)))
  161          }
  ...
  214          future {
  215            // Return the first block, and then fail.
  216:           listener.onBlockFetchSuccess(
  217              ShuffleBlockId(0, 0, 0).toString, blocks(ShuffleBlockId(0, 0, 0)))
  218            listener.onBlockFetchFailure(

/home/daniar/documents/SPARK/spark-1.6.1/core/target/java/org/apache/spark/network/BlockTransferService.java:
   27     * <p>
   28     * Note that this API takes a sequence so the implementation can batch requests, and does not
   29:    * return a future so the underlying implementation can invoke onBlockFetchSuccess as soon as
   30     * the data of a block is fetched, rather than waiting for all blocks to be fetched.
   31     * @param host (undocumented)

/home/daniar/documents/SPARK/spark-1.6.1/core/target/streams/compile/incCompileSetup/$global/streams/inc_compile_2.10:
 5144  /home/daniar/documents/SPARK/spark-1.6.1/core/src/main/scala/org/apache/spark/storage/ShuffleBlockFetcherIterator.scala -> /home/daniar/documents/SPARK/spark-1.6.1/core/target/scala-2.10/classes/org/apache/spark/storage/BufferReleasingInputStream.class
 5145  /home/daniar/documents/SPARK/spark-1.6.1/core/src/main/scala/org/apache/spark/storage/ShuffleBlockFetcherIterator.scala -> /home/daniar/documents/SPARK/spark-1.6.1/core/target/scala-2.10/classes/org/apache/spark/storage/ShuffleBlockFetcherIterator$$anon$1$$anonfun$onBlockFetchFailure$1.class
 5146: /home/daniar/documents/SPARK/spark-1.6.1/core/src/main/scala/org/apache/spark/storage/ShuffleBlockFetcherIterator.scala -> /home/daniar/documents/SPARK/spark-1.6.1/core/target/scala-2.10/classes/org/apache/spark/storage/ShuffleBlockFetcherIterator$$anon$1$$anonfun$onBlockFetchSuccess$1.class
 5147  /home/daniar/documents/SPARK/spark-1.6.1/core/src/main/scala/org/apache/spark/storage/ShuffleBlockFetcherIterator.scala -> /home/daniar/documents/SPARK/spark-1.6.1/core/target/scala-2.10/classes/org/apache/spark/storage/ShuffleBlockFetcherIterator$$anon$1.class
 5148  /home/daniar/documents/SPARK/spark-1.6.1/core/src/main/scala/org/apache/spark/storage/ShuffleBlockFetcherIterator.scala -> /home/daniar/documents/SPARK/spark-1.6.1/core/target/scala-2.10/classes/org/apache/spark/storage/ShuffleBlockFetcherIterator$$anonfun$1.class
 ....
 16999  /home/daniar/documents/SPARK/spark-1.6.1/core/src/main/scala/org/apache/spark/storage/ShuffleBlockFetcherIterator.scala -> org.apache.spark.storage.ShuffleBlockFetcherIterator$$anon$1
 17000  /home/daniar/documents/SPARK/spark-1.6.1/core/src/main/scala/org/apache/spark/storage/ShuffleBlockFetcherIterator.scala -> org.apache.spark.storage.ShuffleBlockFetcherIterator$$anon$1$$anonfun$onBlockFetchFailure$1
 17001: /home/daniar/documents/SPARK/spark-1.6.1/core/src/main/scala/org/apache/spark/storage/ShuffleBlockFetcherIterator.scala -> org.apache.spark.storage.ShuffleBlockFetcherIterator$$anon$1$$anonfun$onBlockFetchSuccess$1
 17002  /home/daniar/documents/SPARK/spark-1.6.1/core/src/main/scala/org/apache/spark/storage/ShuffleBlockFetcherIterator.scala -> org.apache.spark.storage.ShuffleBlockFetcherIterator$$anonfun$1
 17003  /home/daniar/documents/SPARK/spark-1.6.1/core/src/main/scala/org/apache/spark/storage/ShuffleBlockFetcherIterator.scala -> org.apache.spark.storage.ShuffleBlockFetcherIterator$$anonfun$2
 .....
 72211  /home/daniar/documents/SPARK/spark-1.6.1/core/target/scala-2.10/classes/org/apache/spark/storage/RDDInfo.class -> lastModified(1473945315000)
 72212  /home/daniar/documents/SPARK/spark-1.6.1/core/target/scala-2.10/classes/org/apache/spark/storage/ShuffleBlockFetcherIterator$$anon$1$$anonfun$onBlockFetchFailure$1.class -> lastModified(1473945315000)
 72213: /home/daniar/documents/SPARK/spark-1.6.1/core/target/scala-2.10/classes/org/apache/spark/storage/ShuffleBlockFetcherIterator$$anon$1$$anonfun$onBlockFetchSuccess$1.class -> lastModified(1473945315000)
 72214  /home/daniar/documents/SPARK/spark-1.6.1/core/target/scala-2.10/classes/org/apache/spark/storage/ShuffleBlockFetcherIterator$$anon$1.class -> lastModified(1473945315000)
 72215  /home/daniar/documents/SPARK/spark-1.6.1/core/target/scala-2.10/classes/org/apache/spark/storage/ShuffleBlockFetcherIterator$$anonfun$1.class -> lastModified(1473945315000)

/home/daniar/documents/SPARK/spark-1.6.1/network/shuffle/src/main/java/org/apache/spark/network/shuffle/BlockFetchingListener.java:
   28     * and release() the buffer on their own, or copy the data to a new buffer.
   29     */
   30:   void onBlockFetchSuccess(String blockId, ManagedBuffer data);
   31  
   32    /**

/home/daniar/documents/SPARK/spark-1.6.1/network/shuffle/src/main/java/org/apache/spark/network/shuffle/OneForOneBlockFetcher.java:
   70      public void onSuccess(int chunkIndex, ManagedBuffer buffer) {
   71        // On receipt of a chunk, pass it upwards as a block.
   72:       listener.onBlockFetchSuccess(blockIds[chunkIndex], buffer);
   73      }
   74  

/home/daniar/documents/SPARK/spark-1.6.1/network/shuffle/src/main/java/org/apache/spark/network/shuffle/RetryingBlockFetcher.java:
  191    private class RetryingBlockFetchListener implements BlockFetchingListener {
  192      @Override
  193:     public void onBlockFetchSuccess(String blockId, ManagedBuffer data) {
  194        // We will only forward this success message to our parent listener if this block request is
  195        // outstanding and we are still the active listener.
  ...
  204        // Now actually invoke the parent listener, outside of the synchronized block.
  205        if (shouldForwardSuccess) {
  206:         listener.onBlockFetchSuccess(blockId, data);
  207        }
  208      }

/home/daniar/documents/SPARK/spark-1.6.1/network/shuffle/src/main/java/org/apache/spark/network/shuffle/ShuffleClient.java:
   33     *
   34     * Note that this API takes a sequence so the implementation can batch requests, and does not
   35:    * return a future so the underlying implementation can invoke onBlockFetchSuccess as soon as
   36     * the data of a block is fetched, rather than waiting for all blocks to be fetched.
   37     */

/home/daniar/documents/SPARK/spark-1.6.1/network/shuffle/src/test/java/org/apache/spark/network/sasl/SaslIntegrationSuite.java:
  200        BlockFetchingListener listener = new BlockFetchingListener() {
  201          @Override
  202:         public synchronized void onBlockFetchSuccess(String blockId, ManagedBuffer data) {
  203            notifyAll();
  204          }

/home/daniar/documents/SPARK/spark-1.6.1/network/shuffle/src/test/java/org/apache/spark/network/shuffle/ExternalShuffleIntegrationSuite.java:
  142        new BlockFetchingListener() {
  143          @Override
  144:         public void onBlockFetchSuccess(String blockId, ManagedBuffer data) {
  145            synchronized (this) {
  146              if (!res.successBlocks.contains(blockId) && !res.failedBlocks.contains(blockId)) {

/home/daniar/documents/SPARK/spark-1.6.1/network/shuffle/src/test/java/org/apache/spark/network/shuffle/OneForOneBlockFetcherSuite.java:
   58      BlockFetchingListener listener = fetchBlocks(blocks);
   59  
   60:     verify(listener).onBlockFetchSuccess("shuffle_0_0_0", blocks.get("shuffle_0_0_0"));
   61    }
   62  
   ..
   71  
   72      for (int i = 0; i < 3; i ++) {
   73:       verify(listener, times(1)).onBlockFetchSuccess("b" + i, blocks.get("b" + i));
   74      }
   75    }
   ..
   85  
   86      // Each failure will cause a failure to be invoked in all remaining block fetches.
   87:     verify(listener, times(1)).onBlockFetchSuccess("b0", blocks.get("b0"));
   88      verify(listener, times(1)).onBlockFetchFailure(eq("b1"), (Throwable) any());
   89      verify(listener, times(2)).onBlockFetchFailure(eq("b2"), (Throwable) any());
   ..
  100  
  101      // We may call both success and failure for the same block.
  102:     verify(listener, times(1)).onBlockFetchSuccess("b0", blocks.get("b0"));
  103      verify(listener, times(1)).onBlockFetchFailure(eq("b1"), (Throwable) any());
  104:     verify(listener, times(1)).onBlockFetchSuccess("b2", blocks.get("b2"));
  105      verify(listener, times(1)).onBlockFetchFailure(eq("b2"), (Throwable) any());
  106    }

/home/daniar/documents/SPARK/spark-1.6.1/network/shuffle/src/test/java/org/apache/spark/network/shuffle/RetryingBlockFetcherSuite.java:
   80      performInteractions(interactions, listener);
   81  
   82:     verify(listener).onBlockFetchSuccess("b0", block0);
   83:     verify(listener).onBlockFetchSuccess("b1", block1);
   84      verifyNoMoreInteractions(listener);
   85    }
   ..
  100  
  101      verify(listener).onBlockFetchFailure(eq("b0"), (Throwable) any());
  102:     verify(listener).onBlockFetchSuccess("b1", block1);
  103      verifyNoMoreInteractions(listener);
  104    }
  ...
  122      performInteractions(interactions, listener);
  123  
  124:     verify(listener, timeout(5000)).onBlockFetchSuccess("b0", block0);
  125:     verify(listener, timeout(5000)).onBlockFetchSuccess("b1", block1);
  126      verifyNoMoreInteractions(listener);
  127    }
  ...
  144      performInteractions(interactions, listener);
  145  
  146:     verify(listener, timeout(5000)).onBlockFetchSuccess("b0", block0);
  147:     verify(listener, timeout(5000)).onBlockFetchSuccess("b1", block1);
  148      verifyNoMoreInteractions(listener);
  149    }
  ...
  172      performInteractions(interactions, listener);
  173  
  174:     verify(listener, timeout(5000)).onBlockFetchSuccess("b0", block0);
  175:     verify(listener, timeout(5000)).onBlockFetchSuccess("b1", block1);
  176      verifyNoMoreInteractions(listener);
  177    }
  ...
  204      performInteractions(interactions, listener);
  205  
  206:     verify(listener, timeout(5000)).onBlockFetchSuccess("b0", block0);
  207      verify(listener, timeout(5000)).onBlockFetchFailure(eq("b1"), (Throwable) any());
  208      verifyNoMoreInteractions(listener);
  ...
  234      performInteractions(interactions, listener);
  235  
  236:     verify(listener, timeout(5000)).onBlockFetchSuccess("b0", block0);
  237      verify(listener, timeout(5000)).onBlockFetchFailure(eq("b1"), (Throwable) any());
  238:     verify(listener, timeout(5000)).onBlockFetchSuccess("b2", block2);
  239      verifyNoMoreInteractions(listener);
  240    }
  ...
  283  
  284                if (blockValue instanceof ManagedBuffer) {
  285:                 retryListener.onBlockFetchSuccess(blockId, (ManagedBuffer) blockValue);
  286                } else if (blockValue instanceof Exception) {
  287                  retryListener.onBlockFetchFailure(blockId, (Exception) blockValue);

38 matches across 14 files



Searching 17282 files for "logger.org.apache.spark"

/home/daniar/documents/SPARK/spark-1.6.1/assembly/target/scala-2.10/spark-assembly-1.6.1-hadoop2.7.2.jar:
    File too large, skipping

/home/daniar/documents/SPARK/spark-1.6.1/conf/log4j.properties.template:
   26  log4j.logger.org.spark-project.jetty=WARN
   27  log4j.logger.org.spark-project.jetty.util.component.AbstractLifeCycle=ERROR
   28: log4j.logger.org.apache.spark.repl.SparkIMain$exprTyper=INFO
   29: log4j.logger.org.apache.spark.repl.SparkILoop$SparkILoopInterpreter=INFO
   30  log4j.logger.org.apache.parquet=ERROR
   31  log4j.logger.parquet=ERROR

/home/daniar/documents/SPARK/spark-1.6.1/core/src/main/resources/org/apache/spark/log4j-defaults-repl.properties:
   26  log4j.logger.org.spark-project.jetty=ALL
   27  log4j.logger.org.spark-project.jetty.util.component.AbstractLifeCycle=ALL
   28: log4j.logger.org.apache.spark.repl.SparkIMain$exprTyper=ALL
   29: log4j.logger.org.apache.spark.repl.SparkILoop$SparkILoopInterpreter=ALL
   30  
   31  # SPARK-9183: Settings to avoid annoying messages when looking up nonexistent UDFs in SparkSQL with Hive support

/home/daniar/documents/SPARK/spark-1.6.1/core/src/main/resources/org/apache/spark/log4j-defaults.properties:
   26  log4j.logger.org.spark-project.jetty=WARN
   27  log4j.logger.org.spark-project.jetty.util.component.AbstractLifeCycle=ERROR
   28: log4j.logger.org.apache.spark.repl.SparkIMain$exprTyper=INFO
   29: log4j.logger.org.apache.spark.repl.SparkILoop$SparkILoopInterpreter=INFO
   30  
   31  # SPARK-9183: Settings to avoid annoying messages when looking up nonexistent UDFs in SparkSQL with Hive support

/home/daniar/documents/SPARK/spark-1.6.1/core/target/scala-2.10/classes/org/apache/spark/log4j-defaults-repl.properties:
   26  log4j.logger.org.spark-project.jetty=WARN
   27  log4j.logger.org.spark-project.jetty.util.component.AbstractLifeCycle=ERROR
   28: log4j.logger.org.apache.spark.repl.SparkIMain$exprTyper=INFO
   29: log4j.logger.org.apache.spark.repl.SparkILoop$SparkILoopInterpreter=INFO
   30  
   31  # SPARK-9183: Settings to avoid annoying messages when looking up nonexistent UDFs in SparkSQL with Hive support

/home/daniar/documents/SPARK/spark-1.6.1/core/target/scala-2.10/classes/org/apache/spark/log4j-defaults.properties:
   26  log4j.logger.org.spark-project.jetty=WARN
   27  log4j.logger.org.spark-project.jetty.util.component.AbstractLifeCycle=ERROR
   28: log4j.logger.org.apache.spark.repl.SparkIMain$exprTyper=INFO
   29: log4j.logger.org.apache.spark.repl.SparkILoop$SparkILoopInterpreter=INFO
   30  
   31  # SPARK-9183: Settings to avoid annoying messages when looking up nonexistent UDFs in SparkSQL with Hive support

/home/daniar/documents/SPARK/spark-1.6.1/extras/kinesis-asl/src/main/resources/log4j.properties:
   34  log4j.logger.org.spark-project.jetty=WARN
   35  log4j.logger.org.spark-project.jetty.util.component.AbstractLifeCycle=ERROR
   36: log4j.logger.org.apache.spark.repl.SparkIMain$exprTyper=INFO
   37: log4j.logger.org.apache.spark.repl.SparkILoop$SparkILoopInterpreter=INFO

/home/daniar/documents/SPARK/spark-1.6.1/sql/core/src/test/resources/log4j.properties:
   54  # Parquet related logging
   55  log4j.logger.org.apache.parquet.hadoop=WARN
   56: log4j.logger.org.apache.spark.sql.parquet=INFO
   57  

13 matches across 7 files





























Searching 17282 files for "fetchBlocks"

/home/daniar/documents/SPARK/README.md:
  621  		- BlockStoreShuffleReader No
  622  
  623: 	2. Investigate fetchBlocks

/home/daniar/documents/SPARK/spark-1.6.1/assembly/target/scala-2.10/spark-assembly-1.6.1-hadoop2.7.2.jar:
    File too large, skipping

/home/daniar/documents/SPARK/spark-1.6.1/core/src/main/scala/org/apache/spark/network/BlockTransferService.scala:
   61     * the data of a block is fetched, rather than waiting for all blocks to be fetched.
   62     */
   63:   override def fetchBlocks(
   64        host: String,
   65        port: Int,
   ..
   80  
   81    /**
   82:    * A special case of [[fetchBlocks]], as it fetches only one block and is blocking.
   83     *
   84     * It is also only available after [[init]] is invoked.
   85     */
   86:   def fetchBlockSync(host: String, port: Int, execId: String, blockId: String): ManagedBuffer = {
   87      // A monitor for the thread to wait on.
   88      val result = Promise[ManagedBuffer]()
   89:     fetchBlocks(host, port, execId, Array(blockId),
   90        new BlockFetchingListener {
   91          override def onBlockFetchFailure(blockId: String, exception: Throwable): Unit = {
   ..
   96            ret.put(data.nioByteBuffer())
   97            ret.flip()
   98:           logInfo("on BlockTransferService fetchBlockSync success")
   99            result.success(new NioManagedBuffer(ret))
  100          }

/home/daniar/documents/SPARK/spark-1.6.1/core/src/main/scala/org/apache/spark/network/netty/NettyBlockTransferService.scala:
   78    }
   79  
   80:   override def fetchBlocks(
   81        host: String,
   82        port: Int,
   ..
  103      } catch {
  104        case e: Exception =>
  105:         logError("Exception while beginning fetchBlocks", e)
  106          blockIds.foreach(listener.onBlockFetchFailure(_, e))
  107      }

/home/daniar/documents/SPARK/spark-1.6.1/core/src/main/scala/org/apache/spark/storage/BlockManager.scala:
  598        logDebug(s"Getting remote block $blockId from $loc")
  599        val data = try {
  600:         blockTransferService.fetchBlockSync(
  601            loc.host, loc.port, loc.executorId, blockId.toString).nioByteBuffer()
  602        } catch {

/home/daniar/documents/SPARK/spark-1.6.1/core/src/main/scala/org/apache/spark/storage/ShuffleBlockFetcherIterator.scala:
  164  
  165      val address = req.address
  166:     shuffleClient.fetchBlocks(address.host, address.port, address.executorId, blockIds.toArray,
  167        new BlockFetchingListener {
  168          override def onBlockFetchSuccess(blockId: String, buf: ManagedBuffer): Unit = {

/home/daniar/documents/SPARK/spark-1.6.1/core/src/test/scala/org/apache/spark/DistributedSuite.scala:
  195      val blockTransfer = SparkEnv.get.blockTransferService
  196      blockManager.master.getLocations(blockId).foreach { cmId =>
  197:       val bytes = blockTransfer.fetchBlockSync(cmId.host, cmId.port, cmId.executorId,
  198          blockId.toString)
  199        val deserialized = blockManager.dataDeserialize(blockId, bytes.nioByteBuffer())

/home/daniar/documents/SPARK/spark-1.6.1/core/src/test/scala/org/apache/spark/network/netty/NettyBlockTransferSecuritySuite.scala:
  138      val promise = Promise[ManagedBuffer]()
  139  
  140:     self.fetchBlocks(from.hostName, from.port, execId, Array(blockId.toString),
  141        new BlockFetchingListener {
  142          override def onBlockFetchFailure(blockId: String, exception: Throwable): Unit = {

/home/daniar/documents/SPARK/spark-1.6.1/core/src/test/scala/org/apache/spark/storage/ShuffleBlockFetcherIteratorSuite.scala:
   44    private def createMockTransfer(data: Map[BlockId, ManagedBuffer]): BlockTransferService = {
   45      val transfer = mock(classOf[BlockTransferService])
   46:     when(transfer.fetchBlocks(any(), any(), any(), any(), any())).thenAnswer(new Answer[Unit] {
   47        override def answer(invocation: InvocationOnMock): Unit = {
   48          val blocks = invocation.getArguments()(3).asInstanceOf[Array[String]]
   ..
  126  
  127      // 3 local blocks, and 2 remote blocks
  128:     // (but from the same block manager so one call to fetchBlocks)
  129      verify(blockManager, times(3)).getBlockData(any())
  130:     verify(transfer, times(1)).fetchBlocks(any(), any(), any(), any(), any())
  131    }
  132  
  ...
  147  
  148      val transfer = mock(classOf[BlockTransferService])
  149:     when(transfer.fetchBlocks(any(), any(), any(), any(), any())).thenAnswer(new Answer[Unit] {
  150        override def answer(invocation: InvocationOnMock): Unit = {
  151          val listener = invocation.getArguments()(4).asInstanceOf[BlockFetchingListener]
  ...
  209  
  210      val transfer = mock(classOf[BlockTransferService])
  211:     when(transfer.fetchBlocks(any(), any(), any(), any(), any())).thenAnswer(new Answer[Unit] {
  212        override def answer(invocation: InvocationOnMock): Unit = {
  213          val listener = invocation.getArguments()(4).asInstanceOf[BlockFetchingListener]

/home/daniar/documents/SPARK/spark-1.6.1/core/target/java/org/apache/spark/network/BlockTransferService.java:
   35     * @param listener (undocumented)
   36     */
   37:   public abstract  void fetchBlocks (java.lang.String host, int port, java.lang.String execId, java.lang.String[] blockIds, org.apache.spark.network.shuffle.BlockFetchingListener listener) ;
   38    /**
   39     * Upload a single block to a remote node, available only after {@link init} is invoked.
   ..
   48    public abstract  scala.concurrent.Future<scala.runtime.BoxedUnit> uploadBlock (java.lang.String hostname, int port, java.lang.String execId, org.apache.spark.storage.BlockId blockId, org.apache.spark.network.buffer.ManagedBuffer blockData, org.apache.spark.storage.StorageLevel level) ;
   49    /**
   50:    * A special case of {@link fetchBlocks}, as it fetches only one block and is blocking.
   51     * <p>
   52     * It is also only available after {@link init} is invoked.
   ..
   57     * @return (undocumented)
   58     */
   59:   public  org.apache.spark.network.buffer.ManagedBuffer fetchBlockSync (java.lang.String host, int port, java.lang.String execId, java.lang.String blockId) { throw new RuntimeException(); }
   60    /**
   61     * Upload a single block to a remote node, available only after {@link init} is invoked.

/home/daniar/documents/SPARK/spark-1.6.1/core/target/java/org/apache/spark/network/netty/NettyBlockTransferService.java:
   11    /** Creates and binds the TransportServer, possibly trying multiple ports. */
   12    private  org.apache.spark.network.server.TransportServer createServer (scala.collection.immutable.List<org.apache.spark.network.server.TransportServerBootstrap> bootstraps) { throw new RuntimeException(); }
   13:   public  void fetchBlocks (java.lang.String host, int port, java.lang.String execId, java.lang.String[] blockIds, org.apache.spark.network.shuffle.BlockFetchingListener listener) { throw new RuntimeException(); }
   14    public  java.lang.String hostName () { throw new RuntimeException(); }
   15    public  int port () { throw new RuntimeException(); }

/home/daniar/documents/SPARK/spark-1.6.1/core/target/streams/compile/compile/$global/streams/out:
    <binary>


/home/daniar/documents/SPARK/spark-1.6.1/network/shuffle/src/main/java/org/apache/spark/network/shuffle/ExternalShuffleClient.java:
   89  
   90    @Override
   91:   public void fetchBlocks(
   92        final String host,
   93        final int port,
   ..
  117        }
  118      } catch (Exception e) {
  119:       logger.error("Exception while beginning fetchBlocks", e);
  120        for (String blockId : blockIds) {
  121          listener.onBlockFetchFailure(blockId, e);

/home/daniar/documents/SPARK/spark-1.6.1/network/shuffle/src/main/java/org/apache/spark/network/shuffle/ShuffleClient.java:
   36     * the data of a block is fetched, rather than waiting for all blocks to be fetched.
   37     */
   38:   public abstract void fetchBlocks(
   39        String host,
   40        int port,

/home/daniar/documents/SPARK/spark-1.6.1/network/shuffle/src/test/java/org/apache/spark/network/shuffle/ExternalShuffleIntegrationSuite.java:
  123  
  124    // Fetch a set of blocks from a pre-registered executor.
  125:   private FetchResult fetchBlocks(String execId, String[] blockIds) throws Exception {
  126:     return fetchBlocks(execId, blockIds, server.getPort());
  127    }
  128  
  129    // Fetch a set of blocks from a pre-registered executor. Connects to the server on the given port,
  130    // to allow connecting to invalid servers.
  131:   private FetchResult fetchBlocks(String execId, String[] blockIds, int port) throws Exception {
  132      final FetchResult res = new FetchResult();
  133      res.successBlocks = Collections.synchronizedSet(new HashSet<String>());
  ...
  139      ExternalShuffleClient client = new ExternalShuffleClient(conf, null, false, false);
  140      client.init(APP_ID);
  141:     client.fetchBlocks(TestUtils.getLocalHost(), port, execId, blockIds,
  142        new BlockFetchingListener() {
  143          @Override
  ...
  174    public void testFetchOneSort() throws Exception {
  175      registerExecutor("exec-0", dataContext0.createExecutorInfo(SORT_MANAGER));
  176:     FetchResult exec0Fetch = fetchBlocks("exec-0", new String[] { "shuffle_0_0_0" });
  177      assertEquals(Sets.newHashSet("shuffle_0_0_0"), exec0Fetch.successBlocks);
  178      assertTrue(exec0Fetch.failedBlocks.isEmpty());
  ...
  184    public void testFetchThreeSort() throws Exception {
  185      registerExecutor("exec-0", dataContext0.createExecutorInfo(SORT_MANAGER));
  186:     FetchResult exec0Fetch = fetchBlocks("exec-0",
  187        new String[] { "shuffle_0_0_0", "shuffle_0_0_1", "shuffle_0_0_2" });
  188      assertEquals(Sets.newHashSet("shuffle_0_0_0", "shuffle_0_0_1", "shuffle_0_0_2"),
  ...
  196    public void testFetchHash() throws Exception {
  197      registerExecutor("exec-1", dataContext1.createExecutorInfo(HASH_MANAGER));
  198:     FetchResult execFetch = fetchBlocks("exec-1",
  199        new String[] { "shuffle_1_0_0", "shuffle_1_0_1" });
  200      assertEquals(Sets.newHashSet("shuffle_1_0_0", "shuffle_1_0_1"), execFetch.successBlocks);
  ...
  207    public void testFetchWrongShuffle() throws Exception {
  208      registerExecutor("exec-1", dataContext1.createExecutorInfo(SORT_MANAGER /* wrong manager */));
  209:     FetchResult execFetch = fetchBlocks("exec-1",
  210        new String[] { "shuffle_1_0_0", "shuffle_1_0_1" });
  211      assertTrue(execFetch.successBlocks.isEmpty());
  ...
  216    public void testFetchInvalidShuffle() throws Exception {
  217      registerExecutor("exec-1", dataContext1.createExecutorInfo("unknown sort manager"));
  218:     FetchResult execFetch = fetchBlocks("exec-1",
  219        new String[] { "shuffle_1_0_0" });
  220      assertTrue(execFetch.successBlocks.isEmpty());
  ...
  225    public void testFetchWrongBlockId() throws Exception {
  226      registerExecutor("exec-1", dataContext1.createExecutorInfo(SORT_MANAGER /* wrong manager */));
  227:     FetchResult execFetch = fetchBlocks("exec-1",
  228        new String[] { "rdd_1_0_0" });
  229      assertTrue(execFetch.successBlocks.isEmpty());
  ...
  234    public void testFetchNonexistent() throws Exception {
  235      registerExecutor("exec-0", dataContext0.createExecutorInfo(SORT_MANAGER));
  236:     FetchResult execFetch = fetchBlocks("exec-0",
  237        new String[] { "shuffle_2_0_0" });
  238      assertTrue(execFetch.successBlocks.isEmpty());
  ...
  243    public void testFetchWrongExecutor() throws Exception {
  244      registerExecutor("exec-0", dataContext0.createExecutorInfo(SORT_MANAGER));
  245:     FetchResult execFetch = fetchBlocks("exec-0",
  246        new String[] { "shuffle_0_0_0" /* right */, "shuffle_1_0_0" /* wrong */ });
  247      // Both still fail, as we start by checking for all block.
  ...
  253    public void testFetchUnregisteredExecutor() throws Exception {
  254      registerExecutor("exec-0", dataContext0.createExecutorInfo(SORT_MANAGER));
  255:     FetchResult execFetch = fetchBlocks("exec-2",
  256        new String[] { "shuffle_0_0_0", "shuffle_1_0_0" });
  257      assertTrue(execFetch.successBlocks.isEmpty());
  ...
  264      try {
  265        registerExecutor("exec-0", dataContext0.createExecutorInfo(SORT_MANAGER));
  266:       FetchResult execFetch = fetchBlocks("exec-0",
  267          new String[]{"shuffle_1_0_0", "shuffle_1_0_1"}, 1 /* port */);
  268        assertTrue(execFetch.successBlocks.isEmpty());

/home/daniar/documents/SPARK/spark-1.6.1/network/shuffle/src/test/java/org/apache/spark/network/shuffle/OneForOneBlockFetcherSuite.java:
   56      blocks.put("shuffle_0_0_0", new NioManagedBuffer(ByteBuffer.wrap(new byte[0])));
   57  
   58:     BlockFetchingListener listener = fetchBlocks(blocks);
   59  
   60      verify(listener).onBlockFetchSuccess("shuffle_0_0_0", blocks.get("shuffle_0_0_0"));
   ..
   68      blocks.put("b2", new NettyManagedBuffer(Unpooled.wrappedBuffer(new byte[23])));
   69  
   70:     BlockFetchingListener listener = fetchBlocks(blocks);
   71  
   72      for (int i = 0; i < 3; i ++) {
   ..
   82      blocks.put("b2", null);
   83  
   84:     BlockFetchingListener listener = fetchBlocks(blocks);
   85  
   86      // Each failure will cause a failure to be invoked in all remaining block fetches.
   ..
   97      blocks.put("b2", new NioManagedBuffer(ByteBuffer.wrap(new byte[21])));
   98  
   99:     BlockFetchingListener listener = fetchBlocks(blocks);
  100  
  101      // We may call both success and failure for the same block.
  ...
  109    public void testEmptyBlockFetch() {
  110      try {
  111:       fetchBlocks(Maps.<String, ManagedBuffer>newLinkedHashMap());
  112        fail();
  113      } catch (IllegalArgumentException e) {
  ...
  124     * If a block's buffer is "null", an exception will be thrown instead.
  125     */
  126:   private BlockFetchingListener fetchBlocks(final LinkedHashMap<String, ManagedBuffer> blocks) {
  127      TransportClient client = mock(TransportClient.class);
  128      BlockFetchingListener listener = mock(BlockFetchingListener.class);

309 matches across 50 files
