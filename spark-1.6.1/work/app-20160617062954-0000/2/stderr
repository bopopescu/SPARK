Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
16/06/17 06:29:55 INFO CoarseGrainedExecutorBackend: Registered signal handlers for [TERM, HUP, INT]
16/06/17 06:29:56 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
16/06/17 06:29:56 INFO SecurityManager: Changing view acls to: daniar
16/06/17 06:29:56 INFO SecurityManager: Changing modify acls to: daniar
16/06/17 06:29:56 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(daniar); users with modify permissions: Set(daniar)
16/06/17 06:29:57 INFO SecurityManager: Changing view acls to: daniar
16/06/17 06:29:57 INFO SecurityManager: Changing modify acls to: daniar
16/06/17 06:29:57 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(daniar); users with modify permissions: Set(daniar)
16/06/17 06:29:58 INFO Slf4jLogger: Slf4jLogger started
16/06/17 06:29:58 INFO Remoting: Starting remoting
16/06/17 06:29:58 INFO Remoting: Remoting started; listening on addresses :[akka.tcp://sparkExecutorActorSystem@192.168.1.12:53547]
16/06/17 06:29:58 INFO Utils: Successfully started service 'sparkExecutorActorSystem' on port 53547.
16/06/17 06:29:59 INFO DiskBlockManager: Created local directory at /tmp/spark-c628b08d-3925-41a0-829b-4a878fe8e490/executor-33d2aef4-de22-4a54-bebf-80b24606555e/blockmgr-362b7507-59bb-47d1-8ebc-d4914e618e98
16/06/17 06:29:59 INFO MemoryStore: MemoryStore started with capacity 511.1 MB
16/06/17 06:29:59 INFO CoarseGrainedExecutorBackend: Connecting to driver: spark://CoarseGrainedScheduler@192.168.1.12:42879
16/06/17 06:29:59 INFO CoarseGrainedExecutorBackend: Daniarrrr
16/06/17 06:29:59 INFO WorkerWatcher: Connecting to worker spark://Worker@192.168.1.12:53723
16/06/17 06:30:00 INFO CoarseGrainedExecutorBackend: Successfully registered with driver
16/06/17 06:30:00 INFO Executor: Starting executor ID 2 on host 192.168.1.3
16/06/17 06:30:00 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 51942.
16/06/17 06:30:00 INFO NettyBlockTransferService: Server created on 51942
16/06/17 06:30:00 INFO BlockManagerMaster: Trying to register BlockManager
16/06/17 06:30:00 INFO BlockManagerMaster: Registered BlockManager
16/06/17 06:30:00 INFO CoarseGrainedExecutorBackend: Got assigned task 2
16/06/17 06:30:00 INFO CoarseGrainedExecutorBackend: Got assigned task 3
16/06/17 06:30:00 INFO Executor: Running task 2.0 in stage 0.0 (TID 2)
16/06/17 06:30:00 INFO Executor: Running task 3.0 in stage 0.0 (TID 3)
16/06/17 06:30:00 INFO Executor: Fetching http://192.168.1.12:55279/files/sort.py with timestamp 1466119793814
16/06/17 06:30:00 INFO Utils: Fetching http://192.168.1.12:55279/files/sort.py to /tmp/spark-c628b08d-3925-41a0-829b-4a878fe8e490/executor-33d2aef4-de22-4a54-bebf-80b24606555e/spark-75e2d941-c064-40d5-b0e1-5cdb087a58d2/fetchFileTemp6987530791323896718.tmp
16/06/17 06:30:00 INFO Utils: Copying /tmp/spark-c628b08d-3925-41a0-829b-4a878fe8e490/executor-33d2aef4-de22-4a54-bebf-80b24606555e/spark-75e2d941-c064-40d5-b0e1-5cdb087a58d2/-18220874931466119793814_cache to /home/daniar/documents/SPARK/spark-1.6.1/work/app-20160617062954-0000/2/./sort.py
16/06/17 06:30:00 INFO TorrentBroadcast: Started reading broadcast variable 1
16/06/17 06:30:01 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 3.7 KB, free 3.7 KB)
16/06/17 06:30:01 INFO TorrentBroadcast: Reading broadcast variable 1 took 388 ms
16/06/17 06:30:01 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 6.1 KB, free 9.8 KB)
16/06/17 06:30:01 INFO HadoopRDD: Input split: file:/home/daniar/documents/SPARK/generated_file/list_int:9611331+3203777
16/06/17 06:30:01 INFO HadoopRDD: Input split: file:/home/daniar/documents/SPARK/generated_file/list_int:6407554+3203777
16/06/17 06:30:01 INFO TorrentBroadcast: Started reading broadcast variable 0
16/06/17 06:30:01 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 21.8 KB, free 31.6 KB)
16/06/17 06:30:01 INFO TorrentBroadcast: Reading broadcast variable 0 took 10 ms
16/06/17 06:30:01 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 315.4 KB, free 347.0 KB)
16/06/17 06:30:02 INFO deprecation: mapred.tip.id is deprecated. Instead, use mapreduce.task.id
16/06/17 06:30:02 INFO deprecation: mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id
16/06/17 06:30:02 INFO deprecation: mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap
16/06/17 06:30:02 INFO deprecation: mapred.task.partition is deprecated. Instead, use mapreduce.task.partition
16/06/17 06:30:02 INFO deprecation: mapred.job.id is deprecated. Instead, use mapreduce.job.id
16/06/17 06:30:05 INFO PythonRunner: Times: total = 2697, boot = 469, init = 107, finish = 2121
16/06/17 06:30:05 INFO Executor: Finished task 3.0 in stage 0.0 (TID 3). 2129 bytes result sent to driver
16/06/17 06:30:05 INFO PythonRunner: Times: total = 2761, boot = 466, init = 76, finish = 2219
16/06/17 06:30:05 INFO Executor: Finished task 2.0 in stage 0.0 (TID 2). 2129 bytes result sent to driver
16/06/17 06:30:05 INFO CoarseGrainedExecutorBackend: Got assigned task 8
16/06/17 06:30:05 INFO Executor: Running task 8.0 in stage 0.0 (TID 8)
16/06/17 06:30:05 INFO CoarseGrainedExecutorBackend: Got assigned task 9
16/06/17 06:30:05 INFO Executor: Running task 9.0 in stage 0.0 (TID 9)
16/06/17 06:30:05 INFO HadoopRDD: Input split: file:/home/daniar/documents/SPARK/generated_file/list_int:28833993+3203777
16/06/17 06:30:05 INFO HadoopRDD: Input split: file:/home/daniar/documents/SPARK/generated_file/list_int:25630216+3203777
16/06/17 06:30:07 INFO PythonRunner: Times: total = 1636, boot = -142, init = 172, finish = 1606
16/06/17 06:30:07 INFO Executor: Finished task 9.0 in stage 0.0 (TID 9). 2129 bytes result sent to driver
16/06/17 06:30:07 INFO CoarseGrainedExecutorBackend: Got assigned task 12
16/06/17 06:30:07 INFO Executor: Running task 12.0 in stage 0.0 (TID 12)
16/06/17 06:30:07 INFO HadoopRDD: Input split: file:/home/daniar/documents/SPARK/generated_file/list_int:38445324+3203777
16/06/17 06:30:07 INFO PythonRunner: Times: total = 1944, boot = -83, init = 135, finish = 1892
16/06/17 06:30:07 INFO Executor: Finished task 8.0 in stage 0.0 (TID 8). 2129 bytes result sent to driver
16/06/17 06:30:07 INFO CoarseGrainedExecutorBackend: Got assigned task 14
16/06/17 06:30:07 INFO Executor: Running task 14.0 in stage 0.0 (TID 14)
16/06/17 06:30:07 INFO HadoopRDD: Input split: file:/home/daniar/documents/SPARK/generated_file/list_int:44852878+3203777
16/06/17 06:30:08 INFO PythonRunner: Times: total = 1580, boot = -3, init = 39, finish = 1544
16/06/17 06:30:08 INFO Executor: Finished task 12.0 in stage 0.0 (TID 12). 2129 bytes result sent to driver
16/06/17 06:30:08 INFO CoarseGrainedExecutorBackend: Got assigned task 18
16/06/17 06:30:08 INFO Executor: Running task 18.0 in stage 0.0 (TID 18)
16/06/17 06:30:08 INFO HadoopRDD: Input split: file:/home/daniar/documents/SPARK/generated_file/list_int:57667986+3203777
16/06/17 06:30:09 INFO PythonRunner: Times: total = 1907, boot = -29, init = 33, finish = 1903
16/06/17 06:30:09 INFO Executor: Finished task 14.0 in stage 0.0 (TID 14). 2129 bytes result sent to driver
16/06/17 06:30:09 INFO CoarseGrainedExecutorBackend: Got assigned task 21
16/06/17 06:30:09 INFO Executor: Running task 21.0 in stage 0.0 (TID 21)
16/06/17 06:30:09 INFO HadoopRDD: Input split: file:/home/daniar/documents/SPARK/generated_file/list_int:67279317+3203777
16/06/17 06:30:10 INFO PythonRunner: Times: total = 1797, boot = -16, init = 27, finish = 1786
16/06/17 06:30:10 INFO Executor: Finished task 18.0 in stage 0.0 (TID 18). 2129 bytes result sent to driver
16/06/17 06:30:10 INFO CoarseGrainedExecutorBackend: Got assigned task 24
16/06/17 06:30:10 INFO Executor: Running task 24.0 in stage 0.0 (TID 24)
16/06/17 06:30:10 INFO HadoopRDD: Input split: file:/home/daniar/documents/SPARK/generated_file/list_int:76890648+3203777
16/06/17 06:30:11 INFO PythonRunner: Times: total = 1772, boot = -37, init = 71, finish = 1738
16/06/17 06:30:11 INFO Executor: Finished task 21.0 in stage 0.0 (TID 21). 2129 bytes result sent to driver
16/06/17 06:30:11 INFO CoarseGrainedExecutorBackend: Got assigned task 26
16/06/17 06:30:11 INFO Executor: Running task 26.0 in stage 0.0 (TID 26)
16/06/17 06:30:11 INFO HadoopRDD: Input split: file:/home/daniar/documents/SPARK/generated_file/list_int:83298202+3203777
16/06/17 06:30:13 INFO PythonRunner: Times: total = 2612, boot = 3, init = 5, finish = 2604
16/06/17 06:30:13 INFO Executor: Finished task 24.0 in stage 0.0 (TID 24). 2129 bytes result sent to driver
16/06/17 06:30:13 INFO CoarseGrainedExecutorBackend: Got assigned task 30
16/06/17 06:30:13 INFO Executor: Running task 30.0 in stage 0.0 (TID 30)
16/06/17 06:30:13 INFO HadoopRDD: Input split: file:/home/daniar/documents/SPARK/generated_file/list_int:96113310+3203777
16/06/17 06:30:14 INFO PythonRunner: Times: total = 3416, boot = -13, init = 30, finish = 3399
16/06/17 06:30:14 INFO Executor: Finished task 26.0 in stage 0.0 (TID 26). 2129 bytes result sent to driver
16/06/17 06:30:14 INFO CoarseGrainedExecutorBackend: Got assigned task 32
16/06/17 06:30:14 INFO Executor: Running task 32.0 in stage 0.0 (TID 32)
16/06/17 06:30:14 INFO HadoopRDD: Input split: file:/home/daniar/documents/SPARK/generated_file/list_int:102520864+3203777
16/06/17 06:30:16 INFO PythonRunner: Times: total = 2700, boot = -45, init = 59, finish = 2686
16/06/17 06:30:16 INFO Executor: Finished task 30.0 in stage 0.0 (TID 30). 2129 bytes result sent to driver
16/06/17 06:30:16 INFO CoarseGrainedExecutorBackend: Got assigned task 36
16/06/17 06:30:16 INFO Executor: Running task 36.0 in stage 0.0 (TID 36)
16/06/17 06:30:16 INFO HadoopRDD: Input split: file:/home/daniar/documents/SPARK/generated_file/list_int:115335972+3203777
16/06/17 06:30:16 INFO PythonRunner: Times: total = 2131, boot = -8, init = 14, finish = 2125
16/06/17 06:30:16 INFO Executor: Finished task 32.0 in stage 0.0 (TID 32). 2129 bytes result sent to driver
16/06/17 06:30:16 INFO CoarseGrainedExecutorBackend: Got assigned task 39
16/06/17 06:30:16 INFO Executor: Running task 39.0 in stage 0.0 (TID 39)
16/06/17 06:30:16 INFO HadoopRDD: Input split: file:/home/daniar/documents/SPARK/generated_file/list_int:124947303+3203787
16/06/17 06:30:17 INFO PythonRunner: Times: total = 1672, boot = -12, init = 17, finish = 1667
16/06/17 06:30:17 INFO Executor: Finished task 36.0 in stage 0.0 (TID 36). 2129 bytes result sent to driver
16/06/17 06:30:18 INFO PythonRunner: Times: total = 1185, boot = -9, init = 22, finish = 1172
16/06/17 06:30:18 INFO Executor: Finished task 39.0 in stage 0.0 (TID 39). 2129 bytes result sent to driver
16/06/17 06:30:18 INFO CoarseGrainedExecutorBackend: Got assigned task 42
16/06/17 06:30:18 INFO CoarseGrainedExecutorBackend: Got assigned task 45
16/06/17 06:30:18 INFO Executor: Running task 2.0 in stage 1.0 (TID 42)
16/06/17 06:30:18 INFO TorrentBroadcast: Started reading broadcast variable 2
16/06/17 06:30:18 INFO Executor: Running task 5.0 in stage 1.0 (TID 45)
16/06/17 06:30:18 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 3.7 KB, free 350.7 KB)
16/06/17 06:30:18 INFO TorrentBroadcast: Reading broadcast variable 2 took 218 ms
16/06/17 06:30:18 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 5.9 KB, free 356.5 KB)
16/06/17 06:30:18 INFO HadoopRDD: Input split: file:/home/daniar/documents/SPARK/generated_file/list_int:6407554+3203777
16/06/17 06:30:18 INFO HadoopRDD: Input split: file:/home/daniar/documents/SPARK/generated_file/list_int:16018885+3203777
16/06/17 06:30:21 INFO PythonRunner: Times: total = 3090, boot = -920, init = 935, finish = 3075
16/06/17 06:30:21 INFO Executor: Finished task 5.0 in stage 1.0 (TID 45). 2286 bytes result sent to driver
16/06/17 06:30:21 INFO CoarseGrainedExecutorBackend: Got assigned task 50
16/06/17 06:30:21 INFO Executor: Running task 10.0 in stage 1.0 (TID 50)
16/06/17 06:30:21 INFO HadoopRDD: Input split: file:/home/daniar/documents/SPARK/generated_file/list_int:32037770+3203777
16/06/17 06:30:21 INFO PythonRunner: Times: total = 3256, boot = -554, init = 583, finish = 3227
16/06/17 06:30:21 INFO Executor: Finished task 2.0 in stage 1.0 (TID 42). 2275 bytes result sent to driver
16/06/17 06:30:21 INFO CoarseGrainedExecutorBackend: Got assigned task 51
16/06/17 06:30:21 INFO Executor: Running task 11.0 in stage 1.0 (TID 51)
16/06/17 06:30:21 INFO HadoopRDD: Input split: file:/home/daniar/documents/SPARK/generated_file/list_int:35241547+3203777
16/06/17 06:30:24 INFO PythonRunner: Times: total = 2617, boot = 6, init = 14, finish = 2597
16/06/17 06:30:24 INFO Executor: Finished task 11.0 in stage 1.0 (TID 51). 2248 bytes result sent to driver
16/06/17 06:30:24 INFO CoarseGrainedExecutorBackend: Got assigned task 54
16/06/17 06:30:24 INFO Executor: Running task 14.0 in stage 1.0 (TID 54)
16/06/17 06:30:24 INFO HadoopRDD: Input split: file:/home/daniar/documents/SPARK/generated_file/list_int:44852878+3203777
16/06/17 06:30:24 INFO PythonRunner: Times: total = 2871, boot = -2, init = 37, finish = 2836
16/06/17 06:30:24 INFO Executor: Finished task 10.0 in stage 1.0 (TID 50). 2344 bytes result sent to driver
16/06/17 06:30:24 INFO CoarseGrainedExecutorBackend: Got assigned task 56
16/06/17 06:30:24 INFO Executor: Running task 16.0 in stage 1.0 (TID 56)
16/06/17 06:30:24 INFO HadoopRDD: Input split: file:/home/daniar/documents/SPARK/generated_file/list_int:51260432+3203777
16/06/17 06:30:26 INFO PythonRunner: Times: total = 1812, boot = -7, init = 11, finish = 1808
16/06/17 06:30:26 INFO Executor: Finished task 16.0 in stage 1.0 (TID 56). 2238 bytes result sent to driver
16/06/17 06:30:26 INFO CoarseGrainedExecutorBackend: Got assigned task 60
16/06/17 06:30:26 INFO Executor: Running task 20.0 in stage 1.0 (TID 60)
16/06/17 06:30:26 INFO HadoopRDD: Input split: file:/home/daniar/documents/SPARK/generated_file/list_int:64075540+3203777
16/06/17 06:30:26 INFO PythonRunner: Times: total = 2288, boot = -1, init = 13, finish = 2276
16/06/17 06:30:26 INFO Executor: Finished task 14.0 in stage 1.0 (TID 54). 2314 bytes result sent to driver
16/06/17 06:30:26 INFO CoarseGrainedExecutorBackend: Got assigned task 63
16/06/17 06:30:26 INFO Executor: Running task 23.0 in stage 1.0 (TID 63)
16/06/17 06:30:26 INFO HadoopRDD: Input split: file:/home/daniar/documents/SPARK/generated_file/list_int:73686871+3203777
16/06/17 06:30:29 INFO PythonRunner: Times: total = 2706, boot = -16, init = 32, finish = 2690
16/06/17 06:30:29 INFO Executor: Finished task 20.0 in stage 1.0 (TID 60). 2281 bytes result sent to driver
16/06/17 06:30:29 INFO CoarseGrainedExecutorBackend: Got assigned task 65
16/06/17 06:30:29 INFO Executor: Running task 25.0 in stage 1.0 (TID 65)
16/06/17 06:30:29 INFO HadoopRDD: Input split: file:/home/daniar/documents/SPARK/generated_file/list_int:80094425+3203777
16/06/17 06:30:30 INFO PythonRunner: Times: total = 3171, boot = -11, init = 30, finish = 3152
16/06/17 06:30:30 INFO Executor: Finished task 23.0 in stage 1.0 (TID 63). 2253 bytes result sent to driver
16/06/17 06:30:30 INFO CoarseGrainedExecutorBackend: Got assigned task 69
16/06/17 06:30:30 INFO Executor: Running task 29.0 in stage 1.0 (TID 69)
16/06/17 06:30:30 INFO HadoopRDD: Input split: file:/home/daniar/documents/SPARK/generated_file/list_int:92909533+3203777
16/06/17 06:30:32 INFO PythonRunner: Times: total = 3344, boot = 10, init = 14, finish = 3320
16/06/17 06:30:32 INFO Executor: Finished task 25.0 in stage 1.0 (TID 65). 2324 bytes result sent to driver
16/06/17 06:30:32 INFO CoarseGrainedExecutorBackend: Got assigned task 72
16/06/17 06:30:32 INFO Executor: Running task 32.0 in stage 1.0 (TID 72)
16/06/17 06:30:32 INFO HadoopRDD: Input split: file:/home/daniar/documents/SPARK/generated_file/list_int:102520864+3203777
16/06/17 06:30:32 INFO PythonRunner: Times: total = 2786, boot = -12, init = 30, finish = 2768
16/06/17 06:30:32 INFO Executor: Finished task 29.0 in stage 1.0 (TID 69). 2233 bytes result sent to driver
16/06/17 06:30:32 INFO CoarseGrainedExecutorBackend: Got assigned task 74
16/06/17 06:30:32 INFO Executor: Running task 34.0 in stage 1.0 (TID 74)
16/06/17 06:30:32 INFO HadoopRDD: Input split: file:/home/daniar/documents/SPARK/generated_file/list_int:108928418+3203777
16/06/17 06:30:34 INFO PythonRunner: Times: total = 2131, boot = -5, init = 21, finish = 2115
16/06/17 06:30:34 INFO Executor: Finished task 32.0 in stage 1.0 (TID 72). 2253 bytes result sent to driver
16/06/17 06:30:34 INFO CoarseGrainedExecutorBackend: Got assigned task 77
16/06/17 06:30:34 INFO Executor: Running task 37.0 in stage 1.0 (TID 77)
16/06/17 06:30:34 INFO HadoopRDD: Input split: file:/home/daniar/documents/SPARK/generated_file/list_int:118539749+3203777
16/06/17 06:30:35 INFO PythonRunner: Times: total = 2268, boot = -13, init = 25, finish = 2256
16/06/17 06:30:35 INFO Executor: Finished task 34.0 in stage 1.0 (TID 74). 2286 bytes result sent to driver
16/06/17 06:30:36 INFO PythonRunner: Times: total = 1847, boot = 10, init = 9, finish = 1828
16/06/17 06:30:36 INFO Executor: Finished task 37.0 in stage 1.0 (TID 77). 2275 bytes result sent to driver
16/06/17 06:30:38 INFO CoarseGrainedExecutorBackend: Got assigned task 80
16/06/17 06:30:38 INFO Executor: Running task 0.0 in stage 2.0 (TID 80)
16/06/17 06:30:38 INFO CoarseGrainedExecutorBackend: Got assigned task 83
16/06/17 06:30:38 INFO Executor: Running task 3.0 in stage 2.0 (TID 83)
16/06/17 06:30:38 INFO TorrentBroadcast: Started reading broadcast variable 3
16/06/17 06:30:38 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 4.9 KB, free 361.4 KB)
16/06/17 06:30:38 INFO TorrentBroadcast: Reading broadcast variable 3 took 36 ms
16/06/17 06:30:38 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 7.7 KB, free 369.1 KB)
16/06/17 06:30:38 INFO HadoopRDD: Input split: file:/home/daniar/documents/SPARK/generated_file/list_int:0+3203777
16/06/17 06:30:38 INFO HadoopRDD: Input split: file:/home/daniar/documents/SPARK/generated_file/list_int:9611331+3203777
16/06/17 06:30:43 INFO PythonRunner: Times: total = 4575, boot = -2309, init = 2350, finish = 4534
16/06/17 06:30:43 INFO Executor: Finished task 3.0 in stage 2.0 (TID 83). 2355 bytes result sent to driver
16/06/17 06:30:43 INFO CoarseGrainedExecutorBackend: Got assigned task 89
16/06/17 06:30:43 INFO Executor: Running task 9.0 in stage 2.0 (TID 89)
16/06/17 06:30:43 INFO HadoopRDD: Input split: file:/home/daniar/documents/SPARK/generated_file/list_int:28833993+3203777
16/06/17 06:30:43 INFO PythonRunner: Times: total = 4793, boot = -3733, init = 3809, finish = 4717
16/06/17 06:30:43 INFO Executor: Finished task 0.0 in stage 2.0 (TID 80). 2355 bytes result sent to driver
16/06/17 06:30:43 INFO CoarseGrainedExecutorBackend: Got assigned task 91
16/06/17 06:30:43 INFO Executor: Running task 11.0 in stage 2.0 (TID 91)
16/06/17 06:30:43 INFO HadoopRDD: Input split: file:/home/daniar/documents/SPARK/generated_file/list_int:35241547+3203777
16/06/17 06:30:48 INFO PythonRunner: Times: total = 5222, boot = -90, init = 103, finish = 5209
16/06/17 06:30:48 INFO Executor: Finished task 9.0 in stage 2.0 (TID 89). 2355 bytes result sent to driver
16/06/17 06:30:48 INFO CoarseGrainedExecutorBackend: Got assigned task 96
16/06/17 06:30:48 INFO Executor: Running task 16.0 in stage 2.0 (TID 96)
16/06/17 06:30:48 INFO HadoopRDD: Input split: file:/home/daniar/documents/SPARK/generated_file/list_int:51260432+3203777
16/06/17 06:30:49 INFO PythonRunner: Times: total = 5196, boot = -111, init = 133, finish = 5174
16/06/17 06:30:49 INFO Executor: Finished task 11.0 in stage 2.0 (TID 91). 2355 bytes result sent to driver
16/06/17 06:30:49 INFO CoarseGrainedExecutorBackend: Got assigned task 97
16/06/17 06:30:49 INFO Executor: Running task 17.0 in stage 2.0 (TID 97)
16/06/17 06:30:49 INFO HadoopRDD: Input split: file:/home/daniar/documents/SPARK/generated_file/list_int:54464209+3203777
16/06/17 06:30:52 INFO PythonRunner: Times: total = 3688, boot = -55, init = 59, finish = 3684
16/06/17 06:30:52 INFO Executor: Finished task 16.0 in stage 2.0 (TID 96). 2355 bytes result sent to driver
16/06/17 06:30:52 INFO CoarseGrainedExecutorBackend: Got assigned task 101
16/06/17 06:30:52 INFO Executor: Running task 21.0 in stage 2.0 (TID 101)
16/06/17 06:30:52 INFO HadoopRDD: Input split: file:/home/daniar/documents/SPARK/generated_file/list_int:67279317+3203777
16/06/17 06:30:52 INFO PythonRunner: Times: total = 3575, boot = -50, init = 76, finish = 3549
16/06/17 06:30:52 INFO Executor: Finished task 17.0 in stage 2.0 (TID 97). 2355 bytes result sent to driver
16/06/17 06:30:52 INFO CoarseGrainedExecutorBackend: Got assigned task 102
16/06/17 06:30:52 INFO Executor: Running task 22.0 in stage 2.0 (TID 102)
16/06/17 06:30:52 INFO HadoopRDD: Input split: file:/home/daniar/documents/SPARK/generated_file/list_int:70483094+3203777
16/06/17 06:30:59 INFO PythonRunner: Times: total = 6499, boot = -135, init = 163, finish = 6471
16/06/17 06:30:59 INFO Executor: Finished task 22.0 in stage 2.0 (TID 102). 2355 bytes result sent to driver
16/06/17 06:30:59 INFO CoarseGrainedExecutorBackend: Got assigned task 106
16/06/17 06:30:59 INFO Executor: Running task 26.0 in stage 2.0 (TID 106)
16/06/17 06:30:59 INFO HadoopRDD: Input split: file:/home/daniar/documents/SPARK/generated_file/list_int:83298202+3203777
16/06/17 06:30:59 INFO PythonRunner: Times: total = 6949, boot = -92, init = 104, finish = 6937
16/06/17 06:30:59 INFO Executor: Finished task 21.0 in stage 2.0 (TID 101). 2355 bytes result sent to driver
16/06/17 06:30:59 INFO CoarseGrainedExecutorBackend: Got assigned task 108
16/06/17 06:30:59 INFO Executor: Running task 28.0 in stage 2.0 (TID 108)
16/06/17 06:30:59 INFO HadoopRDD: Input split: file:/home/daniar/documents/SPARK/generated_file/list_int:89705756+3203777
16/06/17 06:31:06 INFO PythonRunner: Times: total = 6759, boot = -109, init = 126, finish = 6742
16/06/17 06:31:06 INFO PythonRunner: Times: total = 6409, boot = -63, init = 80, finish = 6392
16/06/17 06:31:06 INFO Executor: Finished task 26.0 in stage 2.0 (TID 106). 2355 bytes result sent to driver
16/06/17 06:31:06 INFO CoarseGrainedExecutorBackend: Got assigned task 113
16/06/17 06:31:06 INFO Executor: Running task 33.0 in stage 2.0 (TID 113)
16/06/17 06:31:06 INFO HadoopRDD: Input split: file:/home/daniar/documents/SPARK/generated_file/list_int:105724641+3203777
16/06/17 06:31:06 INFO Executor: Finished task 28.0 in stage 2.0 (TID 108). 2355 bytes result sent to driver
16/06/17 06:31:06 INFO CoarseGrainedExecutorBackend: Got assigned task 114
16/06/17 06:31:06 INFO Executor: Running task 34.0 in stage 2.0 (TID 114)
16/06/17 06:31:06 INFO HadoopRDD: Input split: file:/home/daniar/documents/SPARK/generated_file/list_int:108928418+3203777
16/06/17 06:31:10 INFO PythonRunner: Times: total = 4135, boot = -64, init = 76, finish = 4123
16/06/17 06:31:10 INFO Executor: Finished task 33.0 in stage 2.0 (TID 113). 2355 bytes result sent to driver
16/06/17 06:31:10 INFO CoarseGrainedExecutorBackend: Got assigned task 117
16/06/17 06:31:10 INFO Executor: Running task 37.0 in stage 2.0 (TID 117)
16/06/17 06:31:10 INFO HadoopRDD: Input split: file:/home/daniar/documents/SPARK/generated_file/list_int:118539749+3203777
16/06/17 06:31:10 INFO PythonRunner: Times: total = 4200, boot = -90, init = 116, finish = 4174
16/06/17 06:31:10 INFO Executor: Finished task 34.0 in stage 2.0 (TID 114). 2355 bytes result sent to driver
16/06/17 06:31:10 INFO CoarseGrainedExecutorBackend: Got assigned task 119
16/06/17 06:31:10 INFO Executor: Running task 39.0 in stage 2.0 (TID 119)
16/06/17 06:31:10 INFO HadoopRDD: Input split: file:/home/daniar/documents/SPARK/generated_file/list_int:124947303+3203787
16/06/17 06:31:14 INFO PythonRunner: Times: total = 4093, boot = -24, init = 40, finish = 4077
16/06/17 06:31:14 INFO PythonRunner: Times: total = 4228, boot = 7, init = 5, finish = 4216
16/06/17 06:31:14 INFO Executor: Finished task 39.0 in stage 2.0 (TID 119). 2355 bytes result sent to driver
16/06/17 06:31:14 INFO Executor: Finished task 37.0 in stage 2.0 (TID 117). 2355 bytes result sent to driver
16/06/17 06:31:16 INFO CoarseGrainedExecutorBackend: Got assigned task 122
16/06/17 06:31:16 INFO CoarseGrainedExecutorBackend: Got assigned task 125
16/06/17 06:31:16 INFO Executor: Running task 2.0 in stage 3.0 (TID 122)
16/06/17 06:31:16 INFO Executor: Running task 5.0 in stage 3.0 (TID 125)
16/06/17 06:31:16 INFO MapOutputTrackerWorker: Updating epoch to 1 and clearing cache
16/06/17 06:31:16 INFO TorrentBroadcast: Started reading broadcast variable 4
16/06/17 06:31:16 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 27.2 KB, free 396.3 KB)
16/06/17 06:31:16 INFO TorrentBroadcast: Reading broadcast variable 4 took 19 ms
16/06/17 06:31:16 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 75.0 KB, free 471.3 KB)
16/06/17 06:31:16 INFO MapOutputTrackerWorker: Don't have map outputs for shuffle 0, fetching them
16/06/17 06:31:16 INFO MapOutputTrackerWorker: Doing the fetch; tracker endpoint = NettyRpcEndpointRef(spark://MapOutputTracker@192.168.1.12:42879)
16/06/17 06:31:16 INFO MapOutputTrackerWorker: Don't have map outputs for shuffle 0, fetching them
16/06/17 06:31:16 INFO MapOutputTrackerWorker: Got the output locations
16/06/17 06:31:16 INFO ShuffleBlockFetcherIterator: Getting 40 non-empty blocks out of 40 blocks
16/06/17 06:31:16 INFO ShuffleBlockFetcherIterator: Getting 40 non-empty blocks out of 40 blocks
16/06/17 06:31:16 INFO ShuffleBlockFetcherIterator: Started 2 remote fetches in 86 ms
16/06/17 06:31:16 INFO ShuffleBlockFetcherIterator: Started 2 remote fetches in 84 ms
16/06/17 06:31:18 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
16/06/17 06:31:18 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
16/06/17 06:31:22 INFO PythonRunner: Times: total = 5802, boot = -2041, init = 2087, finish = 5756
16/06/17 06:31:22 INFO FileOutputCommitter: Saved output of task 'attempt_201606170630_0003_m_000002_122' to file:/home/daniar/documents/SPARK/generated_file/result_py/_temporary/0/task_201606170630_0003_m_000002
16/06/17 06:31:22 INFO SparkHadoopMapRedUtil: attempt_201606170630_0003_m_000002_122: Committed
16/06/17 06:31:22 INFO Executor: Finished task 2.0 in stage 3.0 (TID 122). 2146 bytes result sent to driver
16/06/17 06:31:22 INFO CoarseGrainedExecutorBackend: Got assigned task 129
16/06/17 06:31:22 INFO Executor: Running task 9.0 in stage 3.0 (TID 129)
16/06/17 06:31:22 INFO ShuffleBlockFetcherIterator: Getting 40 non-empty blocks out of 40 blocks
16/06/17 06:31:22 INFO ShuffleBlockFetcherIterator: Started 2 remote fetches in 9 ms
16/06/17 06:31:24 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
16/06/17 06:31:24 INFO PythonRunner: Times: total = 7481, boot = -2024, init = 2067, finish = 7438
16/06/17 06:31:24 INFO FileOutputCommitter: Saved output of task 'attempt_201606170630_0003_m_000005_125' to file:/home/daniar/documents/SPARK/generated_file/result_py/_temporary/0/task_201606170630_0003_m_000005
16/06/17 06:31:24 INFO SparkHadoopMapRedUtil: attempt_201606170630_0003_m_000005_125: Committed
16/06/17 06:31:24 INFO Executor: Finished task 5.0 in stage 3.0 (TID 125). 2146 bytes result sent to driver
16/06/17 06:31:24 INFO CoarseGrainedExecutorBackend: Got assigned task 131
16/06/17 06:31:24 INFO Executor: Running task 11.0 in stage 3.0 (TID 131)
16/06/17 06:31:24 INFO ShuffleBlockFetcherIterator: Getting 40 non-empty blocks out of 40 blocks
16/06/17 06:31:24 INFO ShuffleBlockFetcherIterator: Started 2 remote fetches in 10 ms
16/06/17 06:31:25 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
16/06/17 06:31:26 INFO PythonRunner: Times: total = 3852, boot = -82, init = 105, finish = 3829
16/06/17 06:31:26 INFO FileOutputCommitter: Saved output of task 'attempt_201606170630_0003_m_000009_129' to file:/home/daniar/documents/SPARK/generated_file/result_py/_temporary/0/task_201606170630_0003_m_000009
16/06/17 06:31:26 INFO SparkHadoopMapRedUtil: attempt_201606170630_0003_m_000009_129: Committed
16/06/17 06:31:26 INFO Executor: Finished task 9.0 in stage 3.0 (TID 129). 2146 bytes result sent to driver
16/06/17 06:31:26 INFO CoarseGrainedExecutorBackend: Got assigned task 133
16/06/17 06:31:26 INFO Executor: Running task 13.0 in stage 3.0 (TID 133)
16/06/17 06:31:26 INFO ShuffleBlockFetcherIterator: Getting 40 non-empty blocks out of 40 blocks
16/06/17 06:31:26 INFO ShuffleBlockFetcherIterator: Started 2 remote fetches in 2 ms
16/06/17 06:31:27 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
16/06/17 06:31:29 INFO PythonRunner: Times: total = 4781, boot = -34, init = 36, finish = 4779
16/06/17 06:31:29 INFO FileOutputCommitter: Saved output of task 'attempt_201606170630_0003_m_000011_131' to file:/home/daniar/documents/SPARK/generated_file/result_py/_temporary/0/task_201606170630_0003_m_000011
16/06/17 06:31:29 INFO SparkHadoopMapRedUtil: attempt_201606170630_0003_m_000011_131: Committed
16/06/17 06:31:29 INFO Executor: Finished task 11.0 in stage 3.0 (TID 131). 2146 bytes result sent to driver
16/06/17 06:31:29 INFO CoarseGrainedExecutorBackend: Got assigned task 135
16/06/17 06:31:29 INFO Executor: Running task 15.0 in stage 3.0 (TID 135)
16/06/17 06:31:29 INFO ShuffleBlockFetcherIterator: Getting 40 non-empty blocks out of 40 blocks
16/06/17 06:31:29 INFO ShuffleBlockFetcherIterator: Started 2 remote fetches in 3 ms
16/06/17 06:31:30 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
16/06/17 06:31:30 INFO PythonRunner: Times: total = 4288, boot = -3, init = 22, finish = 4269
16/06/17 06:31:30 INFO FileOutputCommitter: Saved output of task 'attempt_201606170630_0003_m_000013_133' to file:/home/daniar/documents/SPARK/generated_file/result_py/_temporary/0/task_201606170630_0003_m_000013
16/06/17 06:31:30 INFO SparkHadoopMapRedUtil: attempt_201606170630_0003_m_000013_133: Committed
16/06/17 06:31:30 INFO Executor: Finished task 13.0 in stage 3.0 (TID 133). 2146 bytes result sent to driver
16/06/17 06:31:30 INFO CoarseGrainedExecutorBackend: Got assigned task 139
16/06/17 06:31:30 INFO Executor: Running task 19.0 in stage 3.0 (TID 139)
16/06/17 06:31:30 INFO ShuffleBlockFetcherIterator: Getting 40 non-empty blocks out of 40 blocks
16/06/17 06:31:30 INFO ShuffleBlockFetcherIterator: Started 2 remote fetches in 13 ms
16/06/17 06:31:32 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
16/06/17 06:31:33 INFO PythonRunner: Times: total = 4192, boot = -66, init = 83, finish = 4175
16/06/17 06:31:33 INFO FileOutputCommitter: Saved output of task 'attempt_201606170630_0003_m_000015_135' to file:/home/daniar/documents/SPARK/generated_file/result_py/_temporary/0/task_201606170630_0003_m_000015
16/06/17 06:31:33 INFO SparkHadoopMapRedUtil: attempt_201606170630_0003_m_000015_135: Committed
16/06/17 06:31:33 INFO Executor: Finished task 15.0 in stage 3.0 (TID 135). 2146 bytes result sent to driver
16/06/17 06:31:33 INFO CoarseGrainedExecutorBackend: Got assigned task 140
16/06/17 06:31:33 INFO Executor: Running task 20.0 in stage 3.0 (TID 140)
16/06/17 06:31:33 INFO ShuffleBlockFetcherIterator: Getting 40 non-empty blocks out of 40 blocks
16/06/17 06:31:33 INFO ShuffleBlockFetcherIterator: Started 2 remote fetches in 36 ms
16/06/17 06:31:35 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
16/06/17 06:31:35 INFO PythonRunner: Times: total = 4487, boot = -36, init = 38, finish = 4485
16/06/17 06:31:35 INFO FileOutputCommitter: Saved output of task 'attempt_201606170630_0003_m_000019_139' to file:/home/daniar/documents/SPARK/generated_file/result_py/_temporary/0/task_201606170630_0003_m_000019
16/06/17 06:31:35 INFO SparkHadoopMapRedUtil: attempt_201606170630_0003_m_000019_139: Committed
16/06/17 06:31:35 INFO Executor: Finished task 19.0 in stage 3.0 (TID 139). 2146 bytes result sent to driver
16/06/17 06:31:35 INFO CoarseGrainedExecutorBackend: Got assigned task 145
16/06/17 06:31:35 INFO Executor: Running task 25.0 in stage 3.0 (TID 145)
16/06/17 06:31:35 INFO ShuffleBlockFetcherIterator: Getting 40 non-empty blocks out of 40 blocks
16/06/17 06:31:35 INFO ShuffleBlockFetcherIterator: Started 2 remote fetches in 2 ms
16/06/17 06:31:38 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
16/06/17 06:31:38 INFO PythonRunner: Times: total = 5438, boot = -16, init = 17, finish = 5437
16/06/17 06:31:38 INFO FileOutputCommitter: Saved output of task 'attempt_201606170630_0003_m_000020_140' to file:/home/daniar/documents/SPARK/generated_file/result_py/_temporary/0/task_201606170630_0003_m_000020
16/06/17 06:31:38 INFO SparkHadoopMapRedUtil: attempt_201606170630_0003_m_000020_140: Committed
16/06/17 06:31:38 INFO Executor: Finished task 20.0 in stage 3.0 (TID 140). 2146 bytes result sent to driver
16/06/17 06:31:38 INFO CoarseGrainedExecutorBackend: Got assigned task 146
16/06/17 06:31:38 INFO Executor: Running task 26.0 in stage 3.0 (TID 146)
16/06/17 06:31:38 INFO ShuffleBlockFetcherIterator: Getting 40 non-empty blocks out of 40 blocks
16/06/17 06:31:38 INFO ShuffleBlockFetcherIterator: Started 2 remote fetches in 57 ms
16/06/17 06:31:41 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
16/06/17 06:31:47 INFO PythonRunner: Times: total = 8216, boot = -45, init = 99, finish = 8162
16/06/17 06:31:47 INFO FileOutputCommitter: Saved output of task 'attempt_201606170630_0003_m_000026_146' to file:/home/daniar/documents/SPARK/generated_file/result_py/_temporary/0/task_201606170630_0003_m_000026
16/06/17 06:31:47 INFO SparkHadoopMapRedUtil: attempt_201606170630_0003_m_000026_146: Committed
16/06/17 06:31:47 INFO Executor: Finished task 26.0 in stage 3.0 (TID 146). 2146 bytes result sent to driver
16/06/17 06:31:47 INFO CoarseGrainedExecutorBackend: Got assigned task 152
16/06/17 06:31:47 INFO Executor: Running task 32.0 in stage 3.0 (TID 152)
16/06/17 06:31:47 INFO ShuffleBlockFetcherIterator: Getting 40 non-empty blocks out of 40 blocks
16/06/17 06:31:47 INFO ShuffleBlockFetcherIterator: Started 2 remote fetches in 14 ms
16/06/17 06:31:47 INFO PythonRunner: Times: total = 12136, boot = 76, init = 0, finish = 12060
16/06/17 06:31:47 INFO FileOutputCommitter: Saved output of task 'attempt_201606170630_0003_m_000025_145' to file:/home/daniar/documents/SPARK/generated_file/result_py/_temporary/0/task_201606170630_0003_m_000025
16/06/17 06:31:47 INFO SparkHadoopMapRedUtil: attempt_201606170630_0003_m_000025_145: Committed
16/06/17 06:31:47 INFO Executor: Finished task 25.0 in stage 3.0 (TID 145). 2146 bytes result sent to driver
16/06/17 06:31:47 INFO CoarseGrainedExecutorBackend: Got assigned task 154
16/06/17 06:31:47 INFO Executor: Running task 34.0 in stage 3.0 (TID 154)
16/06/17 06:31:47 INFO ShuffleBlockFetcherIterator: Getting 40 non-empty blocks out of 40 blocks
16/06/17 06:31:47 INFO ShuffleBlockFetcherIterator: Started 2 remote fetches in 9 ms
16/06/17 06:31:49 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
16/06/17 06:31:50 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
16/06/17 06:31:54 INFO PythonRunner: Times: total = 6803, boot = -22, init = 24, finish = 6801
16/06/17 06:31:54 INFO FileOutputCommitter: Saved output of task 'attempt_201606170630_0003_m_000034_154' to file:/home/daniar/documents/SPARK/generated_file/result_py/_temporary/0/task_201606170630_0003_m_000034
16/06/17 06:31:54 INFO SparkHadoopMapRedUtil: attempt_201606170630_0003_m_000034_154: Committed
16/06/17 06:31:54 INFO Executor: Finished task 34.0 in stage 3.0 (TID 154). 2146 bytes result sent to driver
16/06/17 06:31:54 INFO CoarseGrainedExecutorBackend: Got assigned task 159
16/06/17 06:31:54 INFO Executor: Running task 39.0 in stage 3.0 (TID 159)
16/06/17 06:31:54 INFO ShuffleBlockFetcherIterator: Getting 40 non-empty blocks out of 40 blocks
16/06/17 06:31:54 INFO ShuffleBlockFetcherIterator: Started 2 remote fetches in 4 ms
16/06/17 06:31:56 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
16/06/17 06:31:57 INFO PythonRunner: Times: total = 10513, boot = -11, init = 22, finish = 10502
16/06/17 06:31:57 INFO FileOutputCommitter: Saved output of task 'attempt_201606170630_0003_m_000032_152' to file:/home/daniar/documents/SPARK/generated_file/result_py/_temporary/0/task_201606170630_0003_m_000032
16/06/17 06:31:57 INFO SparkHadoopMapRedUtil: attempt_201606170630_0003_m_000032_152: Committed
16/06/17 06:31:57 INFO Executor: Finished task 32.0 in stage 3.0 (TID 152). 2146 bytes result sent to driver
16/06/17 06:32:00 INFO PythonRunner: Times: total = 5717, boot = 9, init = 0, finish = 5708
16/06/17 06:32:00 INFO FileOutputCommitter: Saved output of task 'attempt_201606170630_0003_m_000039_159' to file:/home/daniar/documents/SPARK/generated_file/result_py/_temporary/0/task_201606170630_0003_m_000039
16/06/17 06:32:00 INFO SparkHadoopMapRedUtil: attempt_201606170630_0003_m_000039_159: Committed
16/06/17 06:32:00 INFO Executor: Finished task 39.0 in stage 3.0 (TID 159). 2146 bytes result sent to driver
16/06/17 06:32:00 INFO CoarseGrainedExecutorBackend: Driver commanded a shutdown
16/06/17 06:32:00 INFO MemoryStore: MemoryStore cleared
16/06/17 06:32:00 INFO BlockManager: BlockManager stopped
16/06/17 06:32:00 INFO RemoteActorRefProvider$RemotingTerminator: Shutting down remote daemon.
16/06/17 06:32:00 ERROR WorkerWatcher: Lost connection to worker rpc endpoint spark://Worker@192.168.1.12:53723. Exiting.
16/06/17 06:32:00 INFO RemoteActorRefProvider$RemotingTerminator: Remote daemon shut down; proceeding with flushing remote transports.
16/06/17 06:32:00 ERROR CoarseGrainedExecutorBackend: Driver 192.168.1.12:42879 disassociated! Shutting down.
