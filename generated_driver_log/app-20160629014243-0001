{"Event":"SparkListenerLogStart","Spark Version":"1.6.1"}
{"Event":"SparkListenerBlockManagerAdded","Block Manager ID":{"Executor ID":"driver","Host":"192.168.1.3","Port":54551},"Maximum Memory":535953408,"Timestamp":1467139363418}
{"Event":"SparkListenerEnvironmentUpdate","JVM Information":{"Java Home":"/usr/lib/jvm/java-8-oracle/jre","Java Version":"1.8.0_91 (Oracle Corporation)","Scala Version":"version 2.10.5"},"Spark Properties":{"spark.speculation":"true","spark.driver.host":"192.168.1.3","spark.serializer.objectStreamReset":"100","spark.eventLog.enabled":"true","spark.driver.maxResultSize":"1g","spark.driver.port":"38890","spark.rdd.compress":"True","spark.app.name":"Sort5","spark.scheduler.mode":"FIFO","spark.driver.memory":"1g","spark.executor.instances":"2","spark.files":"file:/home/daniar/documents/SPARK/spark-1.6.1/sort.py","spark.executor.id":"driver","spark.submit.deployMode":"client","spark.master":"spark://daniar-X450JF:7077","spark.executor.memory":"1g","spark.eventLog.dir":"../generated_driver_log/","spark.externalBlockStore.folderName":"spark-57dc2591-161e-426e-bdc4-16157232b0b8","spark.app.id":"app-20160629014243-0001"},"System Properties":{"java.io.tmpdir":"/tmp","line.separator":"\n","path.separator":":","sun.management.compiler":"HotSpot 64-Bit Tiered Compilers","SPARK_SUBMIT":"true","sun.cpu.endian":"little","java.specification.version":"1.8","java.vm.specification.name":"Java Virtual Machine Specification","java.vendor":"Oracle Corporation","java.vm.specification.version":"1.8","user.home":"/home/daniar","file.encoding.pkg":"sun.io","sun.nio.ch.bugLevel":"","sun.arch.data.model":"64","sun.boot.library.path":"/usr/lib/jvm/java-8-oracle/jre/lib/amd64","user.dir":"/home/daniar/documents/SPARK/spark-1.6.1","java.library.path":"/usr/java/packages/lib/amd64:/usr/lib64:/lib64:/lib:/usr/lib","sun.cpu.isalist":"","sun.desktop":"gnome","os.arch":"amd64","java.vm.version":"25.91-b14","java.endorsed.dirs":"/usr/lib/jvm/java-8-oracle/jre/lib/endorsed","java.runtime.version":"1.8.0_91-b14","java.vm.info":"mixed mode","java.ext.dirs":"/usr/lib/jvm/java-8-oracle/jre/lib/ext:/usr/java/packages/lib/ext","java.runtime.name":"Java(TM) SE Runtime Environment","file.separator":"/","java.class.version":"52.0","java.specification.name":"Java Platform API Specification","sun.boot.class.path":"/usr/lib/jvm/java-8-oracle/jre/lib/resources.jar:/usr/lib/jvm/java-8-oracle/jre/lib/rt.jar:/usr/lib/jvm/java-8-oracle/jre/lib/sunrsasign.jar:/usr/lib/jvm/java-8-oracle/jre/lib/jsse.jar:/usr/lib/jvm/java-8-oracle/jre/lib/jce.jar:/usr/lib/jvm/java-8-oracle/jre/lib/charsets.jar:/usr/lib/jvm/java-8-oracle/jre/lib/jfr.jar:/usr/lib/jvm/java-8-oracle/jre/classes","file.encoding":"UTF-8","user.timezone":"Asia/Jakarta","java.specification.vendor":"Oracle Corporation","sun.java.launcher":"SUN_STANDARD","os.version":"3.19.0-51-generic","sun.os.patch.level":"unknown","java.vm.specification.vendor":"Oracle Corporation","user.country":"US","sun.jnu.encoding":"UTF-8","user.language":"en","java.vendor.url":"http://java.oracle.com/","java.awt.printerjob":"sun.print.PSPrinterJob","java.awt.graphicsenv":"sun.awt.X11GraphicsEnvironment","awt.toolkit":"sun.awt.X11.XToolkit","os.name":"Linux","java.vm.vendor":"Oracle Corporation","java.vendor.url.bug":"http://bugreport.sun.com/bugreport/","user.name":"daniar","java.vm.name":"Java HotSpot(TM) 64-Bit Server VM","sun.java.command":"org.apache.spark.deploy.SparkSubmit sort.py --master spark://daniar-X450JF:7077 --deploy-mode cluster","java.home":"/usr/lib/jvm/java-8-oracle/jre","java.version":"1.8.0_91","sun.io.unicode.encoding":"UnicodeLittle"},"Classpath Entries":{"/home/daniar/documents/SPARK/spark-1.6.1/tools/target/scala-2.10/classes/":"System Classpath","/home/daniar/documents/SPARK/spark-1.6.1/launcher/target/scala-2.10/classes/":"System Classpath","/home/daniar/documents/SPARK/spark-1.6.1/lib_managed/jars/datanucleus-rdbms-3.2.9.jar":"System Classpath","/home/daniar/documents/SPARK/spark-1.6.1/network/shuffle/target/scala-2.10/classes/":"System Classpath","/home/daniar/documents/SPARK/spark-1.6.1/core/target/scala-2.10/classes/":"System Classpath","/home/daniar/documents/SPARK/spark-1.6.1/core/target/jars/*":"System Classpath","/home/daniar/documents/SPARK/spark-1.6.1/streaming/target/scala-2.10/classes/":"System Classpath","/home/daniar/documents/SPARK/spark-1.6.1/assembly/target/scala-2.10/spark-assembly-1.6.1-hadoop2.7.2.jar":"System Classpath","/home/daniar/documents/SPARK/spark-1.6.1/graphx/target/scala-2.10/classes/":"System Classpath","/home/daniar/documents/SPARK/spark-1.6.1/bagel/target/scala-2.10/classes/":"System Classpath","/home/daniar/documents/SPARK/spark-1.6.1/mllib/target/scala-2.10/classes/":"System Classpath","/home/daniar/documents/SPARK/spark-1.6.1/sql/hive-thriftserver/target/scala-2.10/classes":"System Classpath","/home/daniar/documents/SPARK/spark-1.6.1/lib_managed/jars/datanucleus-core-3.2.10.jar":"System Classpath","/home/daniar/documents/SPARK/spark-1.6.1/yarn/target/scala-2.10/classes/":"System Classpath","/home/daniar/documents/SPARK/spark-1.6.1/network/common/target/scala-2.10/classes/":"System Classpath","/home/daniar/documents/SPARK/spark-1.6.1/network/yarn/target/scala-2.10/classes/":"System Classpath","/home/daniar/documents/SPARK/spark-1.6.1/repl/target/scala-2.10/classes/":"System Classpath","http://192.168.1.3:55550/files/sort.py":"Added By User","/home/daniar/documents/SPARK/spark-1.6.1/sql/core/target/scala-2.10/classes/":"System Classpath","/home/daniar/documents/SPARK/spark-1.6.1/lib_managed/jars/datanucleus-api-jdo-3.2.6.jar":"System Classpath","/home/daniar/documents/SPARK/spark-1.6.1/conf/":"System Classpath","/home/daniar/documents/SPARK/spark-1.6.1/sql/hive/target/scala-2.10/classes/":"System Classpath","/home/daniar/documents/SPARK/spark-1.6.1/sql/catalyst/target/scala-2.10/classes/":"System Classpath"}}
{"Event":"SparkListenerApplicationStart","App Name":"Sort5","App ID":"app-20160629014243-0001","Timestamp":1467139361238,"User":"daniar"}
{"Event":"SparkListenerJobStart","Job ID":0,"Submission Time":1467139364373,"Stage Infos":[{"Stage ID":0,"Stage Attempt ID":0,"Stage Name":"sortByKey at /home/daniar/documents/SPARK/spark-1.6.1/sort.py:12","Number of Tasks":4,"RDD Info":[{"RDD ID":2,"Name":"PythonRDD","Callsite":"sortByKey at /home/daniar/documents/SPARK/spark-1.6.1/sort.py:12","Parent IDs":[1],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":4,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":0,"Name":"../generated_file/list_int","Scope":"{\"id\":\"0\",\"name\":\"textFile\"}","Callsite":"textFile at NativeMethodAccessorImpl.java:-2","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":4,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":1,"Name":"../generated_file/list_int","Scope":"{\"id\":\"0\",\"name\":\"textFile\"}","Callsite":"textFile at NativeMethodAccessorImpl.java:-2","Parent IDs":[0],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":4,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.collect(RDD.scala:926)\norg.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:405)\norg.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:498)\npy4j.reflection.MethodInvoker.invoke(MethodInvoker.java:231)\npy4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:381)\npy4j.Gateway.invoke(Gateway.java:259)\npy4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:133)\npy4j.commands.CallCommand.execute(CallCommand.java:79)\npy4j.GatewayConnection.run(GatewayConnection.java:209)\njava.lang.Thread.run(Thread.java:745)","Accumulables":[]}],"Stage IDs":[0],"Properties":{"spark.rdd.scope.noOverride":"true","spark.rdd.scope":"{\"id\":\"1\",\"name\":\"collect\"}","callSite.short":"sortByKey at /home/daniar/documents/SPARK/spark-1.6.1/sort.py:12"}}
{"Event":"SparkListenerStageSubmitted","Stage Info":{"Stage ID":0,"Stage Attempt ID":0,"Stage Name":"sortByKey at /home/daniar/documents/SPARK/spark-1.6.1/sort.py:12","Number of Tasks":4,"RDD Info":[{"RDD ID":2,"Name":"PythonRDD","Callsite":"sortByKey at /home/daniar/documents/SPARK/spark-1.6.1/sort.py:12","Parent IDs":[1],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":4,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":0,"Name":"../generated_file/list_int","Scope":"{\"id\":\"0\",\"name\":\"textFile\"}","Callsite":"textFile at NativeMethodAccessorImpl.java:-2","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":4,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":1,"Name":"../generated_file/list_int","Scope":"{\"id\":\"0\",\"name\":\"textFile\"}","Callsite":"textFile at NativeMethodAccessorImpl.java:-2","Parent IDs":[0],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":4,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.collect(RDD.scala:926)\norg.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:405)\norg.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:498)\npy4j.reflection.MethodInvoker.invoke(MethodInvoker.java:231)\npy4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:381)\npy4j.Gateway.invoke(Gateway.java:259)\npy4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:133)\npy4j.commands.CallCommand.execute(CallCommand.java:79)\npy4j.GatewayConnection.run(GatewayConnection.java:209)\njava.lang.Thread.run(Thread.java:745)","Submission Time":1467139364411,"Accumulables":[]},"Properties":{"spark.rdd.scope.noOverride":"true","spark.rdd.scope":"{\"id\":\"1\",\"name\":\"collect\"}","callSite.short":"sortByKey at /home/daniar/documents/SPARK/spark-1.6.1/sort.py:12"}}
{"Event":"SparkListenerExecutorAdded","Timestamp":1467139366158,"Executor ID":"0","Executor Info":{"Host":"192.168.1.3","Total Cores":8,"Log Urls":{"stdout":"http://192.168.1.3:8081/logPage/?appId=app-20160629014243-0001&executorId=0&logType=stdout","stderr":"http://192.168.1.3:8081/logPage/?appId=app-20160629014243-0001&executorId=0&logType=stderr"}}}
{"Event":"SparkListenerTaskStart","Stage ID":0,"Stage Attempt ID":0,"Task Info":{"Task ID":0,"Index":0,"Attempt":0,"Launch Time":1467139366161,"Executor ID":"0","Host":"192.168.1.3","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":0,"Failed":false,"Accumulables":[]}}
{"Event":"SparkListenerTaskStart","Stage ID":0,"Stage Attempt ID":0,"Task Info":{"Task ID":1,"Index":1,"Attempt":0,"Launch Time":1467139366181,"Executor ID":"0","Host":"192.168.1.3","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":0,"Failed":false,"Accumulables":[]}}
{"Event":"SparkListenerTaskStart","Stage ID":0,"Stage Attempt ID":0,"Task Info":{"Task ID":2,"Index":2,"Attempt":0,"Launch Time":1467139366182,"Executor ID":"0","Host":"192.168.1.3","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":0,"Failed":false,"Accumulables":[]}}
{"Event":"SparkListenerTaskStart","Stage ID":0,"Stage Attempt ID":0,"Task Info":{"Task ID":3,"Index":3,"Attempt":0,"Launch Time":1467139366182,"Executor ID":"0","Host":"192.168.1.3","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":0,"Failed":false,"Accumulables":[]}}
{"Event":"SparkListenerExecutorAdded","Timestamp":1467139366210,"Executor ID":"1","Executor Info":{"Host":"192.168.1.3","Total Cores":8,"Log Urls":{"stdout":"http://192.168.1.3:8082/logPage/?appId=app-20160629014243-0001&executorId=1&logType=stdout","stderr":"http://192.168.1.3:8082/logPage/?appId=app-20160629014243-0001&executorId=1&logType=stderr"}}}
{"Event":"SparkListenerBlockManagerAdded","Block Manager ID":{"Executor ID":"1","Host":"192.168.1.11","Port":35409},"Maximum Memory":535953408,"Timestamp":1467139366261}
{"Event":"SparkListenerBlockManagerAdded","Block Manager ID":{"Executor ID":"0","Host":"192.168.1.11","Port":33800},"Maximum Memory":535953408,"Timestamp":1467139366291}
{"Event":"SparkListenerTaskEnd","Stage ID":0,"Stage Attempt ID":0,"Task Type":"ResultTask","Task End Reason":{"Reason":"Success"},"Task Info":{"Task ID":3,"Index":3,"Attempt":0,"Launch Time":1467139366182,"Executor ID":"0","Host":"192.168.1.3","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":1467139379498,"Failed":false,"Accumulables":[]},"Task Metrics":{"Host Name":"192.168.1.11","Executor Deserialize Time":539,"Executor Run Time":12594,"Result Size":2130,"JVM GC Time":172,"Result Serialization Time":0,"Memory Bytes Spilled":0,"Disk Bytes Spilled":0,"Input Metrics":{"Data Read Method":"Hadoop","Bytes Read":65536,"Records Read":3484461}}}
{"Event":"SparkListenerTaskEnd","Stage ID":0,"Stage Attempt ID":0,"Task Type":"ResultTask","Task End Reason":{"Reason":"Success"},"Task Info":{"Task ID":1,"Index":1,"Attempt":0,"Launch Time":1467139366181,"Executor ID":"0","Host":"192.168.1.3","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":1467139381147,"Failed":false,"Accumulables":[]},"Task Metrics":{"Host Name":"192.168.1.11","Executor Deserialize Time":564,"Executor Run Time":14244,"Result Size":2130,"JVM GC Time":172,"Result Serialization Time":0,"Memory Bytes Spilled":0,"Disk Bytes Spilled":0,"Input Metrics":{"Data Read Method":"Hadoop","Bytes Read":65536,"Records Read":4253268}}}
{"Event":"SparkListenerTaskEnd","Stage ID":0,"Stage Attempt ID":0,"Task Type":"ResultTask","Task End Reason":{"Reason":"Success"},"Task Info":{"Task ID":2,"Index":2,"Attempt":0,"Launch Time":1467139366182,"Executor ID":"0","Host":"192.168.1.3","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":1467139382127,"Failed":false,"Accumulables":[]},"Task Metrics":{"Host Name":"192.168.1.11","Executor Deserialize Time":567,"Executor Run Time":15225,"Result Size":2130,"JVM GC Time":172,"Result Serialization Time":0,"Memory Bytes Spilled":0,"Disk Bytes Spilled":0,"Input Metrics":{"Data Read Method":"Hadoop","Bytes Read":65536,"Records Read":4253236}}}
{"Event":"SparkListenerTaskEnd","Stage ID":0,"Stage Attempt ID":0,"Task Type":"ResultTask","Task End Reason":{"Reason":"Success"},"Task Info":{"Task ID":0,"Index":0,"Attempt":0,"Launch Time":1467139366161,"Executor ID":"0","Host":"192.168.1.3","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":1467139383261,"Failed":false,"Accumulables":[]},"Task Metrics":{"Host Name":"192.168.1.11","Executor Deserialize Time":566,"Executor Run Time":16360,"Result Size":2130,"JVM GC Time":174,"Result Serialization Time":0,"Memory Bytes Spilled":0,"Disk Bytes Spilled":0,"Input Metrics":{"Data Read Method":"Hadoop","Bytes Read":0,"Records Read":4253453}}}
{"Event":"SparkListenerStageCompleted","Stage Info":{"Stage ID":0,"Stage Attempt ID":0,"Stage Name":"sortByKey at /home/daniar/documents/SPARK/spark-1.6.1/sort.py:12","Number of Tasks":4,"RDD Info":[{"RDD ID":2,"Name":"PythonRDD","Callsite":"sortByKey at /home/daniar/documents/SPARK/spark-1.6.1/sort.py:12","Parent IDs":[1],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":4,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":0,"Name":"../generated_file/list_int","Scope":"{\"id\":\"0\",\"name\":\"textFile\"}","Callsite":"textFile at NativeMethodAccessorImpl.java:-2","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":4,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":1,"Name":"../generated_file/list_int","Scope":"{\"id\":\"0\",\"name\":\"textFile\"}","Callsite":"textFile at NativeMethodAccessorImpl.java:-2","Parent IDs":[0],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":4,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.collect(RDD.scala:926)\norg.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:405)\norg.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:498)\npy4j.reflection.MethodInvoker.invoke(MethodInvoker.java:231)\npy4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:381)\npy4j.Gateway.invoke(Gateway.java:259)\npy4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:133)\npy4j.commands.CallCommand.execute(CallCommand.java:79)\npy4j.GatewayConnection.run(GatewayConnection.java:209)\njava.lang.Thread.run(Thread.java:745)","Submission Time":1467139364411,"Completion Time":1467139383262,"Accumulables":[]}}
{"Event":"SparkListenerJobEnd","Job ID":0,"Completion Time":1467139383265,"Job Result":{"Result":"JobSucceeded"}}
{"Event":"SparkListenerJobStart","Job ID":1,"Submission Time":1467139383292,"Stage Infos":[{"Stage ID":1,"Stage Attempt ID":0,"Stage Name":"sortByKey at /home/daniar/documents/SPARK/spark-1.6.1/sort.py:12","Number of Tasks":4,"RDD Info":[{"RDD ID":3,"Name":"PythonRDD","Callsite":"sortByKey at /home/daniar/documents/SPARK/spark-1.6.1/sort.py:12","Parent IDs":[1],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":4,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":0,"Name":"../generated_file/list_int","Scope":"{\"id\":\"0\",\"name\":\"textFile\"}","Callsite":"textFile at NativeMethodAccessorImpl.java:-2","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":4,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":1,"Name":"../generated_file/list_int","Scope":"{\"id\":\"0\",\"name\":\"textFile\"}","Callsite":"textFile at NativeMethodAccessorImpl.java:-2","Parent IDs":[0],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":4,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.collect(RDD.scala:926)\norg.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:405)\norg.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:498)\npy4j.reflection.MethodInvoker.invoke(MethodInvoker.java:231)\npy4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:381)\npy4j.Gateway.invoke(Gateway.java:259)\npy4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:133)\npy4j.commands.CallCommand.execute(CallCommand.java:79)\npy4j.GatewayConnection.run(GatewayConnection.java:209)\njava.lang.Thread.run(Thread.java:745)","Accumulables":[]}],"Stage IDs":[1],"Properties":{"spark.rdd.scope.noOverride":"true","spark.rdd.scope":"{\"id\":\"5\",\"name\":\"collect\"}","callSite.short":"sortByKey at /home/daniar/documents/SPARK/spark-1.6.1/sort.py:12"}}
{"Event":"SparkListenerStageSubmitted","Stage Info":{"Stage ID":1,"Stage Attempt ID":0,"Stage Name":"sortByKey at /home/daniar/documents/SPARK/spark-1.6.1/sort.py:12","Number of Tasks":4,"RDD Info":[{"RDD ID":3,"Name":"PythonRDD","Callsite":"sortByKey at /home/daniar/documents/SPARK/spark-1.6.1/sort.py:12","Parent IDs":[1],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":4,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":0,"Name":"../generated_file/list_int","Scope":"{\"id\":\"0\",\"name\":\"textFile\"}","Callsite":"textFile at NativeMethodAccessorImpl.java:-2","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":4,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":1,"Name":"../generated_file/list_int","Scope":"{\"id\":\"0\",\"name\":\"textFile\"}","Callsite":"textFile at NativeMethodAccessorImpl.java:-2","Parent IDs":[0],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":4,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.collect(RDD.scala:926)\norg.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:405)\norg.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:498)\npy4j.reflection.MethodInvoker.invoke(MethodInvoker.java:231)\npy4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:381)\npy4j.Gateway.invoke(Gateway.java:259)\npy4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:133)\npy4j.commands.CallCommand.execute(CallCommand.java:79)\npy4j.GatewayConnection.run(GatewayConnection.java:209)\njava.lang.Thread.run(Thread.java:745)","Accumulables":[]},"Properties":{"spark.rdd.scope.noOverride":"true","spark.rdd.scope":"{\"id\":\"5\",\"name\":\"collect\"}","callSite.short":"sortByKey at /home/daniar/documents/SPARK/spark-1.6.1/sort.py:12"}}
{"Event":"SparkListenerTaskStart","Stage ID":1,"Stage Attempt ID":0,"Task Info":{"Task ID":4,"Index":0,"Attempt":0,"Launch Time":1467139383305,"Executor ID":"1","Host":"192.168.1.3","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":0,"Failed":false,"Accumulables":[]}}
{"Event":"SparkListenerTaskStart","Stage ID":1,"Stage Attempt ID":0,"Task Info":{"Task ID":5,"Index":1,"Attempt":0,"Launch Time":1467139383305,"Executor ID":"0","Host":"192.168.1.3","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":0,"Failed":false,"Accumulables":[]}}
{"Event":"SparkListenerTaskStart","Stage ID":1,"Stage Attempt ID":0,"Task Info":{"Task ID":6,"Index":2,"Attempt":0,"Launch Time":1467139383307,"Executor ID":"1","Host":"192.168.1.3","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":0,"Failed":false,"Accumulables":[]}}
{"Event":"SparkListenerTaskStart","Stage ID":1,"Stage Attempt ID":0,"Task Info":{"Task ID":7,"Index":3,"Attempt":0,"Launch Time":1467139383308,"Executor ID":"0","Host":"192.168.1.3","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":0,"Failed":false,"Accumulables":[]}}
{"Event":"SparkListenerTaskEnd","Stage ID":1,"Stage Attempt ID":0,"Task Type":"ResultTask","Task End Reason":{"Reason":"Success"},"Task Info":{"Task ID":7,"Index":3,"Attempt":0,"Launch Time":1467139383308,"Executor ID":"0","Host":"192.168.1.3","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":1467139395127,"Failed":false,"Accumulables":[]},"Task Metrics":{"Host Name":"192.168.1.11","Executor Deserialize Time":36,"Executor Run Time":11774,"Result Size":2292,"JVM GC Time":40,"Result Serialization Time":0,"Memory Bytes Spilled":0,"Disk Bytes Spilled":0,"Input Metrics":{"Data Read Method":"Hadoop","Bytes Read":65536,"Records Read":3484461}}}
{"Event":"SparkListenerTaskEnd","Stage ID":1,"Stage Attempt ID":0,"Task Type":"ResultTask","Task End Reason":{"Reason":"Success"},"Task Info":{"Task ID":5,"Index":1,"Attempt":0,"Launch Time":1467139383305,"Executor ID":"0","Host":"192.168.1.3","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":1467139397152,"Failed":false,"Accumulables":[]},"Task Metrics":{"Host Name":"192.168.1.11","Executor Deserialize Time":40,"Executor Run Time":13795,"Result Size":2320,"JVM GC Time":46,"Result Serialization Time":0,"Memory Bytes Spilled":0,"Disk Bytes Spilled":0,"Input Metrics":{"Data Read Method":"Hadoop","Bytes Read":65536,"Records Read":4253268}}}
{"Event":"SparkListenerTaskEnd","Stage ID":1,"Stage Attempt ID":0,"Task Type":"ResultTask","Task End Reason":{"Reason":"Success"},"Task Info":{"Task ID":6,"Index":2,"Attempt":0,"Launch Time":1467139383307,"Executor ID":"1","Host":"192.168.1.3","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":1467139453424,"Failed":false,"Accumulables":[]},"Task Metrics":{"Host Name":"192.168.1.11","Executor Deserialize Time":27809,"Executor Run Time":42259,"Result Size":2254,"JVM GC Time":173,"Result Serialization Time":1,"Memory Bytes Spilled":0,"Disk Bytes Spilled":0,"Input Metrics":{"Data Read Method":"Hadoop","Bytes Read":65536,"Records Read":4253236}}}
{"Event":"SparkListenerTaskEnd","Stage ID":1,"Stage Attempt ID":0,"Task Type":"ResultTask","Task End Reason":{"Reason":"Success"},"Task Info":{"Task ID":4,"Index":0,"Attempt":0,"Launch Time":1467139383305,"Executor ID":"1","Host":"192.168.1.3","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":1467139454171,"Failed":false,"Accumulables":[]},"Task Metrics":{"Host Name":"192.168.1.11","Executor Deserialize Time":27808,"Executor Run Time":43009,"Result Size":2287,"JVM GC Time":173,"Result Serialization Time":0,"Memory Bytes Spilled":0,"Disk Bytes Spilled":0,"Input Metrics":{"Data Read Method":"Hadoop","Bytes Read":0,"Records Read":4253453}}}
{"Event":"SparkListenerStageCompleted","Stage Info":{"Stage ID":1,"Stage Attempt ID":0,"Stage Name":"sortByKey at /home/daniar/documents/SPARK/spark-1.6.1/sort.py:12","Number of Tasks":4,"RDD Info":[{"RDD ID":3,"Name":"PythonRDD","Callsite":"sortByKey at /home/daniar/documents/SPARK/spark-1.6.1/sort.py:12","Parent IDs":[1],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":4,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":0,"Name":"../generated_file/list_int","Scope":"{\"id\":\"0\",\"name\":\"textFile\"}","Callsite":"textFile at NativeMethodAccessorImpl.java:-2","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":4,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":1,"Name":"../generated_file/list_int","Scope":"{\"id\":\"0\",\"name\":\"textFile\"}","Callsite":"textFile at NativeMethodAccessorImpl.java:-2","Parent IDs":[0],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":4,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.collect(RDD.scala:926)\norg.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:405)\norg.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:498)\npy4j.reflection.MethodInvoker.invoke(MethodInvoker.java:231)\npy4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:381)\npy4j.Gateway.invoke(Gateway.java:259)\npy4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:133)\npy4j.commands.CallCommand.execute(CallCommand.java:79)\npy4j.GatewayConnection.run(GatewayConnection.java:209)\njava.lang.Thread.run(Thread.java:745)","Submission Time":1467139383304,"Completion Time":1467139454172,"Accumulables":[]}}
{"Event":"SparkListenerJobEnd","Job ID":1,"Completion Time":1467139454172,"Job Result":{"Result":"JobSucceeded"}}
{"Event":"SparkListenerJobStart","Job ID":2,"Submission Time":1467139454402,"Stage Infos":[{"Stage ID":2,"Stage Attempt ID":0,"Stage Name":"sortByKey at /home/daniar/documents/SPARK/spark-1.6.1/sort.py:12","Number of Tasks":4,"RDD Info":[{"RDD ID":5,"Name":"PairwiseRDD","Callsite":"sortByKey at /home/daniar/documents/SPARK/spark-1.6.1/sort.py:12","Parent IDs":[4],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":4,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":0,"Name":"../generated_file/list_int","Scope":"{\"id\":\"0\",\"name\":\"textFile\"}","Callsite":"textFile at NativeMethodAccessorImpl.java:-2","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":4,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":4,"Name":"PythonRDD","Callsite":"sortByKey at /home/daniar/documents/SPARK/spark-1.6.1/sort.py:12","Parent IDs":[1],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":4,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":1,"Name":"../generated_file/list_int","Scope":"{\"id\":\"0\",\"name\":\"textFile\"}","Callsite":"textFile at NativeMethodAccessorImpl.java:-2","Parent IDs":[0],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":4,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.<init>(RDD.scala:98)\norg.apache.spark.api.python.PairwiseRDD.<init>(PythonRDD.scala:338)\nsun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\nsun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\nsun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\njava.lang.reflect.Constructor.newInstance(Constructor.java:423)\npy4j.reflection.MethodInvoker.invoke(MethodInvoker.java:234)\npy4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:381)\npy4j.Gateway.invoke(Gateway.java:214)\npy4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:79)\npy4j.commands.ConstructorCommand.execute(ConstructorCommand.java:68)\npy4j.GatewayConnection.run(GatewayConnection.java:209)\njava.lang.Thread.run(Thread.java:745)","Accumulables":[]},{"Stage ID":3,"Stage Attempt ID":0,"Stage Name":"saveAsTextFile at NativeMethodAccessorImpl.java:-2","Number of Tasks":4,"RDD Info":[{"RDD ID":10,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"11\",\"name\":\"saveAsTextFile\"}","Callsite":"saveAsTextFile at NativeMethodAccessorImpl.java:-2","Parent IDs":[9],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":4,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":8,"Name":"PythonRDD","Callsite":"RDD at PythonRDD.scala:43","Parent IDs":[7],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":4,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":6,"Name":"ShuffledRDD","Scope":"{\"id\":\"8\",\"name\":\"partitionBy\"}","Callsite":"partitionBy at NativeMethodAccessorImpl.java:-2","Parent IDs":[5],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":4,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":9,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"10\",\"name\":\"map\"}","Callsite":"map at NativeMethodAccessorImpl.java:-2","Parent IDs":[8],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":4,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":7,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"9\",\"name\":\"mapPartitions\"}","Callsite":"mapPartitions at PythonRDD.scala:374","Parent IDs":[6],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":4,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[2],"Details":"org.apache.spark.api.java.AbstractJavaRDDLike.saveAsTextFile(JavaRDDLike.scala:46)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:498)\npy4j.reflection.MethodInvoker.invoke(MethodInvoker.java:231)\npy4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:381)\npy4j.Gateway.invoke(Gateway.java:259)\npy4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:133)\npy4j.commands.CallCommand.execute(CallCommand.java:79)\npy4j.GatewayConnection.run(GatewayConnection.java:209)\njava.lang.Thread.run(Thread.java:745)","Accumulables":[]}],"Stage IDs":[2,3],"Properties":{"spark.rdd.scope.noOverride":"true","spark.rdd.scope":"{\"id\":\"11\",\"name\":\"saveAsTextFile\"}"}}
{"Event":"SparkListenerStageSubmitted","Stage Info":{"Stage ID":2,"Stage Attempt ID":0,"Stage Name":"sortByKey at /home/daniar/documents/SPARK/spark-1.6.1/sort.py:12","Number of Tasks":4,"RDD Info":[{"RDD ID":5,"Name":"PairwiseRDD","Callsite":"sortByKey at /home/daniar/documents/SPARK/spark-1.6.1/sort.py:12","Parent IDs":[4],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":4,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":0,"Name":"../generated_file/list_int","Scope":"{\"id\":\"0\",\"name\":\"textFile\"}","Callsite":"textFile at NativeMethodAccessorImpl.java:-2","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":4,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":4,"Name":"PythonRDD","Callsite":"sortByKey at /home/daniar/documents/SPARK/spark-1.6.1/sort.py:12","Parent IDs":[1],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":4,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":1,"Name":"../generated_file/list_int","Scope":"{\"id\":\"0\",\"name\":\"textFile\"}","Callsite":"textFile at NativeMethodAccessorImpl.java:-2","Parent IDs":[0],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":4,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.<init>(RDD.scala:98)\norg.apache.spark.api.python.PairwiseRDD.<init>(PythonRDD.scala:338)\nsun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\nsun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\nsun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\njava.lang.reflect.Constructor.newInstance(Constructor.java:423)\npy4j.reflection.MethodInvoker.invoke(MethodInvoker.java:234)\npy4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:381)\npy4j.Gateway.invoke(Gateway.java:214)\npy4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:79)\npy4j.commands.ConstructorCommand.execute(ConstructorCommand.java:68)\npy4j.GatewayConnection.run(GatewayConnection.java:209)\njava.lang.Thread.run(Thread.java:745)","Accumulables":[]},"Properties":{"spark.rdd.scope.noOverride":"true","spark.rdd.scope":"{\"id\":\"11\",\"name\":\"saveAsTextFile\"}"}}
{"Event":"SparkListenerTaskStart","Stage ID":2,"Stage Attempt ID":0,"Task Info":{"Task ID":8,"Index":0,"Attempt":0,"Launch Time":1467139454500,"Executor ID":"0","Host":"192.168.1.3","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":0,"Failed":false,"Accumulables":[]}}
{"Event":"SparkListenerTaskStart","Stage ID":2,"Stage Attempt ID":0,"Task Info":{"Task ID":9,"Index":1,"Attempt":0,"Launch Time":1467139454502,"Executor ID":"1","Host":"192.168.1.3","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":0,"Failed":false,"Accumulables":[]}}
{"Event":"SparkListenerTaskStart","Stage ID":2,"Stage Attempt ID":0,"Task Info":{"Task ID":10,"Index":2,"Attempt":0,"Launch Time":1467139454503,"Executor ID":"0","Host":"192.168.1.3","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":0,"Failed":false,"Accumulables":[]}}
{"Event":"SparkListenerTaskStart","Stage ID":2,"Stage Attempt ID":0,"Task Info":{"Task ID":11,"Index":3,"Attempt":0,"Launch Time":1467139454503,"Executor ID":"1","Host":"192.168.1.3","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":0,"Failed":false,"Accumulables":[]}}
{"Event":"SparkListenerTaskEnd","Stage ID":2,"Stage Attempt ID":0,"Task Type":"ShuffleMapTask","Task End Reason":{"Reason":"Success"},"Task Info":{"Task ID":11,"Index":3,"Attempt":0,"Launch Time":1467139454503,"Executor ID":"1","Host":"192.168.1.3","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":1467139487458,"Failed":false,"Accumulables":[]},"Task Metrics":{"Host Name":"192.168.1.11","Executor Deserialize Time":61,"Executor Run Time":32873,"Result Size":2321,"JVM GC Time":468,"Result Serialization Time":2,"Memory Bytes Spilled":0,"Disk Bytes Spilled":0,"Shuffle Write Metrics":{"Shuffle Bytes Written":53536423,"Shuffle Write Time":269207517,"Shuffle Records Written":148},"Input Metrics":{"Data Read Method":"Hadoop","Bytes Read":65536,"Records Read":3484461}}}
{"Event":"SparkListenerTaskEnd","Stage ID":2,"Stage Attempt ID":0,"Task Type":"ShuffleMapTask","Task End Reason":{"Reason":"Success"},"Task Info":{"Task ID":10,"Index":2,"Attempt":0,"Launch Time":1467139454503,"Executor ID":"0","Host":"192.168.1.3","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":1467139494381,"Failed":false,"Accumulables":[]},"Task Metrics":{"Host Name":"192.168.1.11","Executor Deserialize Time":58,"Executor Run Time":39800,"Result Size":2321,"JVM GC Time":310,"Result Serialization Time":1,"Memory Bytes Spilled":0,"Disk Bytes Spilled":0,"Shuffle Write Metrics":{"Shuffle Bytes Written":65352156,"Shuffle Write Time":1031748370,"Shuffle Records Written":164},"Input Metrics":{"Data Read Method":"Hadoop","Bytes Read":65536,"Records Read":4253236}}}
{"Event":"SparkListenerTaskEnd","Stage ID":2,"Stage Attempt ID":0,"Task Type":"ShuffleMapTask","Task End Reason":{"Reason":"Success"},"Task Info":{"Task ID":8,"Index":0,"Attempt":0,"Launch Time":1467139454500,"Executor ID":"0","Host":"192.168.1.3","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":1467139494381,"Failed":false,"Accumulables":[]},"Task Metrics":{"Host Name":"192.168.1.11","Executor Deserialize Time":68,"Executor Run Time":39800,"Result Size":2321,"JVM GC Time":310,"Result Serialization Time":1,"Memory Bytes Spilled":0,"Disk Bytes Spilled":0,"Shuffle Write Metrics":{"Shuffle Bytes Written":65356545,"Shuffle Write Time":1082577785,"Shuffle Records Written":164},"Input Metrics":{"Data Read Method":"Hadoop","Bytes Read":0,"Records Read":4253453}}}
{"Event":"SparkListenerTaskEnd","Stage ID":2,"Stage Attempt ID":0,"Task Type":"ShuffleMapTask","Task End Reason":{"Reason":"Success"},"Task Info":{"Task ID":9,"Index":1,"Attempt":0,"Launch Time":1467139454502,"Executor ID":"1","Host":"192.168.1.3","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":1467139494634,"Failed":false,"Accumulables":[]},"Task Metrics":{"Host Name":"192.168.1.11","Executor Deserialize Time":63,"Executor Run Time":40058,"Result Size":2321,"JVM GC Time":493,"Result Serialization Time":0,"Memory Bytes Spilled":0,"Disk Bytes Spilled":0,"Shuffle Write Metrics":{"Shuffle Bytes Written":65350616,"Shuffle Write Time":1258926365,"Shuffle Records Written":164},"Input Metrics":{"Data Read Method":"Hadoop","Bytes Read":65536,"Records Read":4253268}}}
{"Event":"SparkListenerStageCompleted","Stage Info":{"Stage ID":2,"Stage Attempt ID":0,"Stage Name":"sortByKey at /home/daniar/documents/SPARK/spark-1.6.1/sort.py:12","Number of Tasks":4,"RDD Info":[{"RDD ID":5,"Name":"PairwiseRDD","Callsite":"sortByKey at /home/daniar/documents/SPARK/spark-1.6.1/sort.py:12","Parent IDs":[4],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":4,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":0,"Name":"../generated_file/list_int","Scope":"{\"id\":\"0\",\"name\":\"textFile\"}","Callsite":"textFile at NativeMethodAccessorImpl.java:-2","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":4,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":4,"Name":"PythonRDD","Callsite":"sortByKey at /home/daniar/documents/SPARK/spark-1.6.1/sort.py:12","Parent IDs":[1],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":4,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":1,"Name":"../generated_file/list_int","Scope":"{\"id\":\"0\",\"name\":\"textFile\"}","Callsite":"textFile at NativeMethodAccessorImpl.java:-2","Parent IDs":[0],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":4,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.<init>(RDD.scala:98)\norg.apache.spark.api.python.PairwiseRDD.<init>(PythonRDD.scala:338)\nsun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\nsun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\nsun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\njava.lang.reflect.Constructor.newInstance(Constructor.java:423)\npy4j.reflection.MethodInvoker.invoke(MethodInvoker.java:234)\npy4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:381)\npy4j.Gateway.invoke(Gateway.java:214)\npy4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:79)\npy4j.commands.ConstructorCommand.execute(ConstructorCommand.java:68)\npy4j.GatewayConnection.run(GatewayConnection.java:209)\njava.lang.Thread.run(Thread.java:745)","Submission Time":1467139454499,"Completion Time":1467139494635,"Accumulables":[]}}
{"Event":"SparkListenerStageSubmitted","Stage Info":{"Stage ID":3,"Stage Attempt ID":0,"Stage Name":"saveAsTextFile at NativeMethodAccessorImpl.java:-2","Number of Tasks":4,"RDD Info":[{"RDD ID":10,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"11\",\"name\":\"saveAsTextFile\"}","Callsite":"saveAsTextFile at NativeMethodAccessorImpl.java:-2","Parent IDs":[9],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":4,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":8,"Name":"PythonRDD","Callsite":"RDD at PythonRDD.scala:43","Parent IDs":[7],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":4,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":6,"Name":"ShuffledRDD","Scope":"{\"id\":\"8\",\"name\":\"partitionBy\"}","Callsite":"partitionBy at NativeMethodAccessorImpl.java:-2","Parent IDs":[5],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":4,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":9,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"10\",\"name\":\"map\"}","Callsite":"map at NativeMethodAccessorImpl.java:-2","Parent IDs":[8],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":4,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":7,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"9\",\"name\":\"mapPartitions\"}","Callsite":"mapPartitions at PythonRDD.scala:374","Parent IDs":[6],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":4,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[2],"Details":"org.apache.spark.api.java.AbstractJavaRDDLike.saveAsTextFile(JavaRDDLike.scala:46)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:498)\npy4j.reflection.MethodInvoker.invoke(MethodInvoker.java:231)\npy4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:381)\npy4j.Gateway.invoke(Gateway.java:259)\npy4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:133)\npy4j.commands.CallCommand.execute(CallCommand.java:79)\npy4j.GatewayConnection.run(GatewayConnection.java:209)\njava.lang.Thread.run(Thread.java:745)","Accumulables":[]},"Properties":{"spark.rdd.scope.noOverride":"true","spark.rdd.scope":"{\"id\":\"11\",\"name\":\"saveAsTextFile\"}"}}
{"Event":"SparkListenerTaskStart","Stage ID":3,"Stage Attempt ID":0,"Task Info":{"Task ID":12,"Index":0,"Attempt":0,"Launch Time":1467139495463,"Executor ID":"1","Host":"192.168.1.3","Locality":"ANY","Speculative":false,"Getting Result Time":0,"Finish Time":0,"Failed":false,"Accumulables":[]}}
{"Event":"SparkListenerTaskStart","Stage ID":3,"Stage Attempt ID":0,"Task Info":{"Task ID":13,"Index":1,"Attempt":0,"Launch Time":1467139495464,"Executor ID":"0","Host":"192.168.1.3","Locality":"ANY","Speculative":false,"Getting Result Time":0,"Finish Time":0,"Failed":false,"Accumulables":[]}}
{"Event":"SparkListenerTaskStart","Stage ID":3,"Stage Attempt ID":0,"Task Info":{"Task ID":14,"Index":2,"Attempt":0,"Launch Time":1467139495464,"Executor ID":"1","Host":"192.168.1.3","Locality":"ANY","Speculative":false,"Getting Result Time":0,"Finish Time":0,"Failed":false,"Accumulables":[]}}
{"Event":"SparkListenerTaskStart","Stage ID":3,"Stage Attempt ID":0,"Task Info":{"Task ID":15,"Index":3,"Attempt":0,"Launch Time":1467139495465,"Executor ID":"0","Host":"192.168.1.3","Locality":"ANY","Speculative":false,"Getting Result Time":0,"Finish Time":0,"Failed":false,"Accumulables":[]}}
{"Event":"SparkListenerTaskEnd","Stage ID":3,"Stage Attempt ID":0,"Task Type":"ResultTask","Task End Reason":{"Reason":"FetchFailed","Block Manager Address":{"Executor ID":"0","Host":"192.168.1.11","Port":33800},"Shuffle ID":0,"Map ID":2,"Reduce ID":0,"Message":"org.apache.spark.shuffle.FetchFailedException: Failed to connect to /192.168.1.11:33800\n\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.throwFetchFailedException(ShuffleBlockFetcherIterator.scala:323)\n\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:300)\n\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:51)\n\tat scala.collection.Iterator$$anon$11.next(Iterator.scala:328)\n\tat scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)\n\tat org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:32)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:39)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:727)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1157)\n\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:452)\n\tat org.apache.spark.api.python.PythonRunner$WriterThread$$anonfun$run$3.apply(PythonRDD.scala:280)\n\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1765)\n\tat org.apache.spark.api.python.PythonRunner$WriterThread.run(PythonRDD.scala:239)\nCaused by: java.io.IOException: Failed to connect to /192.168.1.11:33800\n\tat org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:216)\n\tat org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:167)\n\tat org.apache.spark.network.netty.NettyBlockTransferService$$anon$1.createAndStart(NettyBlockTransferService.scala:90)\n\tat org.apache.spark.network.shuffle.RetryingBlockFetcher.fetchAllOutstanding(RetryingBlockFetcher.java:140)\n\tat org.apache.spark.network.shuffle.RetryingBlockFetcher.access$200(RetryingBlockFetcher.java:43)\n\tat org.apache.spark.network.shuffle.RetryingBlockFetcher$1.run(RetryingBlockFetcher.java:170)\n\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n\tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n\tat java.lang.Thread.run(Thread.java:745)\nCaused by: java.net.NoRouteToHostException: No route to host: /192.168.1.11:33800\n\tat sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)\n\tat sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)\n\tat io.netty.channel.socket.nio.NioSocketChannel.doFinishConnect(NioSocketChannel.java:224)\n\tat io.netty.channel.nio.AbstractNioChannel$AbstractNioUnsafe.finishConnect(AbstractNioChannel.java:289)\n\tat io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:528)\n\tat io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:468)\n\tat io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:382)\n\tat io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:354)\n\tat io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:111)\n\t... 1 more\n"},"Task Info":{"Task ID":12,"Index":0,"Attempt":0,"Launch Time":1467139495463,"Executor ID":"1","Host":"192.168.1.3","Locality":"ANY","Speculative":false,"Getting Result Time":0,"Finish Time":1467139526848,"Failed":true,"Accumulables":[]}}
{"Event":"SparkListenerStageCompleted","Stage Info":{"Stage ID":3,"Stage Attempt ID":0,"Stage Name":"saveAsTextFile at NativeMethodAccessorImpl.java:-2","Number of Tasks":4,"RDD Info":[{"RDD ID":10,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"11\",\"name\":\"saveAsTextFile\"}","Callsite":"saveAsTextFile at NativeMethodAccessorImpl.java:-2","Parent IDs":[9],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":4,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":8,"Name":"PythonRDD","Callsite":"RDD at PythonRDD.scala:43","Parent IDs":[7],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":4,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":6,"Name":"ShuffledRDD","Scope":"{\"id\":\"8\",\"name\":\"partitionBy\"}","Callsite":"partitionBy at NativeMethodAccessorImpl.java:-2","Parent IDs":[5],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":4,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":9,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"10\",\"name\":\"map\"}","Callsite":"map at NativeMethodAccessorImpl.java:-2","Parent IDs":[8],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":4,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":7,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"9\",\"name\":\"mapPartitions\"}","Callsite":"mapPartitions at PythonRDD.scala:374","Parent IDs":[6],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":4,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[2],"Details":"org.apache.spark.api.java.AbstractJavaRDDLike.saveAsTextFile(JavaRDDLike.scala:46)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:498)\npy4j.reflection.MethodInvoker.invoke(MethodInvoker.java:231)\npy4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:381)\npy4j.Gateway.invoke(Gateway.java:259)\npy4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:133)\npy4j.commands.CallCommand.execute(CallCommand.java:79)\npy4j.GatewayConnection.run(GatewayConnection.java:209)\njava.lang.Thread.run(Thread.java:745)","Submission Time":1467139495463,"Completion Time":1467139526884,"Failure Reason":"org.apache.spark.shuffle.FetchFailedException: Failed to connect to /192.168.1.11:33800\n\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.throwFetchFailedException(ShuffleBlockFetcherIterator.scala:323)\n\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:300)\n\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:51)\n\tat scala.collection.Iterator$$anon$11.next(Iterator.scala:328)\n\tat scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)\n\tat org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:32)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:39)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:727)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1157)\n\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:452)\n\tat org.apache.spark.api.python.PythonRunner$WriterThread$$anonfun$run$3.apply(PythonRDD.scala:280)\n\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1765)\n\tat org.apache.spark.api.python.PythonRunner$WriterThread.run(PythonRDD.scala:239)\nCaused by: java.io.IOException: Failed to connect to /192.168.1.11:33800\n\tat org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:216)\n\tat org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:167)\n\tat org.apache.spark.network.netty.NettyBlockTransferService$$anon$1.createAndStart(NettyBlockTransferService.scala:90)\n\tat org.apache.spark.network.shuffle.RetryingBlockFetcher.fetchAllOutstanding(RetryingBlockFetcher.java:140)\n\tat org.apache.spark.network.shuffle.RetryingBlockFetcher.access$200(RetryingBlockFetcher.java:43)\n\tat org.apache.spark.network.shuffle.RetryingBlockFetcher$1.run(RetryingBlockFetcher.java:170)\n\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n\tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n\tat java.lang.Thread.run(Thread.java:745)\nCaused by: java.net.NoRouteToHostException: No route to host: /192.168.1.11:33800\n\tat sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)\n\tat sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)\n\tat io.netty.channel.socket.nio.NioSocketChannel.doFinishConnect(NioSocketChannel.java:224)\n\tat io.netty.channel.nio.AbstractNioChannel$AbstractNioUnsafe.finishConnect(AbstractNioChannel.java:289)\n\tat io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:528)\n\tat io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:468)\n\tat io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:382)\n\tat io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:354)\n\tat io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:111)\n\t... 1 more\n","Accumulables":[]}}
{"Event":"SparkListenerBlockManagerRemoved","Block Manager ID":{"Executor ID":"0","Host":"192.168.1.11","Port":33800},"Timestamp":1467139526926}
{"Event":"SparkListenerStageSubmitted","Stage Info":{"Stage ID":2,"Stage Attempt ID":1,"Stage Name":"sortByKey at /home/daniar/documents/SPARK/spark-1.6.1/sort.py:12","Number of Tasks":2,"RDD Info":[{"RDD ID":5,"Name":"PairwiseRDD","Callsite":"sortByKey at /home/daniar/documents/SPARK/spark-1.6.1/sort.py:12","Parent IDs":[4],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":4,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":0,"Name":"../generated_file/list_int","Scope":"{\"id\":\"0\",\"name\":\"textFile\"}","Callsite":"textFile at NativeMethodAccessorImpl.java:-2","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":4,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":4,"Name":"PythonRDD","Callsite":"sortByKey at /home/daniar/documents/SPARK/spark-1.6.1/sort.py:12","Parent IDs":[1],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":4,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":1,"Name":"../generated_file/list_int","Scope":"{\"id\":\"0\",\"name\":\"textFile\"}","Callsite":"textFile at NativeMethodAccessorImpl.java:-2","Parent IDs":[0],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":4,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.<init>(RDD.scala:98)\norg.apache.spark.api.python.PairwiseRDD.<init>(PythonRDD.scala:338)\nsun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\nsun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\nsun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\njava.lang.reflect.Constructor.newInstance(Constructor.java:423)\npy4j.reflection.MethodInvoker.invoke(MethodInvoker.java:234)\npy4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:381)\npy4j.Gateway.invoke(Gateway.java:214)\npy4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:79)\npy4j.commands.ConstructorCommand.execute(ConstructorCommand.java:68)\npy4j.GatewayConnection.run(GatewayConnection.java:209)\njava.lang.Thread.run(Thread.java:745)","Accumulables":[]},"Properties":{"spark.rdd.scope.noOverride":"true","spark.rdd.scope":"{\"id\":\"11\",\"name\":\"saveAsTextFile\"}"}}
{"Event":"SparkListenerTaskStart","Stage ID":2,"Stage Attempt ID":1,"Task Info":{"Task ID":16,"Index":0,"Attempt":0,"Launch Time":1467139527120,"Executor ID":"0","Host":"192.168.1.3","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":0,"Failed":false,"Accumulables":[]}}
{"Event":"SparkListenerTaskStart","Stage ID":2,"Stage Attempt ID":1,"Task Info":{"Task ID":17,"Index":1,"Attempt":0,"Launch Time":1467139527120,"Executor ID":"1","Host":"192.168.1.3","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":0,"Failed":false,"Accumulables":[]}}
{"Event":"SparkListenerBlockManagerAdded","Block Manager ID":{"Executor ID":"0","Host":"192.168.1.11","Port":33800},"Maximum Memory":535953408,"Timestamp":1467139527159}
{"Event":"SparkListenerTaskEnd","Stage ID":3,"Stage Attempt ID":0,"Task Type":"ResultTask","Task End Reason":{"Reason":"FetchFailed","Block Manager Address":{"Executor ID":"0","Host":"192.168.1.11","Port":33800},"Shuffle ID":0,"Map ID":2,"Reduce ID":2,"Message":"org.apache.spark.shuffle.FetchFailedException: Failed to connect to /192.168.1.11:33800\n\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.throwFetchFailedException(ShuffleBlockFetcherIterator.scala:323)\n\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:300)\n\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:51)\n\tat scala.collection.Iterator$$anon$11.next(Iterator.scala:328)\n\tat scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)\n\tat org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:32)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:39)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:727)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1157)\n\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:452)\n\tat org.apache.spark.api.python.PythonRunner$WriterThread$$anonfun$run$3.apply(PythonRDD.scala:280)\n\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1765)\n\tat org.apache.spark.api.python.PythonRunner$WriterThread.run(PythonRDD.scala:239)\nCaused by: java.io.IOException: Failed to connect to /192.168.1.11:33800\n\tat org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:216)\n\tat org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:167)\n\tat org.apache.spark.network.netty.NettyBlockTransferService$$anon$1.createAndStart(NettyBlockTransferService.scala:90)\n\tat org.apache.spark.network.shuffle.RetryingBlockFetcher.fetchAllOutstanding(RetryingBlockFetcher.java:140)\n\tat org.apache.spark.network.shuffle.RetryingBlockFetcher.access$200(RetryingBlockFetcher.java:43)\n\tat org.apache.spark.network.shuffle.RetryingBlockFetcher$1.run(RetryingBlockFetcher.java:170)\n\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n\tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n\tat java.lang.Thread.run(Thread.java:745)\nCaused by: java.net.NoRouteToHostException: No route to host: /192.168.1.11:33800\n\tat sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)\n\tat sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)\n\tat io.netty.channel.socket.nio.NioSocketChannel.doFinishConnect(NioSocketChannel.java:224)\n\tat io.netty.channel.nio.AbstractNioChannel$AbstractNioUnsafe.finishConnect(AbstractNioChannel.java:289)\n\tat io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:528)\n\tat io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:468)\n\tat io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:382)\n\tat io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:354)\n\tat io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:111)\n\t... 1 more\n"},"Task Info":{"Task ID":14,"Index":2,"Attempt":0,"Launch Time":1467139495464,"Executor ID":"1","Host":"192.168.1.3","Locality":"ANY","Speculative":false,"Getting Result Time":0,"Finish Time":1467139533228,"Failed":true,"Accumulables":[]}}
{"Event":"SparkListenerTaskEnd","Stage ID":3,"Stage Attempt ID":0,"Task Type":"ResultTask","Task End Reason":{"Reason":"FetchFailed","Block Manager Address":{"Executor ID":"1","Host":"192.168.1.11","Port":35409},"Shuffle ID":0,"Map ID":3,"Reduce ID":1,"Message":"org.apache.spark.shuffle.FetchFailedException: Failed to connect to /192.168.1.11:35409\n\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.throwFetchFailedException(ShuffleBlockFetcherIterator.scala:323)\n\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:300)\n\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:51)\n\tat scala.collection.Iterator$$anon$11.next(Iterator.scala:328)\n\tat scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)\n\tat org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:32)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:39)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:727)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1157)\n\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:452)\n\tat org.apache.spark.api.python.PythonRunner$WriterThread$$anonfun$run$3.apply(PythonRDD.scala:280)\n\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1765)\n\tat org.apache.spark.api.python.PythonRunner$WriterThread.run(PythonRDD.scala:239)\nCaused by: java.io.IOException: Failed to connect to /192.168.1.11:35409\n\tat org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:216)\n\tat org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:167)\n\tat org.apache.spark.network.netty.NettyBlockTransferService$$anon$1.createAndStart(NettyBlockTransferService.scala:90)\n\tat org.apache.spark.network.shuffle.RetryingBlockFetcher.fetchAllOutstanding(RetryingBlockFetcher.java:140)\n\tat org.apache.spark.network.shuffle.RetryingBlockFetcher.access$200(RetryingBlockFetcher.java:43)\n\tat org.apache.spark.network.shuffle.RetryingBlockFetcher$1.run(RetryingBlockFetcher.java:170)\n\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n\tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n\tat java.lang.Thread.run(Thread.java:745)\nCaused by: java.net.NoRouteToHostException: No route to host: /192.168.1.11:35409\n\tat sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)\n\tat sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)\n\tat io.netty.channel.socket.nio.NioSocketChannel.doFinishConnect(NioSocketChannel.java:224)\n\tat io.netty.channel.nio.AbstractNioChannel$AbstractNioUnsafe.finishConnect(AbstractNioChannel.java:289)\n\tat io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:528)\n\tat io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:468)\n\tat io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:382)\n\tat io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:354)\n\tat io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:111)\n\t... 1 more\n"},"Task Info":{"Task ID":13,"Index":1,"Attempt":0,"Launch Time":1467139495464,"Executor ID":"0","Host":"192.168.1.3","Locality":"ANY","Speculative":false,"Getting Result Time":0,"Finish Time":1467139536798,"Failed":true,"Accumulables":[]}}
{"Event":"SparkListenerBlockManagerRemoved","Block Manager ID":{"Executor ID":"1","Host":"192.168.1.11","Port":35409},"Timestamp":1467139536809}
{"Event":"SparkListenerBlockManagerAdded","Block Manager ID":{"Executor ID":"1","Host":"192.168.1.11","Port":35409},"Maximum Memory":535953408,"Timestamp":1467139542907}
{"Event":"SparkListenerTaskEnd","Stage ID":3,"Stage Attempt ID":0,"Task Type":"ResultTask","Task End Reason":{"Reason":"FetchFailed","Block Manager Address":{"Executor ID":"1","Host":"192.168.1.11","Port":35409},"Shuffle ID":0,"Map ID":3,"Reduce ID":3,"Message":"org.apache.spark.shuffle.FetchFailedException: Failed to connect to /192.168.1.11:35409\n\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.throwFetchFailedException(ShuffleBlockFetcherIterator.scala:323)\n\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:300)\n\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:51)\n\tat scala.collection.Iterator$$anon$11.next(Iterator.scala:328)\n\tat scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)\n\tat org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:32)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:39)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:727)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1157)\n\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:452)\n\tat org.apache.spark.api.python.PythonRunner$WriterThread$$anonfun$run$3.apply(PythonRDD.scala:280)\n\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1765)\n\tat org.apache.spark.api.python.PythonRunner$WriterThread.run(PythonRDD.scala:239)\nCaused by: java.io.IOException: Failed to connect to /192.168.1.11:35409\n\tat org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:216)\n\tat org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:167)\n\tat org.apache.spark.network.netty.NettyBlockTransferService$$anon$1.createAndStart(NettyBlockTransferService.scala:90)\n\tat org.apache.spark.network.shuffle.RetryingBlockFetcher.fetchAllOutstanding(RetryingBlockFetcher.java:140)\n\tat org.apache.spark.network.shuffle.RetryingBlockFetcher.access$200(RetryingBlockFetcher.java:43)\n\tat org.apache.spark.network.shuffle.RetryingBlockFetcher$1.run(RetryingBlockFetcher.java:170)\n\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n\tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n\tat java.lang.Thread.run(Thread.java:745)\nCaused by: java.net.NoRouteToHostException: No route to host: /192.168.1.11:35409\n\tat sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)\n\tat sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)\n\tat io.netty.channel.socket.nio.NioSocketChannel.doFinishConnect(NioSocketChannel.java:224)\n\tat io.netty.channel.nio.AbstractNioChannel$AbstractNioUnsafe.finishConnect(AbstractNioChannel.java:289)\n\tat io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:528)\n\tat io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:468)\n\tat io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:382)\n\tat io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:354)\n\tat io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:111)\n\t... 1 more\n"},"Task Info":{"Task ID":15,"Index":3,"Attempt":0,"Launch Time":1467139495465,"Executor ID":"0","Host":"192.168.1.3","Locality":"ANY","Speculative":false,"Getting Result Time":0,"Finish Time":1467139545749,"Failed":true,"Accumulables":[]}}
{"Event":"SparkListenerTaskEnd","Stage ID":2,"Stage Attempt ID":1,"Task Type":"ShuffleMapTask","Task End Reason":{"Reason":"Success"},"Task Info":{"Task ID":16,"Index":0,"Attempt":0,"Launch Time":1467139527120,"Executor ID":"0","Host":"192.168.1.3","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":1467139554992,"Failed":false,"Accumulables":[]},"Task Metrics":{"Host Name":"192.168.1.11","Executor Deserialize Time":27,"Executor Run Time":27837,"Result Size":2321,"JVM GC Time":358,"Result Serialization Time":0,"Memory Bytes Spilled":0,"Disk Bytes Spilled":0,"Shuffle Write Metrics":{"Shuffle Bytes Written":65356545,"Shuffle Write Time":181367764,"Shuffle Records Written":164},"Input Metrics":{"Data Read Method":"Hadoop","Bytes Read":0,"Records Read":4253453}}}
{"Event":"SparkListenerTaskEnd","Stage ID":2,"Stage Attempt ID":1,"Task Type":"ShuffleMapTask","Task End Reason":{"Reason":"Success"},"Task Info":{"Task ID":17,"Index":1,"Attempt":0,"Launch Time":1467139527120,"Executor ID":"1","Host":"192.168.1.3","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":1467139555665,"Failed":false,"Accumulables":[]},"Task Metrics":{"Host Name":"192.168.1.11","Executor Deserialize Time":12,"Executor Run Time":28528,"Result Size":2321,"JVM GC Time":196,"Result Serialization Time":0,"Memory Bytes Spilled":0,"Disk Bytes Spilled":0,"Shuffle Write Metrics":{"Shuffle Bytes Written":65352156,"Shuffle Write Time":156259624,"Shuffle Records Written":164},"Input Metrics":{"Data Read Method":"Hadoop","Bytes Read":65536,"Records Read":4253236}}}
{"Event":"SparkListenerStageCompleted","Stage Info":{"Stage ID":2,"Stage Attempt ID":1,"Stage Name":"sortByKey at /home/daniar/documents/SPARK/spark-1.6.1/sort.py:12","Number of Tasks":2,"RDD Info":[{"RDD ID":5,"Name":"PairwiseRDD","Callsite":"sortByKey at /home/daniar/documents/SPARK/spark-1.6.1/sort.py:12","Parent IDs":[4],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":4,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":0,"Name":"../generated_file/list_int","Scope":"{\"id\":\"0\",\"name\":\"textFile\"}","Callsite":"textFile at NativeMethodAccessorImpl.java:-2","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":4,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":4,"Name":"PythonRDD","Callsite":"sortByKey at /home/daniar/documents/SPARK/spark-1.6.1/sort.py:12","Parent IDs":[1],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":4,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":1,"Name":"../generated_file/list_int","Scope":"{\"id\":\"0\",\"name\":\"textFile\"}","Callsite":"textFile at NativeMethodAccessorImpl.java:-2","Parent IDs":[0],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":4,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.<init>(RDD.scala:98)\norg.apache.spark.api.python.PairwiseRDD.<init>(PythonRDD.scala:338)\nsun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\nsun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\nsun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\njava.lang.reflect.Constructor.newInstance(Constructor.java:423)\npy4j.reflection.MethodInvoker.invoke(MethodInvoker.java:234)\npy4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:381)\npy4j.Gateway.invoke(Gateway.java:214)\npy4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:79)\npy4j.commands.ConstructorCommand.execute(ConstructorCommand.java:68)\npy4j.GatewayConnection.run(GatewayConnection.java:209)\njava.lang.Thread.run(Thread.java:745)","Submission Time":1467139527119,"Completion Time":1467139555665,"Accumulables":[]}}
{"Event":"SparkListenerStageSubmitted","Stage Info":{"Stage ID":2,"Stage Attempt ID":2,"Stage Name":"sortByKey at /home/daniar/documents/SPARK/spark-1.6.1/sort.py:12","Number of Tasks":2,"RDD Info":[{"RDD ID":5,"Name":"PairwiseRDD","Callsite":"sortByKey at /home/daniar/documents/SPARK/spark-1.6.1/sort.py:12","Parent IDs":[4],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":4,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":0,"Name":"../generated_file/list_int","Scope":"{\"id\":\"0\",\"name\":\"textFile\"}","Callsite":"textFile at NativeMethodAccessorImpl.java:-2","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":4,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":4,"Name":"PythonRDD","Callsite":"sortByKey at /home/daniar/documents/SPARK/spark-1.6.1/sort.py:12","Parent IDs":[1],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":4,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":1,"Name":"../generated_file/list_int","Scope":"{\"id\":\"0\",\"name\":\"textFile\"}","Callsite":"textFile at NativeMethodAccessorImpl.java:-2","Parent IDs":[0],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":4,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.<init>(RDD.scala:98)\norg.apache.spark.api.python.PairwiseRDD.<init>(PythonRDD.scala:338)\nsun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\nsun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\nsun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\njava.lang.reflect.Constructor.newInstance(Constructor.java:423)\npy4j.reflection.MethodInvoker.invoke(MethodInvoker.java:234)\npy4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:381)\npy4j.Gateway.invoke(Gateway.java:214)\npy4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:79)\npy4j.commands.ConstructorCommand.execute(ConstructorCommand.java:68)\npy4j.GatewayConnection.run(GatewayConnection.java:209)\njava.lang.Thread.run(Thread.java:745)","Accumulables":[]},"Properties":{"spark.rdd.scope.noOverride":"true","spark.rdd.scope":"{\"id\":\"11\",\"name\":\"saveAsTextFile\"}"}}
{"Event":"SparkListenerTaskStart","Stage ID":2,"Stage Attempt ID":2,"Task Info":{"Task ID":18,"Index":0,"Attempt":0,"Launch Time":1467139555691,"Executor ID":"1","Host":"192.168.1.3","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":0,"Failed":false,"Accumulables":[]}}
{"Event":"SparkListenerTaskStart","Stage ID":2,"Stage Attempt ID":2,"Task Info":{"Task ID":19,"Index":1,"Attempt":0,"Launch Time":1467139555691,"Executor ID":"0","Host":"192.168.1.3","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":0,"Failed":false,"Accumulables":[]}}
{"Event":"SparkListenerTaskEnd","Stage ID":2,"Stage Attempt ID":2,"Task Type":"ShuffleMapTask","Task End Reason":{"Reason":"Success"},"Task Info":{"Task ID":19,"Index":1,"Attempt":0,"Launch Time":1467139555691,"Executor ID":"0","Host":"192.168.1.3","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":1467139573155,"Failed":false,"Accumulables":[]},"Task Metrics":{"Host Name":"192.168.1.11","Executor Deserialize Time":11,"Executor Run Time":17446,"Result Size":2321,"JVM GC Time":247,"Result Serialization Time":0,"Memory Bytes Spilled":0,"Disk Bytes Spilled":0,"Shuffle Write Metrics":{"Shuffle Bytes Written":53536423,"Shuffle Write Time":123384390,"Shuffle Records Written":148},"Input Metrics":{"Data Read Method":"Hadoop","Bytes Read":65536,"Records Read":3484461}}}
{"Event":"SparkListenerTaskEnd","Stage ID":2,"Stage Attempt ID":2,"Task Type":"ShuffleMapTask","Task End Reason":{"Reason":"Success"},"Task Info":{"Task ID":18,"Index":0,"Attempt":0,"Launch Time":1467139555691,"Executor ID":"1","Host":"192.168.1.3","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":1467139578117,"Failed":false,"Accumulables":[]},"Task Metrics":{"Host Name":"192.168.1.11","Executor Deserialize Time":10,"Executor Run Time":22409,"Result Size":2321,"JVM GC Time":223,"Result Serialization Time":0,"Memory Bytes Spilled":0,"Disk Bytes Spilled":0,"Shuffle Write Metrics":{"Shuffle Bytes Written":65350616,"Shuffle Write Time":152164896,"Shuffle Records Written":164},"Input Metrics":{"Data Read Method":"Hadoop","Bytes Read":65536,"Records Read":4253268}}}
{"Event":"SparkListenerStageCompleted","Stage Info":{"Stage ID":2,"Stage Attempt ID":2,"Stage Name":"sortByKey at /home/daniar/documents/SPARK/spark-1.6.1/sort.py:12","Number of Tasks":2,"RDD Info":[{"RDD ID":5,"Name":"PairwiseRDD","Callsite":"sortByKey at /home/daniar/documents/SPARK/spark-1.6.1/sort.py:12","Parent IDs":[4],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":4,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":0,"Name":"../generated_file/list_int","Scope":"{\"id\":\"0\",\"name\":\"textFile\"}","Callsite":"textFile at NativeMethodAccessorImpl.java:-2","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":4,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":4,"Name":"PythonRDD","Callsite":"sortByKey at /home/daniar/documents/SPARK/spark-1.6.1/sort.py:12","Parent IDs":[1],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":4,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":1,"Name":"../generated_file/list_int","Scope":"{\"id\":\"0\",\"name\":\"textFile\"}","Callsite":"textFile at NativeMethodAccessorImpl.java:-2","Parent IDs":[0],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":4,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.<init>(RDD.scala:98)\norg.apache.spark.api.python.PairwiseRDD.<init>(PythonRDD.scala:338)\nsun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\nsun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\nsun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\njava.lang.reflect.Constructor.newInstance(Constructor.java:423)\npy4j.reflection.MethodInvoker.invoke(MethodInvoker.java:234)\npy4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:381)\npy4j.Gateway.invoke(Gateway.java:214)\npy4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:79)\npy4j.commands.ConstructorCommand.execute(ConstructorCommand.java:68)\npy4j.GatewayConnection.run(GatewayConnection.java:209)\njava.lang.Thread.run(Thread.java:745)","Submission Time":1467139555691,"Completion Time":1467139578118,"Accumulables":[]}}
{"Event":"SparkListenerStageSubmitted","Stage Info":{"Stage ID":3,"Stage Attempt ID":1,"Stage Name":"saveAsTextFile at NativeMethodAccessorImpl.java:-2","Number of Tasks":4,"RDD Info":[{"RDD ID":10,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"11\",\"name\":\"saveAsTextFile\"}","Callsite":"saveAsTextFile at NativeMethodAccessorImpl.java:-2","Parent IDs":[9],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":4,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":8,"Name":"PythonRDD","Callsite":"RDD at PythonRDD.scala:43","Parent IDs":[7],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":4,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":6,"Name":"ShuffledRDD","Scope":"{\"id\":\"8\",\"name\":\"partitionBy\"}","Callsite":"partitionBy at NativeMethodAccessorImpl.java:-2","Parent IDs":[5],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":4,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":9,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"10\",\"name\":\"map\"}","Callsite":"map at NativeMethodAccessorImpl.java:-2","Parent IDs":[8],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":4,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":7,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"9\",\"name\":\"mapPartitions\"}","Callsite":"mapPartitions at PythonRDD.scala:374","Parent IDs":[6],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":4,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[2],"Details":"org.apache.spark.api.java.AbstractJavaRDDLike.saveAsTextFile(JavaRDDLike.scala:46)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:498)\npy4j.reflection.MethodInvoker.invoke(MethodInvoker.java:231)\npy4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:381)\npy4j.Gateway.invoke(Gateway.java:259)\npy4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:133)\npy4j.commands.CallCommand.execute(CallCommand.java:79)\npy4j.GatewayConnection.run(GatewayConnection.java:209)\njava.lang.Thread.run(Thread.java:745)","Accumulables":[]},"Properties":{"spark.rdd.scope.noOverride":"true","spark.rdd.scope":"{\"id\":\"11\",\"name\":\"saveAsTextFile\"}"}}
{"Event":"SparkListenerTaskStart","Stage ID":3,"Stage Attempt ID":1,"Task Info":{"Task ID":20,"Index":0,"Attempt":0,"Launch Time":1467139578141,"Executor ID":"1","Host":"192.168.1.3","Locality":"ANY","Speculative":false,"Getting Result Time":0,"Finish Time":0,"Failed":false,"Accumulables":[]}}
{"Event":"SparkListenerTaskStart","Stage ID":3,"Stage Attempt ID":1,"Task Info":{"Task ID":21,"Index":1,"Attempt":0,"Launch Time":1467139578141,"Executor ID":"0","Host":"192.168.1.3","Locality":"ANY","Speculative":false,"Getting Result Time":0,"Finish Time":0,"Failed":false,"Accumulables":[]}}
{"Event":"SparkListenerTaskStart","Stage ID":3,"Stage Attempt ID":1,"Task Info":{"Task ID":22,"Index":2,"Attempt":0,"Launch Time":1467139578142,"Executor ID":"1","Host":"192.168.1.3","Locality":"ANY","Speculative":false,"Getting Result Time":0,"Finish Time":0,"Failed":false,"Accumulables":[]}}
{"Event":"SparkListenerTaskStart","Stage ID":3,"Stage Attempt ID":1,"Task Info":{"Task ID":23,"Index":3,"Attempt":0,"Launch Time":1467139578142,"Executor ID":"0","Host":"192.168.1.3","Locality":"ANY","Speculative":false,"Getting Result Time":0,"Finish Time":0,"Failed":false,"Accumulables":[]}}
{"Event":"SparkListenerTaskEnd","Stage ID":3,"Stage Attempt ID":1,"Task Type":"ResultTask","Task End Reason":{"Reason":"FetchFailed","Block Manager Address":{"Executor ID":"1","Host":"192.168.1.11","Port":35409},"Shuffle ID":0,"Map ID":2,"Reduce ID":1,"Message":"org.apache.spark.shuffle.FetchFailedException: Failed to connect to /192.168.1.11:35409\n\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.throwFetchFailedException(ShuffleBlockFetcherIterator.scala:323)\n\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:300)\n\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:51)\n\tat scala.collection.Iterator$$anon$11.next(Iterator.scala:328)\n\tat scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)\n\tat org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:32)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:39)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:727)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1157)\n\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:452)\n\tat org.apache.spark.api.python.PythonRunner$WriterThread$$anonfun$run$3.apply(PythonRDD.scala:280)\n\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1765)\n\tat org.apache.spark.api.python.PythonRunner$WriterThread.run(PythonRDD.scala:239)\nCaused by: java.io.IOException: Failed to connect to /192.168.1.11:35409\n\tat org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:216)\n\tat org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:167)\n\tat org.apache.spark.network.netty.NettyBlockTransferService$$anon$1.createAndStart(NettyBlockTransferService.scala:90)\n\tat org.apache.spark.network.shuffle.RetryingBlockFetcher.fetchAllOutstanding(RetryingBlockFetcher.java:140)\n\tat org.apache.spark.network.shuffle.RetryingBlockFetcher.access$200(RetryingBlockFetcher.java:43)\n\tat org.apache.spark.network.shuffle.RetryingBlockFetcher$1.run(RetryingBlockFetcher.java:170)\n\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n\tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n\tat java.lang.Thread.run(Thread.java:745)\nCaused by: java.net.NoRouteToHostException: No route to host: /192.168.1.11:35409\n\tat sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)\n\tat sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)\n\tat io.netty.channel.socket.nio.NioSocketChannel.doFinishConnect(NioSocketChannel.java:224)\n\tat io.netty.channel.nio.AbstractNioChannel$AbstractNioUnsafe.finishConnect(AbstractNioChannel.java:289)\n\tat io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:528)\n\tat io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:468)\n\tat io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:382)\n\tat io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:354)\n\tat io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:111)\n\t... 1 more\n"},"Task Info":{"Task ID":21,"Index":1,"Attempt":0,"Launch Time":1467139578141,"Executor ID":"0","Host":"192.168.1.3","Locality":"ANY","Speculative":false,"Getting Result Time":0,"Finish Time":1467139617545,"Failed":true,"Accumulables":[]}}
{"Event":"SparkListenerStageCompleted","Stage Info":{"Stage ID":3,"Stage Attempt ID":1,"Stage Name":"saveAsTextFile at NativeMethodAccessorImpl.java:-2","Number of Tasks":4,"RDD Info":[{"RDD ID":10,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"11\",\"name\":\"saveAsTextFile\"}","Callsite":"saveAsTextFile at NativeMethodAccessorImpl.java:-2","Parent IDs":[9],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":4,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":8,"Name":"PythonRDD","Callsite":"RDD at PythonRDD.scala:43","Parent IDs":[7],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":4,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":6,"Name":"ShuffledRDD","Scope":"{\"id\":\"8\",\"name\":\"partitionBy\"}","Callsite":"partitionBy at NativeMethodAccessorImpl.java:-2","Parent IDs":[5],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":4,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":9,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"10\",\"name\":\"map\"}","Callsite":"map at NativeMethodAccessorImpl.java:-2","Parent IDs":[8],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":4,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":7,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"9\",\"name\":\"mapPartitions\"}","Callsite":"mapPartitions at PythonRDD.scala:374","Parent IDs":[6],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":4,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[2],"Details":"org.apache.spark.api.java.AbstractJavaRDDLike.saveAsTextFile(JavaRDDLike.scala:46)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:498)\npy4j.reflection.MethodInvoker.invoke(MethodInvoker.java:231)\npy4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:381)\npy4j.Gateway.invoke(Gateway.java:259)\npy4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:133)\npy4j.commands.CallCommand.execute(CallCommand.java:79)\npy4j.GatewayConnection.run(GatewayConnection.java:209)\njava.lang.Thread.run(Thread.java:745)","Submission Time":1467139578140,"Completion Time":1467139617545,"Failure Reason":"org.apache.spark.shuffle.FetchFailedException: Failed to connect to /192.168.1.11:35409\n\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.throwFetchFailedException(ShuffleBlockFetcherIterator.scala:323)\n\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:300)\n\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:51)\n\tat scala.collection.Iterator$$anon$11.next(Iterator.scala:328)\n\tat scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)\n\tat org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:32)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:39)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:727)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1157)\n\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:452)\n\tat org.apache.spark.api.python.PythonRunner$WriterThread$$anonfun$run$3.apply(PythonRDD.scala:280)\n\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1765)\n\tat org.apache.spark.api.python.PythonRunner$WriterThread.run(PythonRDD.scala:239)\nCaused by: java.io.IOException: Failed to connect to /192.168.1.11:35409\n\tat org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:216)\n\tat org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:167)\n\tat org.apache.spark.network.netty.NettyBlockTransferService$$anon$1.createAndStart(NettyBlockTransferService.scala:90)\n\tat org.apache.spark.network.shuffle.RetryingBlockFetcher.fetchAllOutstanding(RetryingBlockFetcher.java:140)\n\tat org.apache.spark.network.shuffle.RetryingBlockFetcher.access$200(RetryingBlockFetcher.java:43)\n\tat org.apache.spark.network.shuffle.RetryingBlockFetcher$1.run(RetryingBlockFetcher.java:170)\n\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n\tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n\tat java.lang.Thread.run(Thread.java:745)\nCaused by: java.net.NoRouteToHostException: No route to host: /192.168.1.11:35409\n\tat sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)\n\tat sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)\n\tat io.netty.channel.socket.nio.NioSocketChannel.doFinishConnect(NioSocketChannel.java:224)\n\tat io.netty.channel.nio.AbstractNioChannel$AbstractNioUnsafe.finishConnect(AbstractNioChannel.java:289)\n\tat io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:528)\n\tat io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:468)\n\tat io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:382)\n\tat io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:354)\n\tat io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:111)\n\t... 1 more\n","Accumulables":[]}}
{"Event":"SparkListenerBlockManagerRemoved","Block Manager ID":{"Executor ID":"1","Host":"192.168.1.11","Port":35409},"Timestamp":1467139617546}
{"Event":"SparkListenerStageSubmitted","Stage Info":{"Stage ID":2,"Stage Attempt ID":3,"Stage Name":"sortByKey at /home/daniar/documents/SPARK/spark-1.6.1/sort.py:12","Number of Tasks":2,"RDD Info":[{"RDD ID":5,"Name":"PairwiseRDD","Callsite":"sortByKey at /home/daniar/documents/SPARK/spark-1.6.1/sort.py:12","Parent IDs":[4],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":4,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":0,"Name":"../generated_file/list_int","Scope":"{\"id\":\"0\",\"name\":\"textFile\"}","Callsite":"textFile at NativeMethodAccessorImpl.java:-2","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":4,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":4,"Name":"PythonRDD","Callsite":"sortByKey at /home/daniar/documents/SPARK/spark-1.6.1/sort.py:12","Parent IDs":[1],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":4,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":1,"Name":"../generated_file/list_int","Scope":"{\"id\":\"0\",\"name\":\"textFile\"}","Callsite":"textFile at NativeMethodAccessorImpl.java:-2","Parent IDs":[0],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":4,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.<init>(RDD.scala:98)\norg.apache.spark.api.python.PairwiseRDD.<init>(PythonRDD.scala:338)\nsun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\nsun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\nsun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\njava.lang.reflect.Constructor.newInstance(Constructor.java:423)\npy4j.reflection.MethodInvoker.invoke(MethodInvoker.java:234)\npy4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:381)\npy4j.Gateway.invoke(Gateway.java:214)\npy4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:79)\npy4j.commands.ConstructorCommand.execute(ConstructorCommand.java:68)\npy4j.GatewayConnection.run(GatewayConnection.java:209)\njava.lang.Thread.run(Thread.java:745)","Accumulables":[]},"Properties":{"spark.rdd.scope.noOverride":"true","spark.rdd.scope":"{\"id\":\"11\",\"name\":\"saveAsTextFile\"}"}}
{"Event":"SparkListenerTaskStart","Stage ID":2,"Stage Attempt ID":3,"Task Info":{"Task ID":24,"Index":0,"Attempt":0,"Launch Time":1467139617750,"Executor ID":"1","Host":"192.168.1.3","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":0,"Failed":false,"Accumulables":[]}}
{"Event":"SparkListenerTaskStart","Stage ID":2,"Stage Attempt ID":3,"Task Info":{"Task ID":25,"Index":1,"Attempt":0,"Launch Time":1467139617750,"Executor ID":"0","Host":"192.168.1.3","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":0,"Failed":false,"Accumulables":[]}}
{"Event":"SparkListenerBlockManagerAdded","Block Manager ID":{"Executor ID":"1","Host":"192.168.1.11","Port":35409},"Maximum Memory":535953408,"Timestamp":1467139617795}
{"Event":"SparkListenerTaskEnd","Stage ID":3,"Stage Attempt ID":1,"Task Type":"ResultTask","Task End Reason":{"Reason":"FetchFailed","Block Manager Address":{"Executor ID":"0","Host":"192.168.1.11","Port":33800},"Shuffle ID":0,"Map ID":3,"Reduce ID":2,"Message":"org.apache.spark.shuffle.FetchFailedException: Failed to connect to /192.168.1.11:33800\n\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.throwFetchFailedException(ShuffleBlockFetcherIterator.scala:323)\n\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:300)\n\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:51)\n\tat scala.collection.Iterator$$anon$11.next(Iterator.scala:328)\n\tat scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)\n\tat org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:32)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:39)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:727)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1157)\n\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:452)\n\tat org.apache.spark.api.python.PythonRunner$WriterThread$$anonfun$run$3.apply(PythonRDD.scala:280)\n\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1765)\n\tat org.apache.spark.api.python.PythonRunner$WriterThread.run(PythonRDD.scala:239)\nCaused by: java.io.IOException: Failed to connect to /192.168.1.11:33800\n\tat org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:216)\n\tat org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:167)\n\tat org.apache.spark.network.netty.NettyBlockTransferService$$anon$1.createAndStart(NettyBlockTransferService.scala:90)\n\tat org.apache.spark.network.shuffle.RetryingBlockFetcher.fetchAllOutstanding(RetryingBlockFetcher.java:140)\n\tat org.apache.spark.network.shuffle.RetryingBlockFetcher.access$200(RetryingBlockFetcher.java:43)\n\tat org.apache.spark.network.shuffle.RetryingBlockFetcher$1.run(RetryingBlockFetcher.java:170)\n\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n\tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n\tat java.lang.Thread.run(Thread.java:745)\nCaused by: java.net.NoRouteToHostException: No route to host: /192.168.1.11:33800\n\tat sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)\n\tat sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)\n\tat io.netty.channel.socket.nio.NioSocketChannel.doFinishConnect(NioSocketChannel.java:224)\n\tat io.netty.channel.nio.AbstractNioChannel$AbstractNioUnsafe.finishConnect(AbstractNioChannel.java:289)\n\tat io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:528)\n\tat io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:468)\n\tat io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:382)\n\tat io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:354)\n\tat io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:111)\n\t... 1 more\n"},"Task Info":{"Task ID":22,"Index":2,"Attempt":0,"Launch Time":1467139578142,"Executor ID":"1","Host":"192.168.1.3","Locality":"ANY","Speculative":false,"Getting Result Time":0,"Finish Time":1467139619302,"Failed":true,"Accumulables":[]}}
{"Event":"SparkListenerBlockManagerRemoved","Block Manager ID":{"Executor ID":"0","Host":"192.168.1.11","Port":33800},"Timestamp":1467139619304}
{"Event":"SparkListenerTaskEnd","Stage ID":3,"Stage Attempt ID":1,"Task Type":"ResultTask","Task End Reason":{"Reason":"FetchFailed","Block Manager Address":{"Executor ID":"0","Host":"192.168.1.11","Port":33800},"Shuffle ID":0,"Map ID":3,"Reduce ID":0,"Message":"org.apache.spark.shuffle.FetchFailedException: Failed to connect to /192.168.1.11:33800\n\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.throwFetchFailedException(ShuffleBlockFetcherIterator.scala:323)\n\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:300)\n\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:51)\n\tat scala.collection.Iterator$$anon$11.next(Iterator.scala:328)\n\tat scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)\n\tat org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:32)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:39)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:727)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1157)\n\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:452)\n\tat org.apache.spark.api.python.PythonRunner$WriterThread$$anonfun$run$3.apply(PythonRDD.scala:280)\n\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1765)\n\tat org.apache.spark.api.python.PythonRunner$WriterThread.run(PythonRDD.scala:239)\nCaused by: java.io.IOException: Failed to connect to /192.168.1.11:33800\n\tat org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:216)\n\tat org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:167)\n\tat org.apache.spark.network.netty.NettyBlockTransferService$$anon$1.createAndStart(NettyBlockTransferService.scala:90)\n\tat org.apache.spark.network.shuffle.RetryingBlockFetcher.fetchAllOutstanding(RetryingBlockFetcher.java:140)\n\tat org.apache.spark.network.shuffle.RetryingBlockFetcher.access$200(RetryingBlockFetcher.java:43)\n\tat org.apache.spark.network.shuffle.RetryingBlockFetcher$1.run(RetryingBlockFetcher.java:170)\n\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n\tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n\tat java.lang.Thread.run(Thread.java:745)\nCaused by: java.net.NoRouteToHostException: No route to host: /192.168.1.11:33800\n\tat sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)\n\tat sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)\n\tat io.netty.channel.socket.nio.NioSocketChannel.doFinishConnect(NioSocketChannel.java:224)\n\tat io.netty.channel.nio.AbstractNioChannel$AbstractNioUnsafe.finishConnect(AbstractNioChannel.java:289)\n\tat io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:528)\n\tat io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:468)\n\tat io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:382)\n\tat io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:354)\n\tat io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:111)\n\t... 1 more\n"},"Task Info":{"Task ID":20,"Index":0,"Attempt":0,"Launch Time":1467139578141,"Executor ID":"1","Host":"192.168.1.3","Locality":"ANY","Speculative":false,"Getting Result Time":0,"Finish Time":1467139625796,"Failed":true,"Accumulables":[]}}
{"Event":"SparkListenerBlockManagerAdded","Block Manager ID":{"Executor ID":"0","Host":"192.168.1.11","Port":33800},"Maximum Memory":535953408,"Timestamp":1467139626389}
{"Event":"SparkListenerTaskEnd","Stage ID":3,"Stage Attempt ID":1,"Task Type":"ResultTask","Task End Reason":{"Reason":"FetchFailed","Block Manager Address":{"Executor ID":"1","Host":"192.168.1.11","Port":35409},"Shuffle ID":0,"Map ID":2,"Reduce ID":3,"Message":"org.apache.spark.shuffle.FetchFailedException: Failed to connect to /192.168.1.11:35409\n\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.throwFetchFailedException(ShuffleBlockFetcherIterator.scala:323)\n\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:300)\n\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:51)\n\tat scala.collection.Iterator$$anon$11.next(Iterator.scala:328)\n\tat scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)\n\tat org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:32)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:39)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:727)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1157)\n\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:452)\n\tat org.apache.spark.api.python.PythonRunner$WriterThread$$anonfun$run$3.apply(PythonRDD.scala:280)\n\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1765)\n\tat org.apache.spark.api.python.PythonRunner$WriterThread.run(PythonRDD.scala:239)\nCaused by: java.io.IOException: Failed to connect to /192.168.1.11:35409\n\tat org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:216)\n\tat org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:167)\n\tat org.apache.spark.network.netty.NettyBlockTransferService$$anon$1.createAndStart(NettyBlockTransferService.scala:90)\n\tat org.apache.spark.network.shuffle.RetryingBlockFetcher.fetchAllOutstanding(RetryingBlockFetcher.java:140)\n\tat org.apache.spark.network.shuffle.RetryingBlockFetcher.access$200(RetryingBlockFetcher.java:43)\n\tat org.apache.spark.network.shuffle.RetryingBlockFetcher$1.run(RetryingBlockFetcher.java:170)\n\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n\tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n\tat java.lang.Thread.run(Thread.java:745)\nCaused by: java.net.NoRouteToHostException: No route to host: /192.168.1.11:35409\n\tat sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)\n\tat sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)\n\tat io.netty.channel.socket.nio.NioSocketChannel.doFinishConnect(NioSocketChannel.java:224)\n\tat io.netty.channel.nio.AbstractNioChannel$AbstractNioUnsafe.finishConnect(AbstractNioChannel.java:289)\n\tat io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:528)\n\tat io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:468)\n\tat io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:382)\n\tat io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:354)\n\tat io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:111)\n\t... 1 more\n"},"Task Info":{"Task ID":23,"Index":3,"Attempt":0,"Launch Time":1467139578142,"Executor ID":"0","Host":"192.168.1.3","Locality":"ANY","Speculative":false,"Getting Result Time":0,"Finish Time":1467139628221,"Failed":true,"Accumulables":[]}}
{"Event":"SparkListenerTaskEnd","Stage ID":2,"Stage Attempt ID":3,"Task Type":"ShuffleMapTask","Task End Reason":{"Reason":"Success"},"Task Info":{"Task ID":25,"Index":1,"Attempt":0,"Launch Time":1467139617750,"Executor ID":"0","Host":"192.168.1.3","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":1467139644951,"Failed":false,"Accumulables":[]},"Task Metrics":{"Host Name":"192.168.1.11","Executor Deserialize Time":15,"Executor Run Time":27182,"Result Size":2321,"JVM GC Time":178,"Result Serialization Time":0,"Memory Bytes Spilled":0,"Disk Bytes Spilled":0,"Shuffle Write Metrics":{"Shuffle Bytes Written":65352156,"Shuffle Write Time":171489442,"Shuffle Records Written":164},"Input Metrics":{"Data Read Method":"Hadoop","Bytes Read":65536,"Records Read":4253236}}}
{"Event":"SparkListenerTaskEnd","Stage ID":2,"Stage Attempt ID":3,"Task Type":"ShuffleMapTask","Task End Reason":{"Reason":"Success"},"Task Info":{"Task ID":24,"Index":0,"Attempt":0,"Launch Time":1467139617750,"Executor ID":"1","Host":"192.168.1.3","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":1467139645379,"Failed":false,"Accumulables":[]},"Task Metrics":{"Host Name":"192.168.1.11","Executor Deserialize Time":42,"Executor Run Time":27582,"Result Size":2321,"JVM GC Time":158,"Result Serialization Time":0,"Memory Bytes Spilled":0,"Disk Bytes Spilled":0,"Shuffle Write Metrics":{"Shuffle Bytes Written":65350616,"Shuffle Write Time":177721497,"Shuffle Records Written":164},"Input Metrics":{"Data Read Method":"Hadoop","Bytes Read":65536,"Records Read":4253268}}}
{"Event":"SparkListenerStageCompleted","Stage Info":{"Stage ID":2,"Stage Attempt ID":3,"Stage Name":"sortByKey at /home/daniar/documents/SPARK/spark-1.6.1/sort.py:12","Number of Tasks":2,"RDD Info":[{"RDD ID":5,"Name":"PairwiseRDD","Callsite":"sortByKey at /home/daniar/documents/SPARK/spark-1.6.1/sort.py:12","Parent IDs":[4],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":4,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":0,"Name":"../generated_file/list_int","Scope":"{\"id\":\"0\",\"name\":\"textFile\"}","Callsite":"textFile at NativeMethodAccessorImpl.java:-2","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":4,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":4,"Name":"PythonRDD","Callsite":"sortByKey at /home/daniar/documents/SPARK/spark-1.6.1/sort.py:12","Parent IDs":[1],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":4,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":1,"Name":"../generated_file/list_int","Scope":"{\"id\":\"0\",\"name\":\"textFile\"}","Callsite":"textFile at NativeMethodAccessorImpl.java:-2","Parent IDs":[0],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":4,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.<init>(RDD.scala:98)\norg.apache.spark.api.python.PairwiseRDD.<init>(PythonRDD.scala:338)\nsun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\nsun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\nsun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\njava.lang.reflect.Constructor.newInstance(Constructor.java:423)\npy4j.reflection.MethodInvoker.invoke(MethodInvoker.java:234)\npy4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:381)\npy4j.Gateway.invoke(Gateway.java:214)\npy4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:79)\npy4j.commands.ConstructorCommand.execute(ConstructorCommand.java:68)\npy4j.GatewayConnection.run(GatewayConnection.java:209)\njava.lang.Thread.run(Thread.java:745)","Submission Time":1467139617750,"Completion Time":1467139645380,"Accumulables":[]}}
{"Event":"SparkListenerStageSubmitted","Stage Info":{"Stage ID":2,"Stage Attempt ID":4,"Stage Name":"sortByKey at /home/daniar/documents/SPARK/spark-1.6.1/sort.py:12","Number of Tasks":2,"RDD Info":[{"RDD ID":5,"Name":"PairwiseRDD","Callsite":"sortByKey at /home/daniar/documents/SPARK/spark-1.6.1/sort.py:12","Parent IDs":[4],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":4,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":0,"Name":"../generated_file/list_int","Scope":"{\"id\":\"0\",\"name\":\"textFile\"}","Callsite":"textFile at NativeMethodAccessorImpl.java:-2","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":4,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":4,"Name":"PythonRDD","Callsite":"sortByKey at /home/daniar/documents/SPARK/spark-1.6.1/sort.py:12","Parent IDs":[1],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":4,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":1,"Name":"../generated_file/list_int","Scope":"{\"id\":\"0\",\"name\":\"textFile\"}","Callsite":"textFile at NativeMethodAccessorImpl.java:-2","Parent IDs":[0],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":4,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.<init>(RDD.scala:98)\norg.apache.spark.api.python.PairwiseRDD.<init>(PythonRDD.scala:338)\nsun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\nsun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\nsun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\njava.lang.reflect.Constructor.newInstance(Constructor.java:423)\npy4j.reflection.MethodInvoker.invoke(MethodInvoker.java:234)\npy4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:381)\npy4j.Gateway.invoke(Gateway.java:214)\npy4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:79)\npy4j.commands.ConstructorCommand.execute(ConstructorCommand.java:68)\npy4j.GatewayConnection.run(GatewayConnection.java:209)\njava.lang.Thread.run(Thread.java:745)","Accumulables":[]},"Properties":{"spark.rdd.scope.noOverride":"true","spark.rdd.scope":"{\"id\":\"11\",\"name\":\"saveAsTextFile\"}"}}
{"Event":"SparkListenerTaskStart","Stage ID":2,"Stage Attempt ID":4,"Task Info":{"Task ID":26,"Index":0,"Attempt":0,"Launch Time":1467139645385,"Executor ID":"0","Host":"192.168.1.3","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":0,"Failed":false,"Accumulables":[]}}
{"Event":"SparkListenerTaskStart","Stage ID":2,"Stage Attempt ID":4,"Task Info":{"Task ID":27,"Index":1,"Attempt":0,"Launch Time":1467139645385,"Executor ID":"1","Host":"192.168.1.3","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":0,"Failed":false,"Accumulables":[]}}
{"Event":"SparkListenerTaskEnd","Stage ID":2,"Stage Attempt ID":4,"Task Type":"ShuffleMapTask","Task End Reason":{"Reason":"Success"},"Task Info":{"Task ID":27,"Index":1,"Attempt":0,"Launch Time":1467139645385,"Executor ID":"1","Host":"192.168.1.3","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":1467139669000,"Failed":false,"Accumulables":[]},"Task Metrics":{"Host Name":"192.168.1.11","Executor Deserialize Time":17,"Executor Run Time":23590,"Result Size":2321,"JVM GC Time":158,"Result Serialization Time":0,"Memory Bytes Spilled":0,"Disk Bytes Spilled":0,"Shuffle Write Metrics":{"Shuffle Bytes Written":53536423,"Shuffle Write Time":174243516,"Shuffle Records Written":148},"Input Metrics":{"Data Read Method":"Hadoop","Bytes Read":65536,"Records Read":3484461}}}
{"Event":"SparkListenerTaskEnd","Stage ID":2,"Stage Attempt ID":4,"Task Type":"ShuffleMapTask","Task End Reason":{"Reason":"Success"},"Task Info":{"Task ID":26,"Index":0,"Attempt":0,"Launch Time":1467139645385,"Executor ID":"0","Host":"192.168.1.3","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":1467139674027,"Failed":false,"Accumulables":[]},"Task Metrics":{"Host Name":"192.168.1.11","Executor Deserialize Time":27,"Executor Run Time":28610,"Result Size":2321,"JVM GC Time":145,"Result Serialization Time":0,"Memory Bytes Spilled":0,"Disk Bytes Spilled":0,"Shuffle Write Metrics":{"Shuffle Bytes Written":65356545,"Shuffle Write Time":180026754,"Shuffle Records Written":164},"Input Metrics":{"Data Read Method":"Hadoop","Bytes Read":0,"Records Read":4253453}}}
{"Event":"SparkListenerStageCompleted","Stage Info":{"Stage ID":2,"Stage Attempt ID":4,"Stage Name":"sortByKey at /home/daniar/documents/SPARK/spark-1.6.1/sort.py:12","Number of Tasks":2,"RDD Info":[{"RDD ID":5,"Name":"PairwiseRDD","Callsite":"sortByKey at /home/daniar/documents/SPARK/spark-1.6.1/sort.py:12","Parent IDs":[4],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":4,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":0,"Name":"../generated_file/list_int","Scope":"{\"id\":\"0\",\"name\":\"textFile\"}","Callsite":"textFile at NativeMethodAccessorImpl.java:-2","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":4,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":4,"Name":"PythonRDD","Callsite":"sortByKey at /home/daniar/documents/SPARK/spark-1.6.1/sort.py:12","Parent IDs":[1],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":4,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":1,"Name":"../generated_file/list_int","Scope":"{\"id\":\"0\",\"name\":\"textFile\"}","Callsite":"textFile at NativeMethodAccessorImpl.java:-2","Parent IDs":[0],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":4,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.<init>(RDD.scala:98)\norg.apache.spark.api.python.PairwiseRDD.<init>(PythonRDD.scala:338)\nsun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\nsun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\nsun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\njava.lang.reflect.Constructor.newInstance(Constructor.java:423)\npy4j.reflection.MethodInvoker.invoke(MethodInvoker.java:234)\npy4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:381)\npy4j.Gateway.invoke(Gateway.java:214)\npy4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:79)\npy4j.commands.ConstructorCommand.execute(ConstructorCommand.java:68)\npy4j.GatewayConnection.run(GatewayConnection.java:209)\njava.lang.Thread.run(Thread.java:745)","Submission Time":1467139645385,"Completion Time":1467139674028,"Accumulables":[]}}
{"Event":"SparkListenerStageSubmitted","Stage Info":{"Stage ID":3,"Stage Attempt ID":2,"Stage Name":"saveAsTextFile at NativeMethodAccessorImpl.java:-2","Number of Tasks":4,"RDD Info":[{"RDD ID":10,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"11\",\"name\":\"saveAsTextFile\"}","Callsite":"saveAsTextFile at NativeMethodAccessorImpl.java:-2","Parent IDs":[9],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":4,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":8,"Name":"PythonRDD","Callsite":"RDD at PythonRDD.scala:43","Parent IDs":[7],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":4,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":6,"Name":"ShuffledRDD","Scope":"{\"id\":\"8\",\"name\":\"partitionBy\"}","Callsite":"partitionBy at NativeMethodAccessorImpl.java:-2","Parent IDs":[5],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":4,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":9,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"10\",\"name\":\"map\"}","Callsite":"map at NativeMethodAccessorImpl.java:-2","Parent IDs":[8],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":4,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":7,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"9\",\"name\":\"mapPartitions\"}","Callsite":"mapPartitions at PythonRDD.scala:374","Parent IDs":[6],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":4,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[2],"Details":"org.apache.spark.api.java.AbstractJavaRDDLike.saveAsTextFile(JavaRDDLike.scala:46)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:498)\npy4j.reflection.MethodInvoker.invoke(MethodInvoker.java:231)\npy4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:381)\npy4j.Gateway.invoke(Gateway.java:259)\npy4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:133)\npy4j.commands.CallCommand.execute(CallCommand.java:79)\npy4j.GatewayConnection.run(GatewayConnection.java:209)\njava.lang.Thread.run(Thread.java:745)","Accumulables":[]},"Properties":{"spark.rdd.scope.noOverride":"true","spark.rdd.scope":"{\"id\":\"11\",\"name\":\"saveAsTextFile\"}"}}
{"Event":"SparkListenerTaskStart","Stage ID":3,"Stage Attempt ID":2,"Task Info":{"Task ID":28,"Index":0,"Attempt":0,"Launch Time":1467139674048,"Executor ID":"0","Host":"192.168.1.3","Locality":"ANY","Speculative":false,"Getting Result Time":0,"Finish Time":0,"Failed":false,"Accumulables":[]}}
{"Event":"SparkListenerTaskStart","Stage ID":3,"Stage Attempt ID":2,"Task Info":{"Task ID":29,"Index":1,"Attempt":0,"Launch Time":1467139674048,"Executor ID":"1","Host":"192.168.1.3","Locality":"ANY","Speculative":false,"Getting Result Time":0,"Finish Time":0,"Failed":false,"Accumulables":[]}}
{"Event":"SparkListenerTaskStart","Stage ID":3,"Stage Attempt ID":2,"Task Info":{"Task ID":30,"Index":2,"Attempt":0,"Launch Time":1467139674049,"Executor ID":"0","Host":"192.168.1.3","Locality":"ANY","Speculative":false,"Getting Result Time":0,"Finish Time":0,"Failed":false,"Accumulables":[]}}
{"Event":"SparkListenerTaskStart","Stage ID":3,"Stage Attempt ID":2,"Task Info":{"Task ID":31,"Index":3,"Attempt":0,"Launch Time":1467139674049,"Executor ID":"1","Host":"192.168.1.3","Locality":"ANY","Speculative":false,"Getting Result Time":0,"Finish Time":0,"Failed":false,"Accumulables":[]}}
{"Event":"SparkListenerTaskEnd","Stage ID":3,"Stage Attempt ID":2,"Task Type":"ResultTask","Task End Reason":{"Reason":"FetchFailed","Block Manager Address":{"Executor ID":"1","Host":"192.168.1.11","Port":35409},"Shuffle ID":0,"Map ID":3,"Reduce ID":0,"Message":"org.apache.spark.shuffle.FetchFailedException: Failed to connect to /192.168.1.11:35409\n\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.throwFetchFailedException(ShuffleBlockFetcherIterator.scala:323)\n\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:300)\n\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:51)\n\tat scala.collection.Iterator$$anon$11.next(Iterator.scala:328)\n\tat scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)\n\tat org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:32)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:39)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:727)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1157)\n\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:452)\n\tat org.apache.spark.api.python.PythonRunner$WriterThread$$anonfun$run$3.apply(PythonRDD.scala:280)\n\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1765)\n\tat org.apache.spark.api.python.PythonRunner$WriterThread.run(PythonRDD.scala:239)\nCaused by: java.io.IOException: Failed to connect to /192.168.1.11:35409\n\tat org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:216)\n\tat org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:167)\n\tat org.apache.spark.network.netty.NettyBlockTransferService$$anon$1.createAndStart(NettyBlockTransferService.scala:90)\n\tat org.apache.spark.network.shuffle.RetryingBlockFetcher.fetchAllOutstanding(RetryingBlockFetcher.java:140)\n\tat org.apache.spark.network.shuffle.RetryingBlockFetcher.access$200(RetryingBlockFetcher.java:43)\n\tat org.apache.spark.network.shuffle.RetryingBlockFetcher$1.run(RetryingBlockFetcher.java:170)\n\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n\tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n\tat java.lang.Thread.run(Thread.java:745)\nCaused by: java.net.NoRouteToHostException: No route to host: /192.168.1.11:35409\n\tat sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)\n\tat sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)\n\tat io.netty.channel.socket.nio.NioSocketChannel.doFinishConnect(NioSocketChannel.java:224)\n\tat io.netty.channel.nio.AbstractNioChannel$AbstractNioUnsafe.finishConnect(AbstractNioChannel.java:289)\n\tat io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:528)\n\tat io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:468)\n\tat io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:382)\n\tat io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:354)\n\tat io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:111)\n\t... 1 more\n"},"Task Info":{"Task ID":28,"Index":0,"Attempt":0,"Launch Time":1467139674048,"Executor ID":"0","Host":"192.168.1.3","Locality":"ANY","Speculative":false,"Getting Result Time":0,"Finish Time":1467139711690,"Failed":true,"Accumulables":[]}}
{"Event":"SparkListenerStageCompleted","Stage Info":{"Stage ID":3,"Stage Attempt ID":2,"Stage Name":"saveAsTextFile at NativeMethodAccessorImpl.java:-2","Number of Tasks":4,"RDD Info":[{"RDD ID":10,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"11\",\"name\":\"saveAsTextFile\"}","Callsite":"saveAsTextFile at NativeMethodAccessorImpl.java:-2","Parent IDs":[9],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":4,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":8,"Name":"PythonRDD","Callsite":"RDD at PythonRDD.scala:43","Parent IDs":[7],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":4,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":6,"Name":"ShuffledRDD","Scope":"{\"id\":\"8\",\"name\":\"partitionBy\"}","Callsite":"partitionBy at NativeMethodAccessorImpl.java:-2","Parent IDs":[5],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":4,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":9,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"10\",\"name\":\"map\"}","Callsite":"map at NativeMethodAccessorImpl.java:-2","Parent IDs":[8],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":4,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":7,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"9\",\"name\":\"mapPartitions\"}","Callsite":"mapPartitions at PythonRDD.scala:374","Parent IDs":[6],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":4,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[2],"Details":"org.apache.spark.api.java.AbstractJavaRDDLike.saveAsTextFile(JavaRDDLike.scala:46)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:498)\npy4j.reflection.MethodInvoker.invoke(MethodInvoker.java:231)\npy4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:381)\npy4j.Gateway.invoke(Gateway.java:259)\npy4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:133)\npy4j.commands.CallCommand.execute(CallCommand.java:79)\npy4j.GatewayConnection.run(GatewayConnection.java:209)\njava.lang.Thread.run(Thread.java:745)","Submission Time":1467139674048,"Completion Time":1467139711691,"Failure Reason":"org.apache.spark.shuffle.FetchFailedException: Failed to connect to /192.168.1.11:35409\n\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.throwFetchFailedException(ShuffleBlockFetcherIterator.scala:323)\n\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:300)\n\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:51)\n\tat scala.collection.Iterator$$anon$11.next(Iterator.scala:328)\n\tat scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)\n\tat org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:32)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:39)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:727)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1157)\n\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:452)\n\tat org.apache.spark.api.python.PythonRunner$WriterThread$$anonfun$run$3.apply(PythonRDD.scala:280)\n\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1765)\n\tat org.apache.spark.api.python.PythonRunner$WriterThread.run(PythonRDD.scala:239)\nCaused by: java.io.IOException: Failed to connect to /192.168.1.11:35409\n\tat org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:216)\n\tat org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:167)\n\tat org.apache.spark.network.netty.NettyBlockTransferService$$anon$1.createAndStart(NettyBlockTransferService.scala:90)\n\tat org.apache.spark.network.shuffle.RetryingBlockFetcher.fetchAllOutstanding(RetryingBlockFetcher.java:140)\n\tat org.apache.spark.network.shuffle.RetryingBlockFetcher.access$200(RetryingBlockFetcher.java:43)\n\tat org.apache.spark.network.shuffle.RetryingBlockFetcher$1.run(RetryingBlockFetcher.java:170)\n\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n\tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n\tat java.lang.Thread.run(Thread.java:745)\nCaused by: java.net.NoRouteToHostException: No route to host: /192.168.1.11:35409\n\tat sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)\n\tat sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)\n\tat io.netty.channel.socket.nio.NioSocketChannel.doFinishConnect(NioSocketChannel.java:224)\n\tat io.netty.channel.nio.AbstractNioChannel$AbstractNioUnsafe.finishConnect(AbstractNioChannel.java:289)\n\tat io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:528)\n\tat io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:468)\n\tat io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:382)\n\tat io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:354)\n\tat io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:111)\n\t... 1 more\n","Accumulables":[]}}
{"Event":"SparkListenerBlockManagerRemoved","Block Manager ID":{"Executor ID":"1","Host":"192.168.1.11","Port":35409},"Timestamp":1467139711692}
{"Event":"SparkListenerStageSubmitted","Stage Info":{"Stage ID":2,"Stage Attempt ID":5,"Stage Name":"sortByKey at /home/daniar/documents/SPARK/spark-1.6.1/sort.py:12","Number of Tasks":2,"RDD Info":[{"RDD ID":5,"Name":"PairwiseRDD","Callsite":"sortByKey at /home/daniar/documents/SPARK/spark-1.6.1/sort.py:12","Parent IDs":[4],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":4,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":0,"Name":"../generated_file/list_int","Scope":"{\"id\":\"0\",\"name\":\"textFile\"}","Callsite":"textFile at NativeMethodAccessorImpl.java:-2","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":4,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":4,"Name":"PythonRDD","Callsite":"sortByKey at /home/daniar/documents/SPARK/spark-1.6.1/sort.py:12","Parent IDs":[1],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":4,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":1,"Name":"../generated_file/list_int","Scope":"{\"id\":\"0\",\"name\":\"textFile\"}","Callsite":"textFile at NativeMethodAccessorImpl.java:-2","Parent IDs":[0],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":4,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.<init>(RDD.scala:98)\norg.apache.spark.api.python.PairwiseRDD.<init>(PythonRDD.scala:338)\nsun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\nsun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\nsun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\njava.lang.reflect.Constructor.newInstance(Constructor.java:423)\npy4j.reflection.MethodInvoker.invoke(MethodInvoker.java:234)\npy4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:381)\npy4j.Gateway.invoke(Gateway.java:214)\npy4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:79)\npy4j.commands.ConstructorCommand.execute(ConstructorCommand.java:68)\npy4j.GatewayConnection.run(GatewayConnection.java:209)\njava.lang.Thread.run(Thread.java:745)","Accumulables":[]},"Properties":{"spark.rdd.scope.noOverride":"true","spark.rdd.scope":"{\"id\":\"11\",\"name\":\"saveAsTextFile\"}"}}
{"Event":"SparkListenerTaskStart","Stage ID":2,"Stage Attempt ID":5,"Task Info":{"Task ID":32,"Index":0,"Attempt":0,"Launch Time":1467139711896,"Executor ID":"0","Host":"192.168.1.3","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":0,"Failed":false,"Accumulables":[]}}
{"Event":"SparkListenerTaskStart","Stage ID":2,"Stage Attempt ID":5,"Task Info":{"Task ID":33,"Index":1,"Attempt":0,"Launch Time":1467139711896,"Executor ID":"1","Host":"192.168.1.3","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":0,"Failed":false,"Accumulables":[]}}
{"Event":"SparkListenerBlockManagerAdded","Block Manager ID":{"Executor ID":"1","Host":"192.168.1.11","Port":35409},"Maximum Memory":535953408,"Timestamp":1467139711917}
{"Event":"SparkListenerTaskEnd","Stage ID":3,"Stage Attempt ID":2,"Task Type":"ResultTask","Task End Reason":{"Reason":"FetchFailed","Block Manager Address":{"Executor ID":"0","Host":"192.168.1.11","Port":33800},"Shuffle ID":0,"Map ID":2,"Reduce ID":1,"Message":"org.apache.spark.shuffle.FetchFailedException: Failed to connect to /192.168.1.11:33800\n\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.throwFetchFailedException(ShuffleBlockFetcherIterator.scala:323)\n\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:300)\n\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:51)\n\tat scala.collection.Iterator$$anon$11.next(Iterator.scala:328)\n\tat scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)\n\tat org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:32)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:39)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:727)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1157)\n\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:452)\n\tat org.apache.spark.api.python.PythonRunner$WriterThread$$anonfun$run$3.apply(PythonRDD.scala:280)\n\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1765)\n\tat org.apache.spark.api.python.PythonRunner$WriterThread.run(PythonRDD.scala:239)\nCaused by: java.io.IOException: Failed to connect to /192.168.1.11:33800\n\tat org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:216)\n\tat org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:167)\n\tat org.apache.spark.network.netty.NettyBlockTransferService$$anon$1.createAndStart(NettyBlockTransferService.scala:90)\n\tat org.apache.spark.network.shuffle.RetryingBlockFetcher.fetchAllOutstanding(RetryingBlockFetcher.java:140)\n\tat org.apache.spark.network.shuffle.RetryingBlockFetcher.access$200(RetryingBlockFetcher.java:43)\n\tat org.apache.spark.network.shuffle.RetryingBlockFetcher$1.run(RetryingBlockFetcher.java:170)\n\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n\tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n\tat java.lang.Thread.run(Thread.java:745)\nCaused by: java.net.NoRouteToHostException: No route to host: /192.168.1.11:33800\n\tat sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)\n\tat sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)\n\tat io.netty.channel.socket.nio.NioSocketChannel.doFinishConnect(NioSocketChannel.java:224)\n\tat io.netty.channel.nio.AbstractNioChannel$AbstractNioUnsafe.finishConnect(AbstractNioChannel.java:289)\n\tat io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:528)\n\tat io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:468)\n\tat io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:382)\n\tat io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:354)\n\tat io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:111)\n\t... 1 more\n"},"Task Info":{"Task ID":29,"Index":1,"Attempt":0,"Launch Time":1467139674048,"Executor ID":"1","Host":"192.168.1.3","Locality":"ANY","Speculative":false,"Getting Result Time":0,"Finish Time":1467139713075,"Failed":true,"Accumulables":[]}}
{"Event":"SparkListenerBlockManagerRemoved","Block Manager ID":{"Executor ID":"0","Host":"192.168.1.11","Port":33800},"Timestamp":1467139713077}
{"Event":"SparkListenerBlockManagerAdded","Block Manager ID":{"Executor ID":"0","Host":"192.168.1.11","Port":33800},"Maximum Memory":535953408,"Timestamp":1467139716370}
{"Event":"SparkListenerTaskEnd","Stage ID":3,"Stage Attempt ID":2,"Task Type":"ResultTask","Task End Reason":{"Reason":"FetchFailed","Block Manager Address":{"Executor ID":"0","Host":"192.168.1.11","Port":33800},"Shuffle ID":0,"Map ID":2,"Reduce ID":3,"Message":"org.apache.spark.shuffle.FetchFailedException: Failed to connect to /192.168.1.11:33800\n\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.throwFetchFailedException(ShuffleBlockFetcherIterator.scala:323)\n\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:300)\n\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:51)\n\tat scala.collection.Iterator$$anon$11.next(Iterator.scala:328)\n\tat scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)\n\tat org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:32)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:39)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:727)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1157)\n\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:452)\n\tat org.apache.spark.api.python.PythonRunner$WriterThread$$anonfun$run$3.apply(PythonRDD.scala:280)\n\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1765)\n\tat org.apache.spark.api.python.PythonRunner$WriterThread.run(PythonRDD.scala:239)\nCaused by: java.io.IOException: Failed to connect to /192.168.1.11:33800\n\tat org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:216)\n\tat org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:167)\n\tat org.apache.spark.network.netty.NettyBlockTransferService$$anon$1.createAndStart(NettyBlockTransferService.scala:90)\n\tat org.apache.spark.network.shuffle.RetryingBlockFetcher.fetchAllOutstanding(RetryingBlockFetcher.java:140)\n\tat org.apache.spark.network.shuffle.RetryingBlockFetcher.access$200(RetryingBlockFetcher.java:43)\n\tat org.apache.spark.network.shuffle.RetryingBlockFetcher$1.run(RetryingBlockFetcher.java:170)\n\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n\tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n\tat java.lang.Thread.run(Thread.java:745)\nCaused by: java.net.NoRouteToHostException: No route to host: /192.168.1.11:33800\n\tat sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)\n\tat sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)\n\tat io.netty.channel.socket.nio.NioSocketChannel.doFinishConnect(NioSocketChannel.java:224)\n\tat io.netty.channel.nio.AbstractNioChannel$AbstractNioUnsafe.finishConnect(AbstractNioChannel.java:289)\n\tat io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:528)\n\tat io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:468)\n\tat io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:382)\n\tat io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:354)\n\tat io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:111)\n\t... 1 more\n"},"Task Info":{"Task ID":31,"Index":3,"Attempt":0,"Launch Time":1467139674049,"Executor ID":"1","Host":"192.168.1.3","Locality":"ANY","Speculative":false,"Getting Result Time":0,"Finish Time":1467139723401,"Failed":true,"Accumulables":[]}}
{"Event":"SparkListenerTaskEnd","Stage ID":3,"Stage Attempt ID":2,"Task Type":"ResultTask","Task End Reason":{"Reason":"FetchFailed","Block Manager Address":{"Executor ID":"1","Host":"192.168.1.11","Port":35409},"Shuffle ID":0,"Map ID":3,"Reduce ID":2,"Message":"org.apache.spark.shuffle.FetchFailedException: Failed to connect to /192.168.1.11:35409\n\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.throwFetchFailedException(ShuffleBlockFetcherIterator.scala:323)\n\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:300)\n\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:51)\n\tat scala.collection.Iterator$$anon$11.next(Iterator.scala:328)\n\tat scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)\n\tat org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:32)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:39)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:727)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1157)\n\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:452)\n\tat org.apache.spark.api.python.PythonRunner$WriterThread$$anonfun$run$3.apply(PythonRDD.scala:280)\n\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1765)\n\tat org.apache.spark.api.python.PythonRunner$WriterThread.run(PythonRDD.scala:239)\nCaused by: java.io.IOException: Failed to connect to /192.168.1.11:35409\n\tat org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:216)\n\tat org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:167)\n\tat org.apache.spark.network.netty.NettyBlockTransferService$$anon$1.createAndStart(NettyBlockTransferService.scala:90)\n\tat org.apache.spark.network.shuffle.RetryingBlockFetcher.fetchAllOutstanding(RetryingBlockFetcher.java:140)\n\tat org.apache.spark.network.shuffle.RetryingBlockFetcher.access$200(RetryingBlockFetcher.java:43)\n\tat org.apache.spark.network.shuffle.RetryingBlockFetcher$1.run(RetryingBlockFetcher.java:170)\n\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n\tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n\tat java.lang.Thread.run(Thread.java:745)\nCaused by: java.net.NoRouteToHostException: No route to host: /192.168.1.11:35409\n\tat sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)\n\tat sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)\n\tat io.netty.channel.socket.nio.NioSocketChannel.doFinishConnect(NioSocketChannel.java:224)\n\tat io.netty.channel.nio.AbstractNioChannel$AbstractNioUnsafe.finishConnect(AbstractNioChannel.java:289)\n\tat io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:528)\n\tat io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:468)\n\tat io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:382)\n\tat io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:354)\n\tat io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:111)\n\t... 1 more\n"},"Task Info":{"Task ID":30,"Index":2,"Attempt":0,"Launch Time":1467139674049,"Executor ID":"0","Host":"192.168.1.3","Locality":"ANY","Speculative":false,"Getting Result Time":0,"Finish Time":1467139726130,"Failed":true,"Accumulables":[]}}
{"Event":"SparkListenerTaskEnd","Stage ID":2,"Stage Attempt ID":5,"Task Type":"ShuffleMapTask","Task End Reason":{"Reason":"Success"},"Task Info":{"Task ID":33,"Index":1,"Attempt":0,"Launch Time":1467139711896,"Executor ID":"1","Host":"192.168.1.3","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":1467139731801,"Failed":false,"Accumulables":[]},"Task Metrics":{"Host Name":"192.168.1.11","Executor Deserialize Time":16,"Executor Run Time":19884,"Result Size":2321,"JVM GC Time":210,"Result Serialization Time":0,"Memory Bytes Spilled":0,"Disk Bytes Spilled":0,"Shuffle Write Metrics":{"Shuffle Bytes Written":53536423,"Shuffle Write Time":197095028,"Shuffle Records Written":148},"Input Metrics":{"Data Read Method":"Hadoop","Bytes Read":65536,"Records Read":3484461}}}
{"Event":"SparkListenerTaskEnd","Stage ID":2,"Stage Attempt ID":5,"Task Type":"ShuffleMapTask","Task End Reason":{"Reason":"Success"},"Task Info":{"Task ID":32,"Index":0,"Attempt":0,"Launch Time":1467139711896,"Executor ID":"0","Host":"192.168.1.3","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":1467139737525,"Failed":false,"Accumulables":[]},"Task Metrics":{"Host Name":"192.168.1.11","Executor Deserialize Time":23,"Executor Run Time":25601,"Result Size":2321,"JVM GC Time":185,"Result Serialization Time":0,"Memory Bytes Spilled":0,"Disk Bytes Spilled":0,"Shuffle Write Metrics":{"Shuffle Bytes Written":65350616,"Shuffle Write Time":162078948,"Shuffle Records Written":164},"Input Metrics":{"Data Read Method":"Hadoop","Bytes Read":65536,"Records Read":4253268}}}
{"Event":"SparkListenerStageCompleted","Stage Info":{"Stage ID":2,"Stage Attempt ID":5,"Stage Name":"sortByKey at /home/daniar/documents/SPARK/spark-1.6.1/sort.py:12","Number of Tasks":2,"RDD Info":[{"RDD ID":5,"Name":"PairwiseRDD","Callsite":"sortByKey at /home/daniar/documents/SPARK/spark-1.6.1/sort.py:12","Parent IDs":[4],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":4,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":0,"Name":"../generated_file/list_int","Scope":"{\"id\":\"0\",\"name\":\"textFile\"}","Callsite":"textFile at NativeMethodAccessorImpl.java:-2","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":4,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":4,"Name":"PythonRDD","Callsite":"sortByKey at /home/daniar/documents/SPARK/spark-1.6.1/sort.py:12","Parent IDs":[1],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":4,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":1,"Name":"../generated_file/list_int","Scope":"{\"id\":\"0\",\"name\":\"textFile\"}","Callsite":"textFile at NativeMethodAccessorImpl.java:-2","Parent IDs":[0],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":4,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.<init>(RDD.scala:98)\norg.apache.spark.api.python.PairwiseRDD.<init>(PythonRDD.scala:338)\nsun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\nsun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\nsun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\njava.lang.reflect.Constructor.newInstance(Constructor.java:423)\npy4j.reflection.MethodInvoker.invoke(MethodInvoker.java:234)\npy4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:381)\npy4j.Gateway.invoke(Gateway.java:214)\npy4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:79)\npy4j.commands.ConstructorCommand.execute(ConstructorCommand.java:68)\npy4j.GatewayConnection.run(GatewayConnection.java:209)\njava.lang.Thread.run(Thread.java:745)","Submission Time":1467139711897,"Completion Time":1467139737526,"Accumulables":[]}}
{"Event":"SparkListenerStageSubmitted","Stage Info":{"Stage ID":2,"Stage Attempt ID":6,"Stage Name":"sortByKey at /home/daniar/documents/SPARK/spark-1.6.1/sort.py:12","Number of Tasks":2,"RDD Info":[{"RDD ID":5,"Name":"PairwiseRDD","Callsite":"sortByKey at /home/daniar/documents/SPARK/spark-1.6.1/sort.py:12","Parent IDs":[4],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":4,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":0,"Name":"../generated_file/list_int","Scope":"{\"id\":\"0\",\"name\":\"textFile\"}","Callsite":"textFile at NativeMethodAccessorImpl.java:-2","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":4,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":4,"Name":"PythonRDD","Callsite":"sortByKey at /home/daniar/documents/SPARK/spark-1.6.1/sort.py:12","Parent IDs":[1],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":4,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":1,"Name":"../generated_file/list_int","Scope":"{\"id\":\"0\",\"name\":\"textFile\"}","Callsite":"textFile at NativeMethodAccessorImpl.java:-2","Parent IDs":[0],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":4,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.<init>(RDD.scala:98)\norg.apache.spark.api.python.PairwiseRDD.<init>(PythonRDD.scala:338)\nsun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\nsun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\nsun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\njava.lang.reflect.Constructor.newInstance(Constructor.java:423)\npy4j.reflection.MethodInvoker.invoke(MethodInvoker.java:234)\npy4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:381)\npy4j.Gateway.invoke(Gateway.java:214)\npy4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:79)\npy4j.commands.ConstructorCommand.execute(ConstructorCommand.java:68)\npy4j.GatewayConnection.run(GatewayConnection.java:209)\njava.lang.Thread.run(Thread.java:745)","Accumulables":[]},"Properties":{"spark.rdd.scope.noOverride":"true","spark.rdd.scope":"{\"id\":\"11\",\"name\":\"saveAsTextFile\"}"}}
{"Event":"SparkListenerTaskStart","Stage ID":2,"Stage Attempt ID":6,"Task Info":{"Task ID":34,"Index":0,"Attempt":0,"Launch Time":1467139737533,"Executor ID":"0","Host":"192.168.1.3","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":0,"Failed":false,"Accumulables":[]}}
{"Event":"SparkListenerTaskStart","Stage ID":2,"Stage Attempt ID":6,"Task Info":{"Task ID":35,"Index":1,"Attempt":0,"Launch Time":1467139737534,"Executor ID":"1","Host":"192.168.1.3","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":0,"Failed":false,"Accumulables":[]}}
{"Event":"SparkListenerTaskEnd","Stage ID":2,"Stage Attempt ID":6,"Task Type":"ShuffleMapTask","Task End Reason":{"Reason":"Success"},"Task Info":{"Task ID":34,"Index":0,"Attempt":0,"Launch Time":1467139737533,"Executor ID":"0","Host":"192.168.1.3","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":1467139766467,"Failed":false,"Accumulables":[]},"Task Metrics":{"Host Name":"192.168.1.11","Executor Deserialize Time":31,"Executor Run Time":28897,"Result Size":2321,"JVM GC Time":108,"Result Serialization Time":0,"Memory Bytes Spilled":0,"Disk Bytes Spilled":0,"Shuffle Write Metrics":{"Shuffle Bytes Written":65356545,"Shuffle Write Time":168651739,"Shuffle Records Written":164},"Input Metrics":{"Data Read Method":"Hadoop","Bytes Read":0,"Records Read":4253453}}}
{"Event":"SparkListenerTaskEnd","Stage ID":2,"Stage Attempt ID":6,"Task Type":"ShuffleMapTask","Task End Reason":{"Reason":"Success"},"Task Info":{"Task ID":35,"Index":1,"Attempt":0,"Launch Time":1467139737534,"Executor ID":"1","Host":"192.168.1.3","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":1467139767131,"Failed":false,"Accumulables":[]},"Task Metrics":{"Host Name":"192.168.1.11","Executor Deserialize Time":29,"Executor Run Time":29561,"Result Size":2321,"JVM GC Time":110,"Result Serialization Time":0,"Memory Bytes Spilled":0,"Disk Bytes Spilled":0,"Shuffle Write Metrics":{"Shuffle Bytes Written":65352156,"Shuffle Write Time":174707192,"Shuffle Records Written":164},"Input Metrics":{"Data Read Method":"Hadoop","Bytes Read":65536,"Records Read":4253236}}}
{"Event":"SparkListenerStageCompleted","Stage Info":{"Stage ID":2,"Stage Attempt ID":6,"Stage Name":"sortByKey at /home/daniar/documents/SPARK/spark-1.6.1/sort.py:12","Number of Tasks":2,"RDD Info":[{"RDD ID":5,"Name":"PairwiseRDD","Callsite":"sortByKey at /home/daniar/documents/SPARK/spark-1.6.1/sort.py:12","Parent IDs":[4],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":4,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":0,"Name":"../generated_file/list_int","Scope":"{\"id\":\"0\",\"name\":\"textFile\"}","Callsite":"textFile at NativeMethodAccessorImpl.java:-2","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":4,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":4,"Name":"PythonRDD","Callsite":"sortByKey at /home/daniar/documents/SPARK/spark-1.6.1/sort.py:12","Parent IDs":[1],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":4,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":1,"Name":"../generated_file/list_int","Scope":"{\"id\":\"0\",\"name\":\"textFile\"}","Callsite":"textFile at NativeMethodAccessorImpl.java:-2","Parent IDs":[0],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":4,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.<init>(RDD.scala:98)\norg.apache.spark.api.python.PairwiseRDD.<init>(PythonRDD.scala:338)\nsun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\nsun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\nsun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\njava.lang.reflect.Constructor.newInstance(Constructor.java:423)\npy4j.reflection.MethodInvoker.invoke(MethodInvoker.java:234)\npy4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:381)\npy4j.Gateway.invoke(Gateway.java:214)\npy4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:79)\npy4j.commands.ConstructorCommand.execute(ConstructorCommand.java:68)\npy4j.GatewayConnection.run(GatewayConnection.java:209)\njava.lang.Thread.run(Thread.java:745)","Submission Time":1467139737533,"Completion Time":1467139767132,"Accumulables":[]}}
{"Event":"SparkListenerStageSubmitted","Stage Info":{"Stage ID":3,"Stage Attempt ID":3,"Stage Name":"saveAsTextFile at NativeMethodAccessorImpl.java:-2","Number of Tasks":4,"RDD Info":[{"RDD ID":10,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"11\",\"name\":\"saveAsTextFile\"}","Callsite":"saveAsTextFile at NativeMethodAccessorImpl.java:-2","Parent IDs":[9],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":4,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":8,"Name":"PythonRDD","Callsite":"RDD at PythonRDD.scala:43","Parent IDs":[7],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":4,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":6,"Name":"ShuffledRDD","Scope":"{\"id\":\"8\",\"name\":\"partitionBy\"}","Callsite":"partitionBy at NativeMethodAccessorImpl.java:-2","Parent IDs":[5],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":4,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":9,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"10\",\"name\":\"map\"}","Callsite":"map at NativeMethodAccessorImpl.java:-2","Parent IDs":[8],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":4,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":7,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"9\",\"name\":\"mapPartitions\"}","Callsite":"mapPartitions at PythonRDD.scala:374","Parent IDs":[6],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":4,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[2],"Details":"org.apache.spark.api.java.AbstractJavaRDDLike.saveAsTextFile(JavaRDDLike.scala:46)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:498)\npy4j.reflection.MethodInvoker.invoke(MethodInvoker.java:231)\npy4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:381)\npy4j.Gateway.invoke(Gateway.java:259)\npy4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:133)\npy4j.commands.CallCommand.execute(CallCommand.java:79)\npy4j.GatewayConnection.run(GatewayConnection.java:209)\njava.lang.Thread.run(Thread.java:745)","Accumulables":[]},"Properties":{"spark.rdd.scope.noOverride":"true","spark.rdd.scope":"{\"id\":\"11\",\"name\":\"saveAsTextFile\"}"}}
{"Event":"SparkListenerTaskStart","Stage ID":3,"Stage Attempt ID":3,"Task Info":{"Task ID":36,"Index":0,"Attempt":0,"Launch Time":1467139767158,"Executor ID":"0","Host":"192.168.1.3","Locality":"ANY","Speculative":false,"Getting Result Time":0,"Finish Time":0,"Failed":false,"Accumulables":[]}}
{"Event":"SparkListenerTaskStart","Stage ID":3,"Stage Attempt ID":3,"Task Info":{"Task ID":37,"Index":1,"Attempt":0,"Launch Time":1467139767158,"Executor ID":"1","Host":"192.168.1.3","Locality":"ANY","Speculative":false,"Getting Result Time":0,"Finish Time":0,"Failed":false,"Accumulables":[]}}
{"Event":"SparkListenerTaskStart","Stage ID":3,"Stage Attempt ID":3,"Task Info":{"Task ID":38,"Index":2,"Attempt":0,"Launch Time":1467139767158,"Executor ID":"0","Host":"192.168.1.3","Locality":"ANY","Speculative":false,"Getting Result Time":0,"Finish Time":0,"Failed":false,"Accumulables":[]}}
{"Event":"SparkListenerTaskStart","Stage ID":3,"Stage Attempt ID":3,"Task Info":{"Task ID":39,"Index":3,"Attempt":0,"Launch Time":1467139767158,"Executor ID":"1","Host":"192.168.1.3","Locality":"ANY","Speculative":false,"Getting Result Time":0,"Finish Time":0,"Failed":false,"Accumulables":[]}}
{"Event":"SparkListenerTaskEnd","Stage ID":3,"Stage Attempt ID":3,"Task Type":"ResultTask","Task End Reason":{"Reason":"FetchFailed","Block Manager Address":{"Executor ID":"0","Host":"192.168.1.11","Port":33800},"Shuffle ID":0,"Map ID":1,"Reduce ID":3,"Message":"org.apache.spark.shuffle.FetchFailedException: Failed to connect to /192.168.1.11:33800\n\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.throwFetchFailedException(ShuffleBlockFetcherIterator.scala:323)\n\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:300)\n\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:51)\n\tat scala.collection.Iterator$$anon$11.next(Iterator.scala:328)\n\tat scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)\n\tat org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:32)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:39)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:727)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1157)\n\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:452)\n\tat org.apache.spark.api.python.PythonRunner$WriterThread$$anonfun$run$3.apply(PythonRDD.scala:280)\n\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1765)\n\tat org.apache.spark.api.python.PythonRunner$WriterThread.run(PythonRDD.scala:239)\nCaused by: java.io.IOException: Failed to connect to /192.168.1.11:33800\n\tat org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:216)\n\tat org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:167)\n\tat org.apache.spark.network.netty.NettyBlockTransferService$$anon$1.createAndStart(NettyBlockTransferService.scala:90)\n\tat org.apache.spark.network.shuffle.RetryingBlockFetcher.fetchAllOutstanding(RetryingBlockFetcher.java:140)\n\tat org.apache.spark.network.shuffle.RetryingBlockFetcher.access$200(RetryingBlockFetcher.java:43)\n\tat org.apache.spark.network.shuffle.RetryingBlockFetcher$1.run(RetryingBlockFetcher.java:170)\n\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n\tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n\tat java.lang.Thread.run(Thread.java:745)\nCaused by: java.net.NoRouteToHostException: No route to host: /192.168.1.11:33800\n\tat sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)\n\tat sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)\n\tat io.netty.channel.socket.nio.NioSocketChannel.doFinishConnect(NioSocketChannel.java:224)\n\tat io.netty.channel.nio.AbstractNioChannel$AbstractNioUnsafe.finishConnect(AbstractNioChannel.java:289)\n\tat io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:528)\n\tat io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:468)\n\tat io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:382)\n\tat io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:354)\n\tat io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:111)\n\t... 1 more\n"},"Task Info":{"Task ID":39,"Index":3,"Attempt":0,"Launch Time":1467139767158,"Executor ID":"1","Host":"192.168.1.3","Locality":"ANY","Speculative":false,"Getting Result Time":0,"Finish Time":1467139805942,"Failed":true,"Accumulables":[]}}
{"Event":"SparkListenerStageCompleted","Stage Info":{"Stage ID":3,"Stage Attempt ID":3,"Stage Name":"saveAsTextFile at NativeMethodAccessorImpl.java:-2","Number of Tasks":4,"RDD Info":[{"RDD ID":10,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"11\",\"name\":\"saveAsTextFile\"}","Callsite":"saveAsTextFile at NativeMethodAccessorImpl.java:-2","Parent IDs":[9],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":4,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":8,"Name":"PythonRDD","Callsite":"RDD at PythonRDD.scala:43","Parent IDs":[7],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":4,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":6,"Name":"ShuffledRDD","Scope":"{\"id\":\"8\",\"name\":\"partitionBy\"}","Callsite":"partitionBy at NativeMethodAccessorImpl.java:-2","Parent IDs":[5],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":4,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":9,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"10\",\"name\":\"map\"}","Callsite":"map at NativeMethodAccessorImpl.java:-2","Parent IDs":[8],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":4,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":7,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"9\",\"name\":\"mapPartitions\"}","Callsite":"mapPartitions at PythonRDD.scala:374","Parent IDs":[6],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":4,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[2],"Details":"org.apache.spark.api.java.AbstractJavaRDDLike.saveAsTextFile(JavaRDDLike.scala:46)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:498)\npy4j.reflection.MethodInvoker.invoke(MethodInvoker.java:231)\npy4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:381)\npy4j.Gateway.invoke(Gateway.java:259)\npy4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:133)\npy4j.commands.CallCommand.execute(CallCommand.java:79)\npy4j.GatewayConnection.run(GatewayConnection.java:209)\njava.lang.Thread.run(Thread.java:745)","Submission Time":1467139767157,"Completion Time":1467139805942,"Failure Reason":"org.apache.spark.shuffle.FetchFailedException: Failed to connect to /192.168.1.11:33800\n\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.throwFetchFailedException(ShuffleBlockFetcherIterator.scala:323)\n\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:300)\n\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:51)\n\tat scala.collection.Iterator$$anon$11.next(Iterator.scala:328)\n\tat scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)\n\tat org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:32)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:39)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:727)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1157)\n\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:452)\n\tat org.apache.spark.api.python.PythonRunner$WriterThread$$anonfun$run$3.apply(PythonRDD.scala:280)\n\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1765)\n\tat org.apache.spark.api.python.PythonRunner$WriterThread.run(PythonRDD.scala:239)\nCaused by: java.io.IOException: Failed to connect to /192.168.1.11:33800\n\tat org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:216)\n\tat org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:167)\n\tat org.apache.spark.network.netty.NettyBlockTransferService$$anon$1.createAndStart(NettyBlockTransferService.scala:90)\n\tat org.apache.spark.network.shuffle.RetryingBlockFetcher.fetchAllOutstanding(RetryingBlockFetcher.java:140)\n\tat org.apache.spark.network.shuffle.RetryingBlockFetcher.access$200(RetryingBlockFetcher.java:43)\n\tat org.apache.spark.network.shuffle.RetryingBlockFetcher$1.run(RetryingBlockFetcher.java:170)\n\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n\tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n\tat java.lang.Thread.run(Thread.java:745)\nCaused by: java.net.NoRouteToHostException: No route to host: /192.168.1.11:33800\n\tat sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)\n\tat sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)\n\tat io.netty.channel.socket.nio.NioSocketChannel.doFinishConnect(NioSocketChannel.java:224)\n\tat io.netty.channel.nio.AbstractNioChannel$AbstractNioUnsafe.finishConnect(AbstractNioChannel.java:289)\n\tat io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:528)\n\tat io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:468)\n\tat io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:382)\n\tat io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:354)\n\tat io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:111)\n\t... 1 more\n","Accumulables":[]}}
{"Event":"SparkListenerJobEnd","Job ID":2,"Completion Time":1467139805993,"Job Result":{"Result":"JobFailed","Exception":{"Message":"Job aborted due to stage failure: ResultStage 3 (saveAsTextFile at NativeMethodAccessorImpl.java:-2) has failed the maximum allowable number of times: 4. Most recent failure reason: org.apache.spark.shuffle.FetchFailedException: Failed to connect to /192.168.1.11:33800\n\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.throwFetchFailedException(ShuffleBlockFetcherIterator.scala:323)\n\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:300)\n\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:51)\n\tat scala.collection.Iterator$$anon$11.next(Iterator.scala:328)\n\tat scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)\n\tat org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:32)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:39)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:727)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1157)\n\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:452)\n\tat org.apache.spark.api.python.PythonRunner$WriterThread$$anonfun$run$3.apply(PythonRDD.scala:280)\n\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1765)\n\tat org.apache.spark.api.python.PythonRunner$WriterThread.run(PythonRDD.scala:239)\nCaused by: java.io.IOException: Failed to connect to /192.168.1.11:33800\n\tat org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:216)\n\tat org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:167)\n\tat org.apache.spark.network.netty.NettyBlockTransferService$$anon$1.createAndStart(NettyBlockTransferService.scala:90)\n\tat org.apache.spark.network.shuffle.RetryingBlockFetcher.fetchAllOutstanding(RetryingBlockFetcher.java:140)\n\tat org.apache.spark.network.shuffle.RetryingBlockFetcher.access$200(RetryingBlockFetcher.java:43)\n\tat org.apache.spark.network.shuffle.RetryingBlockFetcher$1.run(RetryingBlockFetcher.java:170)\n\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n\tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n\tat java.lang.Thread.run(Thread.java:745)\nCaused by: java.net.NoRouteToHostException: No route to host: /192.168.1.11:33800\n\tat sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)\n\tat sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)\n\tat io.netty.channel.socket.nio.NioSocketChannel.doFinishConnect(NioSocketChannel.java:224)\n\tat io.netty.channel.nio.AbstractNioChannel$AbstractNioUnsafe.finishConnect(AbstractNioChannel.java:289)\n\tat io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:528)\n\tat io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:468)\n\tat io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:382)\n\tat io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:354)\n\tat io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:111)\n\t... 1 more\n","Stack Trace":[{"Declaring Class":"org.apache.spark.scheduler.DAGScheduler","Method Name":"org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages","File Name":"DAGScheduler.scala","Line Number":1431},{"Declaring Class":"org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1","Method Name":"apply","File Name":"DAGScheduler.scala","Line Number":1419},{"Declaring Class":"org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1","Method Name":"apply","File Name":"DAGScheduler.scala","Line Number":1418},{"Declaring Class":"scala.collection.mutable.ResizableArray$class","Method Name":"foreach","File Name":"ResizableArray.scala","Line Number":59},{"Declaring Class":"scala.collection.mutable.ArrayBuffer","Method Name":"foreach","File Name":"ArrayBuffer.scala","Line Number":47},{"Declaring Class":"org.apache.spark.scheduler.DAGScheduler","Method Name":"abortStage","File Name":"DAGScheduler.scala","Line Number":1418},{"Declaring Class":"org.apache.spark.scheduler.DAGScheduler","Method Name":"handleTaskCompletion","File Name":"DAGScheduler.scala","Line Number":1258},{"Declaring Class":"org.apache.spark.scheduler.DAGSchedulerEventProcessLoop","Method Name":"doOnReceive","File Name":"DAGScheduler.scala","Line Number":1637},{"Declaring Class":"org.apache.spark.scheduler.DAGSchedulerEventProcessLoop","Method Name":"onReceive","File Name":"DAGScheduler.scala","Line Number":1599},{"Declaring Class":"org.apache.spark.scheduler.DAGSchedulerEventProcessLoop","Method Name":"onReceive","File Name":"DAGScheduler.scala","Line Number":1588},{"Declaring Class":"org.apache.spark.util.EventLoop$$anon$1","Method Name":"run","File Name":"EventLoop.scala","Line Number":48},{"Declaring Class":"org.apache.spark.scheduler.DAGScheduler","Method Name":"runJob","File Name":"DAGScheduler.scala","Line Number":620},{"Declaring Class":"org.apache.spark.SparkContext","Method Name":"runJob","File Name":"SparkContext.scala","Line Number":1832},{"Declaring Class":"org.apache.spark.SparkContext","Method Name":"runJob","File Name":"SparkContext.scala","Line Number":1845},{"Declaring Class":"org.apache.spark.SparkContext","Method Name":"runJob","File Name":"SparkContext.scala","Line Number":1922},{"Declaring Class":"org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1","Method Name":"apply$mcV$sp","File Name":"PairRDDFunctions.scala","Line Number":1213},{"Declaring Class":"org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1","Method Name":"apply","File Name":"PairRDDFunctions.scala","Line Number":1156},{"Declaring Class":"org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1","Method Name":"apply","File Name":"PairRDDFunctions.scala","Line Number":1156},{"Declaring Class":"org.apache.spark.rdd.RDDOperationScope$","Method Name":"withScope","File Name":"RDDOperationScope.scala","Line Number":150},{"Declaring Class":"org.apache.spark.rdd.RDDOperationScope$","Method Name":"withScope","File Name":"RDDOperationScope.scala","Line Number":111},{"Declaring Class":"org.apache.spark.rdd.RDD","Method Name":"withScope","File Name":"RDD.scala","Line Number":316},{"Declaring Class":"org.apache.spark.rdd.PairRDDFunctions","Method Name":"saveAsHadoopDataset","File Name":"PairRDDFunctions.scala","Line Number":1156},{"Declaring Class":"org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$4","Method Name":"apply$mcV$sp","File Name":"PairRDDFunctions.scala","Line Number":1060},{"Declaring Class":"org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$4","Method Name":"apply","File Name":"PairRDDFunctions.scala","Line Number":1026},{"Declaring Class":"org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$4","Method Name":"apply","File Name":"PairRDDFunctions.scala","Line Number":1026},{"Declaring Class":"org.apache.spark.rdd.RDDOperationScope$","Method Name":"withScope","File Name":"RDDOperationScope.scala","Line Number":150},{"Declaring Class":"org.apache.spark.rdd.RDDOperationScope$","Method Name":"withScope","File Name":"RDDOperationScope.scala","Line Number":111},{"Declaring Class":"org.apache.spark.rdd.RDD","Method Name":"withScope","File Name":"RDD.scala","Line Number":316},{"Declaring Class":"org.apache.spark.rdd.PairRDDFunctions","Method Name":"saveAsHadoopFile","File Name":"PairRDDFunctions.scala","Line Number":1026},{"Declaring Class":"org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$1","Method Name":"apply$mcV$sp","File Name":"PairRDDFunctions.scala","Line Number":952},{"Declaring Class":"org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$1","Method Name":"apply","File Name":"PairRDDFunctions.scala","Line Number":952},{"Declaring Class":"org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$1","Method Name":"apply","File Name":"PairRDDFunctions.scala","Line Number":952},{"Declaring Class":"org.apache.spark.rdd.RDDOperationScope$","Method Name":"withScope","File Name":"RDDOperationScope.scala","Line Number":150},{"Declaring Class":"org.apache.spark.rdd.RDDOperationScope$","Method Name":"withScope","File Name":"RDDOperationScope.scala","Line Number":111},{"Declaring Class":"org.apache.spark.rdd.RDD","Method Name":"withScope","File Name":"RDD.scala","Line Number":316},{"Declaring Class":"org.apache.spark.rdd.PairRDDFunctions","Method Name":"saveAsHadoopFile","File Name":"PairRDDFunctions.scala","Line Number":951},{"Declaring Class":"org.apache.spark.rdd.RDD$$anonfun$saveAsTextFile$1","Method Name":"apply$mcV$sp","File Name":"RDD.scala","Line Number":1457},{"Declaring Class":"org.apache.spark.rdd.RDD$$anonfun$saveAsTextFile$1","Method Name":"apply","File Name":"RDD.scala","Line Number":1436},{"Declaring Class":"org.apache.spark.rdd.RDD$$anonfun$saveAsTextFile$1","Method Name":"apply","File Name":"RDD.scala","Line Number":1436},{"Declaring Class":"org.apache.spark.rdd.RDDOperationScope$","Method Name":"withScope","File Name":"RDDOperationScope.scala","Line Number":150},{"Declaring Class":"org.apache.spark.rdd.RDDOperationScope$","Method Name":"withScope","File Name":"RDDOperationScope.scala","Line Number":111},{"Declaring Class":"org.apache.spark.rdd.RDD","Method Name":"withScope","File Name":"RDD.scala","Line Number":316},{"Declaring Class":"org.apache.spark.rdd.RDD","Method Name":"saveAsTextFile","File Name":"RDD.scala","Line Number":1436},{"Declaring Class":"org.apache.spark.api.java.JavaRDDLike$class","Method Name":"saveAsTextFile","File Name":"JavaRDDLike.scala","Line Number":507},{"Declaring Class":"org.apache.spark.api.java.AbstractJavaRDDLike","Method Name":"saveAsTextFile","File Name":"JavaRDDLike.scala","Line Number":46},{"Declaring Class":"sun.reflect.NativeMethodAccessorImpl","Method Name":"invoke0","File Name":"NativeMethodAccessorImpl.java","Line Number":-2},{"Declaring Class":"sun.reflect.NativeMethodAccessorImpl","Method Name":"invoke","File Name":"NativeMethodAccessorImpl.java","Line Number":62},{"Declaring Class":"sun.reflect.DelegatingMethodAccessorImpl","Method Name":"invoke","File Name":"DelegatingMethodAccessorImpl.java","Line Number":43},{"Declaring Class":"java.lang.reflect.Method","Method Name":"invoke","File Name":"Method.java","Line Number":498},{"Declaring Class":"py4j.reflection.MethodInvoker","Method Name":"invoke","File Name":"MethodInvoker.java","Line Number":231},{"Declaring Class":"py4j.reflection.ReflectionEngine","Method Name":"invoke","File Name":"ReflectionEngine.java","Line Number":381},{"Declaring Class":"py4j.Gateway","Method Name":"invoke","File Name":"Gateway.java","Line Number":259},{"Declaring Class":"py4j.commands.AbstractCommand","Method Name":"invokeMethod","File Name":"AbstractCommand.java","Line Number":133},{"Declaring Class":"py4j.commands.CallCommand","Method Name":"execute","File Name":"CallCommand.java","Line Number":79},{"Declaring Class":"py4j.GatewayConnection","Method Name":"run","File Name":"GatewayConnection.java","Line Number":209},{"Declaring Class":"java.lang.Thread","Method Name":"run","File Name":"Thread.java","Line Number":745}]}}}
{"Event":"SparkListenerBlockManagerRemoved","Block Manager ID":{"Executor ID":"0","Host":"192.168.1.11","Port":33800},"Timestamp":1467139805993}
{"Event":"SparkListenerApplicationEnd","Timestamp":1467139806356}
{"Event":"SparkListenerBlockManagerAdded","Block Manager ID":{"Executor ID":"0","Host":"192.168.1.11","Port":33800},"Maximum Memory":535953408,"Timestamp":1467139806369}
