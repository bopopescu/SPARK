{"Event":"SparkListenerLogStart","Spark Version":"1.6.1"}
{"Event":"SparkListenerBlockManagerAdded","Block Manager ID":{"Executor ID":"driver","Host":"192.168.1.3","Port":54389},"Maximum Memory":535953408,"Timestamp":1467148257151}
{"Event":"SparkListenerEnvironmentUpdate","JVM Information":{"Java Home":"/usr/lib/jvm/java-8-oracle/jre","Java Version":"1.8.0_91 (Oracle Corporation)","Scala Version":"version 2.10.5"},"Spark Properties":{"spark.speculation":"true","spark.driver.host":"192.168.1.3","spark.serializer.objectStreamReset":"100","spark.eventLog.enabled":"true","spark.driver.maxResultSize":"1g","spark.driver.port":"60031","spark.rdd.compress":"True","spark.app.name":"Sorting","spark.scheduler.mode":"FIFO","spark.driver.memory":"1g","spark.executor.instances":"2","spark.files":"file:/home/daniar/documents/SPARK/spark-1.6.1/sort.py","spark.executor.id":"driver","spark.submit.deployMode":"client","spark.master":"spark://daniar-X450JF:7077","spark.executor.memory":"1g","spark.eventLog.dir":"../generated_driver_log/","spark.executor.cores":"2","spark.externalBlockStore.folderName":"spark-9f08d0e4-1c39-4ab0-ad45-a84a13339ff2","spark.app.id":"app-20160629041057-0001"},"System Properties":{"java.io.tmpdir":"/tmp","line.separator":"\n","path.separator":":","sun.management.compiler":"HotSpot 64-Bit Tiered Compilers","SPARK_SUBMIT":"true","sun.cpu.endian":"little","java.specification.version":"1.8","java.vm.specification.name":"Java Virtual Machine Specification","java.vendor":"Oracle Corporation","java.vm.specification.version":"1.8","user.home":"/home/daniar","file.encoding.pkg":"sun.io","sun.nio.ch.bugLevel":"","sun.arch.data.model":"64","sun.boot.library.path":"/usr/lib/jvm/java-8-oracle/jre/lib/amd64","user.dir":"/home/daniar/documents/SPARK/spark-1.6.1","java.library.path":"/usr/java/packages/lib/amd64:/usr/lib64:/lib64:/lib:/usr/lib","sun.cpu.isalist":"","sun.desktop":"gnome","os.arch":"amd64","java.vm.version":"25.91-b14","java.endorsed.dirs":"/usr/lib/jvm/java-8-oracle/jre/lib/endorsed","java.runtime.version":"1.8.0_91-b14","java.vm.info":"mixed mode","java.ext.dirs":"/usr/lib/jvm/java-8-oracle/jre/lib/ext:/usr/java/packages/lib/ext","java.runtime.name":"Java(TM) SE Runtime Environment","file.separator":"/","java.class.version":"52.0","java.specification.name":"Java Platform API Specification","sun.boot.class.path":"/usr/lib/jvm/java-8-oracle/jre/lib/resources.jar:/usr/lib/jvm/java-8-oracle/jre/lib/rt.jar:/usr/lib/jvm/java-8-oracle/jre/lib/sunrsasign.jar:/usr/lib/jvm/java-8-oracle/jre/lib/jsse.jar:/usr/lib/jvm/java-8-oracle/jre/lib/jce.jar:/usr/lib/jvm/java-8-oracle/jre/lib/charsets.jar:/usr/lib/jvm/java-8-oracle/jre/lib/jfr.jar:/usr/lib/jvm/java-8-oracle/jre/classes","file.encoding":"UTF-8","user.timezone":"Asia/Jakarta","java.specification.vendor":"Oracle Corporation","sun.java.launcher":"SUN_STANDARD","os.version":"3.19.0-51-generic","sun.os.patch.level":"unknown","java.vm.specification.vendor":"Oracle Corporation","user.country":"US","sun.jnu.encoding":"UTF-8","user.language":"en","java.vendor.url":"http://java.oracle.com/","java.awt.printerjob":"sun.print.PSPrinterJob","java.awt.graphicsenv":"sun.awt.X11GraphicsEnvironment","awt.toolkit":"sun.awt.X11.XToolkit","os.name":"Linux","java.vm.vendor":"Oracle Corporation","java.vendor.url.bug":"http://bugreport.sun.com/bugreport/","user.name":"daniar","java.vm.name":"Java HotSpot(TM) 64-Bit Server VM","sun.java.command":"org.apache.spark.deploy.SparkSubmit sort.py --master spark://daniar-X450JF:7077 --deploy-mode cluster","java.home":"/usr/lib/jvm/java-8-oracle/jre","java.version":"1.8.0_91","sun.io.unicode.encoding":"UnicodeLittle"},"Classpath Entries":{"/home/daniar/documents/SPARK/spark-1.6.1/tools/target/scala-2.10/classes/":"System Classpath","/home/daniar/documents/SPARK/spark-1.6.1/launcher/target/scala-2.10/classes/":"System Classpath","/home/daniar/documents/SPARK/spark-1.6.1/lib_managed/jars/datanucleus-rdbms-3.2.9.jar":"System Classpath","/home/daniar/documents/SPARK/spark-1.6.1/network/shuffle/target/scala-2.10/classes/":"System Classpath","/home/daniar/documents/SPARK/spark-1.6.1/core/target/scala-2.10/classes/":"System Classpath","/home/daniar/documents/SPARK/spark-1.6.1/core/target/jars/*":"System Classpath","/home/daniar/documents/SPARK/spark-1.6.1/streaming/target/scala-2.10/classes/":"System Classpath","/home/daniar/documents/SPARK/spark-1.6.1/assembly/target/scala-2.10/spark-assembly-1.6.1-hadoop2.7.2.jar":"System Classpath","/home/daniar/documents/SPARK/spark-1.6.1/graphx/target/scala-2.10/classes/":"System Classpath","/home/daniar/documents/SPARK/spark-1.6.1/bagel/target/scala-2.10/classes/":"System Classpath","/home/daniar/documents/SPARK/spark-1.6.1/mllib/target/scala-2.10/classes/":"System Classpath","/home/daniar/documents/SPARK/spark-1.6.1/sql/hive-thriftserver/target/scala-2.10/classes":"System Classpath","/home/daniar/documents/SPARK/spark-1.6.1/lib_managed/jars/datanucleus-core-3.2.10.jar":"System Classpath","/home/daniar/documents/SPARK/spark-1.6.1/yarn/target/scala-2.10/classes/":"System Classpath","/home/daniar/documents/SPARK/spark-1.6.1/network/common/target/scala-2.10/classes/":"System Classpath","/home/daniar/documents/SPARK/spark-1.6.1/network/yarn/target/scala-2.10/classes/":"System Classpath","/home/daniar/documents/SPARK/spark-1.6.1/repl/target/scala-2.10/classes/":"System Classpath","/home/daniar/documents/SPARK/spark-1.6.1/sql/core/target/scala-2.10/classes/":"System Classpath","/home/daniar/documents/SPARK/spark-1.6.1/lib_managed/jars/datanucleus-api-jdo-3.2.6.jar":"System Classpath","http://192.168.1.3:46238/files/sort.py":"Added By User","/home/daniar/documents/SPARK/spark-1.6.1/conf/":"System Classpath","/home/daniar/documents/SPARK/spark-1.6.1/sql/hive/target/scala-2.10/classes/":"System Classpath","/home/daniar/documents/SPARK/spark-1.6.1/sql/catalyst/target/scala-2.10/classes/":"System Classpath"}}
{"Event":"SparkListenerApplicationStart","App Name":"Sorting","App ID":"app-20160629041057-0001","Timestamp":1467148255230,"User":"daniar"}
{"Event":"SparkListenerJobStart","Job ID":0,"Submission Time":1467148258637,"Stage Infos":[{"Stage ID":0,"Stage Attempt ID":0,"Stage Name":"sortByKey at /home/daniar/documents/SPARK/spark-1.6.1/sort.py:12","Number of Tasks":4,"RDD Info":[{"RDD ID":2,"Name":"PythonRDD","Callsite":"sortByKey at /home/daniar/documents/SPARK/spark-1.6.1/sort.py:12","Parent IDs":[1],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":4,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":0,"Name":"../generated_file/list_int","Scope":"{\"id\":\"0\",\"name\":\"textFile\"}","Callsite":"textFile at NativeMethodAccessorImpl.java:-2","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":4,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":1,"Name":"../generated_file/list_int","Scope":"{\"id\":\"0\",\"name\":\"textFile\"}","Callsite":"textFile at NativeMethodAccessorImpl.java:-2","Parent IDs":[0],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":4,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.collect(RDD.scala:926)\norg.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:405)\norg.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:498)\npy4j.reflection.MethodInvoker.invoke(MethodInvoker.java:231)\npy4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:381)\npy4j.Gateway.invoke(Gateway.java:259)\npy4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:133)\npy4j.commands.CallCommand.execute(CallCommand.java:79)\npy4j.GatewayConnection.run(GatewayConnection.java:209)\njava.lang.Thread.run(Thread.java:745)","Accumulables":[]}],"Stage IDs":[0],"Properties":{"spark.rdd.scope.noOverride":"true","spark.rdd.scope":"{\"id\":\"1\",\"name\":\"collect\"}","callSite.short":"sortByKey at /home/daniar/documents/SPARK/spark-1.6.1/sort.py:12"}}
{"Event":"SparkListenerStageSubmitted","Stage Info":{"Stage ID":0,"Stage Attempt ID":0,"Stage Name":"sortByKey at /home/daniar/documents/SPARK/spark-1.6.1/sort.py:12","Number of Tasks":4,"RDD Info":[{"RDD ID":2,"Name":"PythonRDD","Callsite":"sortByKey at /home/daniar/documents/SPARK/spark-1.6.1/sort.py:12","Parent IDs":[1],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":4,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":0,"Name":"../generated_file/list_int","Scope":"{\"id\":\"0\",\"name\":\"textFile\"}","Callsite":"textFile at NativeMethodAccessorImpl.java:-2","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":4,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":1,"Name":"../generated_file/list_int","Scope":"{\"id\":\"0\",\"name\":\"textFile\"}","Callsite":"textFile at NativeMethodAccessorImpl.java:-2","Parent IDs":[0],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":4,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.collect(RDD.scala:926)\norg.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:405)\norg.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:498)\npy4j.reflection.MethodInvoker.invoke(MethodInvoker.java:231)\npy4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:381)\npy4j.Gateway.invoke(Gateway.java:259)\npy4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:133)\npy4j.commands.CallCommand.execute(CallCommand.java:79)\npy4j.GatewayConnection.run(GatewayConnection.java:209)\njava.lang.Thread.run(Thread.java:745)","Submission Time":1467148258731,"Accumulables":[]},"Properties":{"spark.rdd.scope.noOverride":"true","spark.rdd.scope":"{\"id\":\"1\",\"name\":\"collect\"}","callSite.short":"sortByKey at /home/daniar/documents/SPARK/spark-1.6.1/sort.py:12"}}
{"Event":"SparkListenerExecutorAdded","Timestamp":1467148264599,"Executor ID":"0","Executor Info":{"Host":"192.168.1.3","Total Cores":2,"Log Urls":{"stdout":"http://192.168.1.3:8082/logPage/?appId=app-20160629041057-0001&executorId=0&logType=stdout","stderr":"http://192.168.1.3:8082/logPage/?appId=app-20160629041057-0001&executorId=0&logType=stderr"}}}
{"Event":"SparkListenerTaskStart","Stage ID":0,"Stage Attempt ID":0,"Task Info":{"Task ID":0,"Index":0,"Attempt":0,"Launch Time":1467148264616,"Executor ID":"0","Host":"192.168.1.3","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":0,"Failed":false,"Accumulables":[]}}
{"Event":"SparkListenerTaskStart","Stage ID":0,"Stage Attempt ID":0,"Task Info":{"Task ID":1,"Index":1,"Attempt":0,"Launch Time":1467148264660,"Executor ID":"0","Host":"192.168.1.3","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":0,"Failed":false,"Accumulables":[]}}
{"Event":"SparkListenerBlockManagerAdded","Block Manager ID":{"Executor ID":"0","Host":"192.168.1.11","Port":43480},"Maximum Memory":535953408,"Timestamp":1467148264978}
{"Event":"SparkListenerExecutorAdded","Timestamp":1467148265048,"Executor ID":"1","Executor Info":{"Host":"192.168.1.3","Total Cores":2,"Log Urls":{"stdout":"http://192.168.1.3:8082/logPage/?appId=app-20160629041057-0001&executorId=1&logType=stdout","stderr":"http://192.168.1.3:8082/logPage/?appId=app-20160629041057-0001&executorId=1&logType=stderr"}}}
{"Event":"SparkListenerTaskStart","Stage ID":0,"Stage Attempt ID":0,"Task Info":{"Task ID":2,"Index":2,"Attempt":0,"Launch Time":1467148265049,"Executor ID":"1","Host":"192.168.1.3","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":0,"Failed":false,"Accumulables":[]}}
{"Event":"SparkListenerTaskStart","Stage ID":0,"Stage Attempt ID":0,"Task Info":{"Task ID":3,"Index":3,"Attempt":0,"Launch Time":1467148265051,"Executor ID":"1","Host":"192.168.1.3","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":0,"Failed":false,"Accumulables":[]}}
{"Event":"SparkListenerBlockManagerAdded","Block Manager ID":{"Executor ID":"1","Host":"192.168.1.11","Port":50530},"Maximum Memory":535953408,"Timestamp":1467148265325}
{"Event":"SparkListenerExecutorAdded","Timestamp":1467148265410,"Executor ID":"5","Executor Info":{"Host":"192.168.1.3","Total Cores":2,"Log Urls":{"stdout":"http://192.168.1.3:8081/logPage/?appId=app-20160629041057-0001&executorId=5&logType=stdout","stderr":"http://192.168.1.3:8081/logPage/?appId=app-20160629041057-0001&executorId=5&logType=stderr"}}}
{"Event":"SparkListenerBlockManagerAdded","Block Manager ID":{"Executor ID":"5","Host":"192.168.1.11","Port":36316},"Maximum Memory":535953408,"Timestamp":1467148265633}
{"Event":"SparkListenerExecutorAdded","Timestamp":1467148266712,"Executor ID":"6","Executor Info":{"Host":"192.168.1.3","Total Cores":2,"Log Urls":{"stdout":"http://192.168.1.3:8081/logPage/?appId=app-20160629041057-0001&executorId=6&logType=stdout","stderr":"http://192.168.1.3:8081/logPage/?appId=app-20160629041057-0001&executorId=6&logType=stderr"}}}
{"Event":"SparkListenerBlockManagerAdded","Block Manager ID":{"Executor ID":"6","Host":"192.168.1.11","Port":38720},"Maximum Memory":535953408,"Timestamp":1467148267076}
{"Event":"SparkListenerExecutorAdded","Timestamp":1467148267699,"Executor ID":"4","Executor Info":{"Host":"192.168.1.3","Total Cores":2,"Log Urls":{"stdout":"http://192.168.1.3:8081/logPage/?appId=app-20160629041057-0001&executorId=4&logType=stdout","stderr":"http://192.168.1.3:8081/logPage/?appId=app-20160629041057-0001&executorId=4&logType=stderr"}}}
{"Event":"SparkListenerExecutorAdded","Timestamp":1467148267850,"Executor ID":"3","Executor Info":{"Host":"192.168.1.3","Total Cores":2,"Log Urls":{"stdout":"http://192.168.1.3:8082/logPage/?appId=app-20160629041057-0001&executorId=3&logType=stdout","stderr":"http://192.168.1.3:8082/logPage/?appId=app-20160629041057-0001&executorId=3&logType=stderr"}}}
{"Event":"SparkListenerBlockManagerAdded","Block Manager ID":{"Executor ID":"4","Host":"192.168.1.11","Port":46524},"Maximum Memory":535953408,"Timestamp":1467148268014}
{"Event":"SparkListenerBlockManagerAdded","Block Manager ID":{"Executor ID":"3","Host":"192.168.1.11","Port":46354},"Maximum Memory":535953408,"Timestamp":1467148268201}
{"Event":"SparkListenerExecutorAdded","Timestamp":1467148268278,"Executor ID":"7","Executor Info":{"Host":"192.168.1.3","Total Cores":2,"Log Urls":{"stdout":"http://192.168.1.3:8081/logPage/?appId=app-20160629041057-0001&executorId=7&logType=stdout","stderr":"http://192.168.1.3:8081/logPage/?appId=app-20160629041057-0001&executorId=7&logType=stderr"}}}
{"Event":"SparkListenerExecutorAdded","Timestamp":1467148268514,"Executor ID":"2","Executor Info":{"Host":"192.168.1.3","Total Cores":2,"Log Urls":{"stdout":"http://192.168.1.3:8082/logPage/?appId=app-20160629041057-0001&executorId=2&logType=stdout","stderr":"http://192.168.1.3:8082/logPage/?appId=app-20160629041057-0001&executorId=2&logType=stderr"}}}
{"Event":"SparkListenerBlockManagerAdded","Block Manager ID":{"Executor ID":"7","Host":"192.168.1.11","Port":50553},"Maximum Memory":535953408,"Timestamp":1467148268618}
{"Event":"SparkListenerBlockManagerAdded","Block Manager ID":{"Executor ID":"2","Host":"192.168.1.11","Port":40778},"Maximum Memory":535953408,"Timestamp":1467148268869}
{"Event":"SparkListenerTaskEnd","Stage ID":0,"Stage Attempt ID":0,"Task Type":"ResultTask","Task End Reason":{"Reason":"Success"},"Task Info":{"Task ID":3,"Index":3,"Attempt":0,"Launch Time":1467148265051,"Executor ID":"1","Host":"192.168.1.3","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":1467148282707,"Failed":false,"Accumulables":[]},"Task Metrics":{"Host Name":"192.168.1.11","Executor Deserialize Time":2489,"Executor Run Time":14762,"Result Size":2130,"JVM GC Time":782,"Result Serialization Time":0,"Memory Bytes Spilled":0,"Disk Bytes Spilled":0,"Input Metrics":{"Data Read Method":"Hadoop","Bytes Read":65536,"Records Read":3484461}}}
{"Event":"SparkListenerTaskEnd","Stage ID":0,"Stage Attempt ID":0,"Task Type":"ResultTask","Task End Reason":{"Reason":"Success"},"Task Info":{"Task ID":2,"Index":2,"Attempt":0,"Launch Time":1467148265049,"Executor ID":"1","Host":"192.168.1.3","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":1467148284040,"Failed":false,"Accumulables":[]},"Task Metrics":{"Host Name":"192.168.1.11","Executor Deserialize Time":2478,"Executor Run Time":16124,"Result Size":2130,"JVM GC Time":782,"Result Serialization Time":0,"Memory Bytes Spilled":0,"Disk Bytes Spilled":0,"Input Metrics":{"Data Read Method":"Hadoop","Bytes Read":65536,"Records Read":4253236}}}
{"Event":"SparkListenerTaskEnd","Stage ID":0,"Stage Attempt ID":0,"Task Type":"ResultTask","Task End Reason":{"Reason":"Success"},"Task Info":{"Task ID":1,"Index":1,"Attempt":0,"Launch Time":1467148264660,"Executor ID":"0","Host":"192.168.1.3","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":1467148306801,"Failed":false,"Accumulables":[]},"Task Metrics":{"Host Name":"192.168.1.11","Executor Deserialize Time":2888,"Executor Run Time":38732,"Result Size":2130,"JVM GC Time":111,"Result Serialization Time":0,"Memory Bytes Spilled":0,"Disk Bytes Spilled":0,"Input Metrics":{"Data Read Method":"Hadoop","Bytes Read":65536,"Records Read":4253268}}}
{"Event":"SparkListenerTaskEnd","Stage ID":0,"Stage Attempt ID":0,"Task Type":"ResultTask","Task End Reason":{"Reason":"Success"},"Task Info":{"Task ID":0,"Index":0,"Attempt":0,"Launch Time":1467148264616,"Executor ID":"0","Host":"192.168.1.3","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":1467148308783,"Failed":false,"Accumulables":[]},"Task Metrics":{"Host Name":"192.168.1.11","Executor Deserialize Time":2903,"Executor Run Time":40716,"Result Size":2130,"JVM GC Time":111,"Result Serialization Time":0,"Memory Bytes Spilled":0,"Disk Bytes Spilled":0,"Input Metrics":{"Data Read Method":"Hadoop","Bytes Read":0,"Records Read":4253453}}}
{"Event":"SparkListenerStageCompleted","Stage Info":{"Stage ID":0,"Stage Attempt ID":0,"Stage Name":"sortByKey at /home/daniar/documents/SPARK/spark-1.6.1/sort.py:12","Number of Tasks":4,"RDD Info":[{"RDD ID":2,"Name":"PythonRDD","Callsite":"sortByKey at /home/daniar/documents/SPARK/spark-1.6.1/sort.py:12","Parent IDs":[1],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":4,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":0,"Name":"../generated_file/list_int","Scope":"{\"id\":\"0\",\"name\":\"textFile\"}","Callsite":"textFile at NativeMethodAccessorImpl.java:-2","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":4,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":1,"Name":"../generated_file/list_int","Scope":"{\"id\":\"0\",\"name\":\"textFile\"}","Callsite":"textFile at NativeMethodAccessorImpl.java:-2","Parent IDs":[0],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":4,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.collect(RDD.scala:926)\norg.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:405)\norg.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:498)\npy4j.reflection.MethodInvoker.invoke(MethodInvoker.java:231)\npy4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:381)\npy4j.Gateway.invoke(Gateway.java:259)\npy4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:133)\npy4j.commands.CallCommand.execute(CallCommand.java:79)\npy4j.GatewayConnection.run(GatewayConnection.java:209)\njava.lang.Thread.run(Thread.java:745)","Submission Time":1467148258731,"Completion Time":1467148308785,"Accumulables":[]}}
{"Event":"SparkListenerJobEnd","Job ID":0,"Completion Time":1467148308789,"Job Result":{"Result":"JobSucceeded"}}
{"Event":"SparkListenerJobStart","Job ID":1,"Submission Time":1467148308819,"Stage Infos":[{"Stage ID":1,"Stage Attempt ID":0,"Stage Name":"sortByKey at /home/daniar/documents/SPARK/spark-1.6.1/sort.py:12","Number of Tasks":4,"RDD Info":[{"RDD ID":3,"Name":"PythonRDD","Callsite":"sortByKey at /home/daniar/documents/SPARK/spark-1.6.1/sort.py:12","Parent IDs":[1],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":4,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":0,"Name":"../generated_file/list_int","Scope":"{\"id\":\"0\",\"name\":\"textFile\"}","Callsite":"textFile at NativeMethodAccessorImpl.java:-2","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":4,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":1,"Name":"../generated_file/list_int","Scope":"{\"id\":\"0\",\"name\":\"textFile\"}","Callsite":"textFile at NativeMethodAccessorImpl.java:-2","Parent IDs":[0],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":4,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.collect(RDD.scala:926)\norg.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:405)\norg.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:498)\npy4j.reflection.MethodInvoker.invoke(MethodInvoker.java:231)\npy4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:381)\npy4j.Gateway.invoke(Gateway.java:259)\npy4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:133)\npy4j.commands.CallCommand.execute(CallCommand.java:79)\npy4j.GatewayConnection.run(GatewayConnection.java:209)\njava.lang.Thread.run(Thread.java:745)","Accumulables":[]}],"Stage IDs":[1],"Properties":{"spark.rdd.scope.noOverride":"true","spark.rdd.scope":"{\"id\":\"5\",\"name\":\"collect\"}","callSite.short":"sortByKey at /home/daniar/documents/SPARK/spark-1.6.1/sort.py:12"}}
{"Event":"SparkListenerStageSubmitted","Stage Info":{"Stage ID":1,"Stage Attempt ID":0,"Stage Name":"sortByKey at /home/daniar/documents/SPARK/spark-1.6.1/sort.py:12","Number of Tasks":4,"RDD Info":[{"RDD ID":3,"Name":"PythonRDD","Callsite":"sortByKey at /home/daniar/documents/SPARK/spark-1.6.1/sort.py:12","Parent IDs":[1],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":4,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":0,"Name":"../generated_file/list_int","Scope":"{\"id\":\"0\",\"name\":\"textFile\"}","Callsite":"textFile at NativeMethodAccessorImpl.java:-2","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":4,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":1,"Name":"../generated_file/list_int","Scope":"{\"id\":\"0\",\"name\":\"textFile\"}","Callsite":"textFile at NativeMethodAccessorImpl.java:-2","Parent IDs":[0],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":4,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.collect(RDD.scala:926)\norg.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:405)\norg.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:498)\npy4j.reflection.MethodInvoker.invoke(MethodInvoker.java:231)\npy4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:381)\npy4j.Gateway.invoke(Gateway.java:259)\npy4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:133)\npy4j.commands.CallCommand.execute(CallCommand.java:79)\npy4j.GatewayConnection.run(GatewayConnection.java:209)\njava.lang.Thread.run(Thread.java:745)","Accumulables":[]},"Properties":{"spark.rdd.scope.noOverride":"true","spark.rdd.scope":"{\"id\":\"5\",\"name\":\"collect\"}","callSite.short":"sortByKey at /home/daniar/documents/SPARK/spark-1.6.1/sort.py:12"}}
{"Event":"SparkListenerTaskStart","Stage ID":1,"Stage Attempt ID":0,"Task Info":{"Task ID":4,"Index":0,"Attempt":0,"Launch Time":1467148308831,"Executor ID":"0","Host":"192.168.1.3","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":0,"Failed":false,"Accumulables":[]}}
{"Event":"SparkListenerTaskStart","Stage ID":1,"Stage Attempt ID":0,"Task Info":{"Task ID":5,"Index":1,"Attempt":0,"Launch Time":1467148308832,"Executor ID":"3","Host":"192.168.1.3","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":0,"Failed":false,"Accumulables":[]}}
{"Event":"SparkListenerTaskStart","Stage ID":1,"Stage Attempt ID":0,"Task Info":{"Task ID":6,"Index":2,"Attempt":0,"Launch Time":1467148308832,"Executor ID":"6","Host":"192.168.1.3","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":0,"Failed":false,"Accumulables":[]}}
{"Event":"SparkListenerTaskStart","Stage ID":1,"Stage Attempt ID":0,"Task Info":{"Task ID":7,"Index":3,"Attempt":0,"Launch Time":1467148308833,"Executor ID":"1","Host":"192.168.1.3","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":0,"Failed":false,"Accumulables":[]}}
{"Event":"SparkListenerTaskEnd","Stage ID":1,"Stage Attempt ID":0,"Task Type":"ResultTask","Task End Reason":{"Reason":"Success"},"Task Info":{"Task ID":7,"Index":3,"Attempt":0,"Launch Time":1467148308833,"Executor ID":"1","Host":"192.168.1.3","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":1467148319757,"Failed":false,"Accumulables":[]},"Task Metrics":{"Host Name":"192.168.1.11","Executor Deserialize Time":38,"Executor Run Time":10874,"Result Size":2292,"JVM GC Time":32,"Result Serialization Time":1,"Memory Bytes Spilled":0,"Disk Bytes Spilled":0,"Input Metrics":{"Data Read Method":"Hadoop","Bytes Read":65536,"Records Read":3484461}}}
{"Event":"SparkListenerTaskEnd","Stage ID":1,"Stage Attempt ID":0,"Task Type":"ResultTask","Task End Reason":{"Reason":"Success"},"Task Info":{"Task ID":4,"Index":0,"Attempt":0,"Launch Time":1467148308831,"Executor ID":"0","Host":"192.168.1.3","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":1467148321991,"Failed":false,"Accumulables":[]},"Task Metrics":{"Host Name":"192.168.1.11","Executor Deserialize Time":59,"Executor Run Time":13093,"Result Size":2287,"JVM GC Time":20,"Result Serialization Time":0,"Memory Bytes Spilled":0,"Disk Bytes Spilled":0,"Input Metrics":{"Data Read Method":"Hadoop","Bytes Read":0,"Records Read":4253453}}}
{"Event":"SparkListenerTaskEnd","Stage ID":1,"Stage Attempt ID":0,"Task Type":"ResultTask","Task End Reason":{"Reason":"Success"},"Task Info":{"Task ID":5,"Index":1,"Attempt":0,"Launch Time":1467148308832,"Executor ID":"3","Host":"192.168.1.3","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":1467148399519,"Failed":false,"Accumulables":[]},"Task Metrics":{"Host Name":"192.168.1.11","Executor Deserialize Time":54835,"Executor Run Time":35809,"Result Size":2320,"JVM GC Time":156,"Result Serialization Time":0,"Memory Bytes Spilled":0,"Disk Bytes Spilled":0,"Input Metrics":{"Data Read Method":"Hadoop","Bytes Read":65536,"Records Read":4253268}}}
{"Event":"SparkListenerTaskEnd","Stage ID":1,"Stage Attempt ID":0,"Task Type":"ResultTask","Task End Reason":{"Reason":"Success"},"Task Info":{"Task ID":6,"Index":2,"Attempt":0,"Launch Time":1467148308832,"Executor ID":"6","Host":"192.168.1.3","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":1467148426380,"Failed":false,"Accumulables":[]},"Task Metrics":{"Host Name":"192.168.1.11","Executor Deserialize Time":54800,"Executor Run Time":62705,"Result Size":2254,"JVM GC Time":217,"Result Serialization Time":1,"Memory Bytes Spilled":0,"Disk Bytes Spilled":0,"Input Metrics":{"Data Read Method":"Hadoop","Bytes Read":65536,"Records Read":4253236}}}
{"Event":"SparkListenerStageCompleted","Stage Info":{"Stage ID":1,"Stage Attempt ID":0,"Stage Name":"sortByKey at /home/daniar/documents/SPARK/spark-1.6.1/sort.py:12","Number of Tasks":4,"RDD Info":[{"RDD ID":3,"Name":"PythonRDD","Callsite":"sortByKey at /home/daniar/documents/SPARK/spark-1.6.1/sort.py:12","Parent IDs":[1],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":4,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":0,"Name":"../generated_file/list_int","Scope":"{\"id\":\"0\",\"name\":\"textFile\"}","Callsite":"textFile at NativeMethodAccessorImpl.java:-2","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":4,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":1,"Name":"../generated_file/list_int","Scope":"{\"id\":\"0\",\"name\":\"textFile\"}","Callsite":"textFile at NativeMethodAccessorImpl.java:-2","Parent IDs":[0],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":4,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.collect(RDD.scala:926)\norg.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:405)\norg.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:498)\npy4j.reflection.MethodInvoker.invoke(MethodInvoker.java:231)\npy4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:381)\npy4j.Gateway.invoke(Gateway.java:259)\npy4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:133)\npy4j.commands.CallCommand.execute(CallCommand.java:79)\npy4j.GatewayConnection.run(GatewayConnection.java:209)\njava.lang.Thread.run(Thread.java:745)","Submission Time":1467148308831,"Completion Time":1467148426381,"Accumulables":[]}}
{"Event":"SparkListenerJobEnd","Job ID":1,"Completion Time":1467148426381,"Job Result":{"Result":"JobSucceeded"}}
{"Event":"SparkListenerJobStart","Job ID":2,"Submission Time":1467148426568,"Stage Infos":[{"Stage ID":2,"Stage Attempt ID":0,"Stage Name":"sortByKey at /home/daniar/documents/SPARK/spark-1.6.1/sort.py:12","Number of Tasks":4,"RDD Info":[{"RDD ID":5,"Name":"PairwiseRDD","Callsite":"sortByKey at /home/daniar/documents/SPARK/spark-1.6.1/sort.py:12","Parent IDs":[4],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":4,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":4,"Name":"PythonRDD","Callsite":"sortByKey at /home/daniar/documents/SPARK/spark-1.6.1/sort.py:12","Parent IDs":[1],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":4,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":0,"Name":"../generated_file/list_int","Scope":"{\"id\":\"0\",\"name\":\"textFile\"}","Callsite":"textFile at NativeMethodAccessorImpl.java:-2","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":4,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":1,"Name":"../generated_file/list_int","Scope":"{\"id\":\"0\",\"name\":\"textFile\"}","Callsite":"textFile at NativeMethodAccessorImpl.java:-2","Parent IDs":[0],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":4,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.<init>(RDD.scala:98)\norg.apache.spark.api.python.PairwiseRDD.<init>(PythonRDD.scala:338)\nsun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\nsun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\nsun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\njava.lang.reflect.Constructor.newInstance(Constructor.java:423)\npy4j.reflection.MethodInvoker.invoke(MethodInvoker.java:234)\npy4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:381)\npy4j.Gateway.invoke(Gateway.java:214)\npy4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:79)\npy4j.commands.ConstructorCommand.execute(ConstructorCommand.java:68)\npy4j.GatewayConnection.run(GatewayConnection.java:209)\njava.lang.Thread.run(Thread.java:745)","Accumulables":[]},{"Stage ID":3,"Stage Attempt ID":0,"Stage Name":"saveAsTextFile at NativeMethodAccessorImpl.java:-2","Number of Tasks":4,"RDD Info":[{"RDD ID":10,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"11\",\"name\":\"saveAsTextFile\"}","Callsite":"saveAsTextFile at NativeMethodAccessorImpl.java:-2","Parent IDs":[9],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":4,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":7,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"9\",\"name\":\"mapPartitions\"}","Callsite":"mapPartitions at PythonRDD.scala:374","Parent IDs":[6],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":4,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":6,"Name":"ShuffledRDD","Scope":"{\"id\":\"8\",\"name\":\"partitionBy\"}","Callsite":"partitionBy at NativeMethodAccessorImpl.java:-2","Parent IDs":[5],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":4,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":8,"Name":"PythonRDD","Callsite":"RDD at PythonRDD.scala:43","Parent IDs":[7],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":4,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":9,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"10\",\"name\":\"map\"}","Callsite":"map at NativeMethodAccessorImpl.java:-2","Parent IDs":[8],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":4,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[2],"Details":"org.apache.spark.api.java.AbstractJavaRDDLike.saveAsTextFile(JavaRDDLike.scala:46)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:498)\npy4j.reflection.MethodInvoker.invoke(MethodInvoker.java:231)\npy4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:381)\npy4j.Gateway.invoke(Gateway.java:259)\npy4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:133)\npy4j.commands.CallCommand.execute(CallCommand.java:79)\npy4j.GatewayConnection.run(GatewayConnection.java:209)\njava.lang.Thread.run(Thread.java:745)","Accumulables":[]}],"Stage IDs":[2,3],"Properties":{"spark.rdd.scope.noOverride":"true","spark.rdd.scope":"{\"id\":\"11\",\"name\":\"saveAsTextFile\"}"}}
{"Event":"SparkListenerStageSubmitted","Stage Info":{"Stage ID":2,"Stage Attempt ID":0,"Stage Name":"sortByKey at /home/daniar/documents/SPARK/spark-1.6.1/sort.py:12","Number of Tasks":4,"RDD Info":[{"RDD ID":5,"Name":"PairwiseRDD","Callsite":"sortByKey at /home/daniar/documents/SPARK/spark-1.6.1/sort.py:12","Parent IDs":[4],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":4,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":4,"Name":"PythonRDD","Callsite":"sortByKey at /home/daniar/documents/SPARK/spark-1.6.1/sort.py:12","Parent IDs":[1],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":4,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":0,"Name":"../generated_file/list_int","Scope":"{\"id\":\"0\",\"name\":\"textFile\"}","Callsite":"textFile at NativeMethodAccessorImpl.java:-2","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":4,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":1,"Name":"../generated_file/list_int","Scope":"{\"id\":\"0\",\"name\":\"textFile\"}","Callsite":"textFile at NativeMethodAccessorImpl.java:-2","Parent IDs":[0],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":4,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.<init>(RDD.scala:98)\norg.apache.spark.api.python.PairwiseRDD.<init>(PythonRDD.scala:338)\nsun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\nsun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\nsun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\njava.lang.reflect.Constructor.newInstance(Constructor.java:423)\npy4j.reflection.MethodInvoker.invoke(MethodInvoker.java:234)\npy4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:381)\npy4j.Gateway.invoke(Gateway.java:214)\npy4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:79)\npy4j.commands.ConstructorCommand.execute(ConstructorCommand.java:68)\npy4j.GatewayConnection.run(GatewayConnection.java:209)\njava.lang.Thread.run(Thread.java:745)","Accumulables":[]},"Properties":{"spark.rdd.scope.noOverride":"true","spark.rdd.scope":"{\"id\":\"11\",\"name\":\"saveAsTextFile\"}"}}
{"Event":"SparkListenerTaskStart","Stage ID":2,"Stage Attempt ID":0,"Task Info":{"Task ID":8,"Index":0,"Attempt":0,"Launch Time":1467148426588,"Executor ID":"6","Host":"192.168.1.3","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":0,"Failed":false,"Accumulables":[]}}
{"Event":"SparkListenerTaskStart","Stage ID":2,"Stage Attempt ID":0,"Task Info":{"Task ID":9,"Index":1,"Attempt":0,"Launch Time":1467148426590,"Executor ID":"7","Host":"192.168.1.3","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":0,"Failed":false,"Accumulables":[]}}
{"Event":"SparkListenerTaskStart","Stage ID":2,"Stage Attempt ID":0,"Task Info":{"Task ID":10,"Index":2,"Attempt":0,"Launch Time":1467148426591,"Executor ID":"1","Host":"192.168.1.3","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":0,"Failed":false,"Accumulables":[]}}
{"Event":"SparkListenerTaskStart","Stage ID":2,"Stage Attempt ID":0,"Task Info":{"Task ID":11,"Index":3,"Attempt":0,"Launch Time":1467148426591,"Executor ID":"5","Host":"192.168.1.3","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":0,"Failed":false,"Accumulables":[]}}
{"Event":"SparkListenerTaskEnd","Stage ID":2,"Stage Attempt ID":0,"Task Type":"ShuffleMapTask","Task End Reason":{"Reason":"Success"},"Task Info":{"Task ID":8,"Index":0,"Attempt":0,"Launch Time":1467148426588,"Executor ID":"6","Host":"192.168.1.3","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":1467148449664,"Failed":false,"Accumulables":[]},"Task Metrics":{"Host Name":"192.168.1.11","Executor Deserialize Time":65,"Executor Run Time":22997,"Result Size":2321,"JVM GC Time":160,"Result Serialization Time":1,"Memory Bytes Spilled":0,"Disk Bytes Spilled":0,"Shuffle Write Metrics":{"Shuffle Bytes Written":65356545,"Shuffle Write Time":997448530,"Shuffle Records Written":164},"Input Metrics":{"Data Read Method":"Hadoop","Bytes Read":0,"Records Read":4253453}}}
{"Event":"SparkListenerTaskEnd","Stage ID":2,"Stage Attempt ID":0,"Task Type":"ShuffleMapTask","Task End Reason":{"Reason":"Success"},"Task Info":{"Task ID":10,"Index":2,"Attempt":0,"Launch Time":1467148426591,"Executor ID":"1","Host":"192.168.1.3","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":1467148452696,"Failed":false,"Accumulables":[]},"Task Metrics":{"Host Name":"192.168.1.11","Executor Deserialize Time":86,"Executor Run Time":26012,"Result Size":2321,"JVM GC Time":199,"Result Serialization Time":1,"Memory Bytes Spilled":0,"Disk Bytes Spilled":0,"Shuffle Write Metrics":{"Shuffle Bytes Written":65352156,"Shuffle Write Time":3055369859,"Shuffle Records Written":164},"Input Metrics":{"Data Read Method":"Hadoop","Bytes Read":65536,"Records Read":4253236}}}
{"Event":"SparkListenerTaskEnd","Stage ID":2,"Stage Attempt ID":0,"Task Type":"ShuffleMapTask","Task End Reason":{"Reason":"Success"},"Task Info":{"Task ID":9,"Index":1,"Attempt":0,"Launch Time":1467148426590,"Executor ID":"7","Host":"192.168.1.3","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":1467148453568,"Failed":false,"Accumulables":[]},"Task Metrics":{"Host Name":"192.168.1.11","Executor Deserialize Time":766,"Executor Run Time":26081,"Result Size":2321,"JVM GC Time":352,"Result Serialization Time":1,"Memory Bytes Spilled":0,"Disk Bytes Spilled":0,"Shuffle Write Metrics":{"Shuffle Bytes Written":65350616,"Shuffle Write Time":3012032585,"Shuffle Records Written":164},"Input Metrics":{"Data Read Method":"Hadoop","Bytes Read":65536,"Records Read":4253268}}}
{"Event":"SparkListenerTaskEnd","Stage ID":2,"Stage Attempt ID":0,"Task Type":"ShuffleMapTask","Task End Reason":{"Reason":"Success"},"Task Info":{"Task ID":11,"Index":3,"Attempt":0,"Launch Time":1467148426591,"Executor ID":"5","Host":"192.168.1.3","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":1467148497839,"Failed":false,"Accumulables":[]},"Task Metrics":{"Host Name":"192.168.1.11","Executor Deserialize Time":27631,"Executor Run Time":43477,"Result Size":2321,"JVM GC Time":131,"Result Serialization Time":0,"Memory Bytes Spilled":0,"Disk Bytes Spilled":0,"Shuffle Write Metrics":{"Shuffle Bytes Written":53536423,"Shuffle Write Time":994485354,"Shuffle Records Written":148},"Input Metrics":{"Data Read Method":"Hadoop","Bytes Read":65536,"Records Read":3484461}}}
{"Event":"SparkListenerStageCompleted","Stage Info":{"Stage ID":2,"Stage Attempt ID":0,"Stage Name":"sortByKey at /home/daniar/documents/SPARK/spark-1.6.1/sort.py:12","Number of Tasks":4,"RDD Info":[{"RDD ID":5,"Name":"PairwiseRDD","Callsite":"sortByKey at /home/daniar/documents/SPARK/spark-1.6.1/sort.py:12","Parent IDs":[4],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":4,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":4,"Name":"PythonRDD","Callsite":"sortByKey at /home/daniar/documents/SPARK/spark-1.6.1/sort.py:12","Parent IDs":[1],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":4,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":0,"Name":"../generated_file/list_int","Scope":"{\"id\":\"0\",\"name\":\"textFile\"}","Callsite":"textFile at NativeMethodAccessorImpl.java:-2","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":4,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":1,"Name":"../generated_file/list_int","Scope":"{\"id\":\"0\",\"name\":\"textFile\"}","Callsite":"textFile at NativeMethodAccessorImpl.java:-2","Parent IDs":[0],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":4,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.<init>(RDD.scala:98)\norg.apache.spark.api.python.PairwiseRDD.<init>(PythonRDD.scala:338)\nsun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\nsun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\nsun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\njava.lang.reflect.Constructor.newInstance(Constructor.java:423)\npy4j.reflection.MethodInvoker.invoke(MethodInvoker.java:234)\npy4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:381)\npy4j.Gateway.invoke(Gateway.java:214)\npy4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:79)\npy4j.commands.ConstructorCommand.execute(ConstructorCommand.java:68)\npy4j.GatewayConnection.run(GatewayConnection.java:209)\njava.lang.Thread.run(Thread.java:745)","Submission Time":1467148426588,"Completion Time":1467148497839,"Accumulables":[]}}
{"Event":"SparkListenerStageSubmitted","Stage Info":{"Stage ID":3,"Stage Attempt ID":0,"Stage Name":"saveAsTextFile at NativeMethodAccessorImpl.java:-2","Number of Tasks":4,"RDD Info":[{"RDD ID":10,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"11\",\"name\":\"saveAsTextFile\"}","Callsite":"saveAsTextFile at NativeMethodAccessorImpl.java:-2","Parent IDs":[9],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":4,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":7,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"9\",\"name\":\"mapPartitions\"}","Callsite":"mapPartitions at PythonRDD.scala:374","Parent IDs":[6],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":4,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":6,"Name":"ShuffledRDD","Scope":"{\"id\":\"8\",\"name\":\"partitionBy\"}","Callsite":"partitionBy at NativeMethodAccessorImpl.java:-2","Parent IDs":[5],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":4,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":8,"Name":"PythonRDD","Callsite":"RDD at PythonRDD.scala:43","Parent IDs":[7],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":4,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":9,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"10\",\"name\":\"map\"}","Callsite":"map at NativeMethodAccessorImpl.java:-2","Parent IDs":[8],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":4,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[2],"Details":"org.apache.spark.api.java.AbstractJavaRDDLike.saveAsTextFile(JavaRDDLike.scala:46)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:498)\npy4j.reflection.MethodInvoker.invoke(MethodInvoker.java:231)\npy4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:381)\npy4j.Gateway.invoke(Gateway.java:259)\npy4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:133)\npy4j.commands.CallCommand.execute(CallCommand.java:79)\npy4j.GatewayConnection.run(GatewayConnection.java:209)\njava.lang.Thread.run(Thread.java:745)","Accumulables":[]},"Properties":{"spark.rdd.scope.noOverride":"true","spark.rdd.scope":"{\"id\":\"11\",\"name\":\"saveAsTextFile\"}"}}
{"Event":"SparkListenerTaskStart","Stage ID":3,"Stage Attempt ID":0,"Task Info":{"Task ID":12,"Index":0,"Attempt":0,"Launch Time":1467148498021,"Executor ID":"7","Host":"192.168.1.3","Locality":"ANY","Speculative":false,"Getting Result Time":0,"Finish Time":0,"Failed":false,"Accumulables":[]}}
{"Event":"SparkListenerTaskStart","Stage ID":3,"Stage Attempt ID":0,"Task Info":{"Task ID":13,"Index":1,"Attempt":0,"Launch Time":1467148498022,"Executor ID":"1","Host":"192.168.1.3","Locality":"ANY","Speculative":false,"Getting Result Time":0,"Finish Time":0,"Failed":false,"Accumulables":[]}}
{"Event":"SparkListenerTaskStart","Stage ID":3,"Stage Attempt ID":0,"Task Info":{"Task ID":14,"Index":2,"Attempt":0,"Launch Time":1467148498022,"Executor ID":"0","Host":"192.168.1.3","Locality":"ANY","Speculative":false,"Getting Result Time":0,"Finish Time":0,"Failed":false,"Accumulables":[]}}
{"Event":"SparkListenerTaskStart","Stage ID":3,"Stage Attempt ID":0,"Task Info":{"Task ID":15,"Index":3,"Attempt":0,"Launch Time":1467148498023,"Executor ID":"6","Host":"192.168.1.3","Locality":"ANY","Speculative":false,"Getting Result Time":0,"Finish Time":0,"Failed":false,"Accumulables":[]}}
{"Event":"SparkListenerTaskEnd","Stage ID":3,"Stage Attempt ID":0,"Task Type":"ResultTask","Task End Reason":{"Reason":"FetchFailed","Block Manager Address":{"Executor ID":"5","Host":"192.168.1.11","Port":36316},"Shuffle ID":0,"Map ID":3,"Reduce ID":2,"Message":"org.apache.spark.shuffle.FetchFailedException: Failed to connect to /192.168.1.11:36316\n\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.throwFetchFailedException(ShuffleBlockFetcherIterator.scala:323)\n\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:300)\n\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:51)\n\tat scala.collection.Iterator$$anon$11.next(Iterator.scala:328)\n\tat scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)\n\tat org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:32)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:39)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:727)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1157)\n\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:452)\n\tat org.apache.spark.api.python.PythonRunner$WriterThread$$anonfun$run$3.apply(PythonRDD.scala:280)\n\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1765)\n\tat org.apache.spark.api.python.PythonRunner$WriterThread.run(PythonRDD.scala:239)\nCaused by: java.io.IOException: Failed to connect to /192.168.1.11:36316\n\tat org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:216)\n\tat org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:167)\n\tat org.apache.spark.network.netty.NettyBlockTransferService$$anon$1.createAndStart(NettyBlockTransferService.scala:90)\n\tat org.apache.spark.network.shuffle.RetryingBlockFetcher.fetchAllOutstanding(RetryingBlockFetcher.java:140)\n\tat org.apache.spark.network.shuffle.RetryingBlockFetcher.access$200(RetryingBlockFetcher.java:43)\n\tat org.apache.spark.network.shuffle.RetryingBlockFetcher$1.run(RetryingBlockFetcher.java:170)\n\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n\tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n\tat java.lang.Thread.run(Thread.java:745)\nCaused by: java.net.NoRouteToHostException: No route to host: /192.168.1.11:36316\n\tat sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)\n\tat sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)\n\tat io.netty.channel.socket.nio.NioSocketChannel.doFinishConnect(NioSocketChannel.java:224)\n\tat io.netty.channel.nio.AbstractNioChannel$AbstractNioUnsafe.finishConnect(AbstractNioChannel.java:289)\n\tat io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:528)\n\tat io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:468)\n\tat io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:382)\n\tat io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:354)\n\tat io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:111)\n\t... 1 more\n"},"Task Info":{"Task ID":14,"Index":2,"Attempt":0,"Launch Time":1467148498022,"Executor ID":"0","Host":"192.168.1.3","Locality":"ANY","Speculative":false,"Getting Result Time":0,"Finish Time":1467148519394,"Failed":true,"Accumulables":[]}}
{"Event":"SparkListenerStageCompleted","Stage Info":{"Stage ID":3,"Stage Attempt ID":0,"Stage Name":"saveAsTextFile at NativeMethodAccessorImpl.java:-2","Number of Tasks":4,"RDD Info":[{"RDD ID":10,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"11\",\"name\":\"saveAsTextFile\"}","Callsite":"saveAsTextFile at NativeMethodAccessorImpl.java:-2","Parent IDs":[9],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":4,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":7,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"9\",\"name\":\"mapPartitions\"}","Callsite":"mapPartitions at PythonRDD.scala:374","Parent IDs":[6],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":4,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":6,"Name":"ShuffledRDD","Scope":"{\"id\":\"8\",\"name\":\"partitionBy\"}","Callsite":"partitionBy at NativeMethodAccessorImpl.java:-2","Parent IDs":[5],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":4,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":8,"Name":"PythonRDD","Callsite":"RDD at PythonRDD.scala:43","Parent IDs":[7],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":4,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":9,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"10\",\"name\":\"map\"}","Callsite":"map at NativeMethodAccessorImpl.java:-2","Parent IDs":[8],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":4,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[2],"Details":"org.apache.spark.api.java.AbstractJavaRDDLike.saveAsTextFile(JavaRDDLike.scala:46)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:498)\npy4j.reflection.MethodInvoker.invoke(MethodInvoker.java:231)\npy4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:381)\npy4j.Gateway.invoke(Gateway.java:259)\npy4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:133)\npy4j.commands.CallCommand.execute(CallCommand.java:79)\npy4j.GatewayConnection.run(GatewayConnection.java:209)\njava.lang.Thread.run(Thread.java:745)","Submission Time":1467148498021,"Completion Time":1467148519409,"Failure Reason":"org.apache.spark.shuffle.FetchFailedException: Failed to connect to /192.168.1.11:36316\n\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.throwFetchFailedException(ShuffleBlockFetcherIterator.scala:323)\n\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:300)\n\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:51)\n\tat scala.collection.Iterator$$anon$11.next(Iterator.scala:328)\n\tat scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)\n\tat org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:32)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:39)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:727)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1157)\n\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:452)\n\tat org.apache.spark.api.python.PythonRunner$WriterThread$$anonfun$run$3.apply(PythonRDD.scala:280)\n\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1765)\n\tat org.apache.spark.api.python.PythonRunner$WriterThread.run(PythonRDD.scala:239)\nCaused by: java.io.IOException: Failed to connect to /192.168.1.11:36316\n\tat org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:216)\n\tat org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:167)\n\tat org.apache.spark.network.netty.NettyBlockTransferService$$anon$1.createAndStart(NettyBlockTransferService.scala:90)\n\tat org.apache.spark.network.shuffle.RetryingBlockFetcher.fetchAllOutstanding(RetryingBlockFetcher.java:140)\n\tat org.apache.spark.network.shuffle.RetryingBlockFetcher.access$200(RetryingBlockFetcher.java:43)\n\tat org.apache.spark.network.shuffle.RetryingBlockFetcher$1.run(RetryingBlockFetcher.java:170)\n\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n\tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n\tat java.lang.Thread.run(Thread.java:745)\nCaused by: java.net.NoRouteToHostException: No route to host: /192.168.1.11:36316\n\tat sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)\n\tat sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)\n\tat io.netty.channel.socket.nio.NioSocketChannel.doFinishConnect(NioSocketChannel.java:224)\n\tat io.netty.channel.nio.AbstractNioChannel$AbstractNioUnsafe.finishConnect(AbstractNioChannel.java:289)\n\tat io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:528)\n\tat io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:468)\n\tat io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:382)\n\tat io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:354)\n\tat io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:111)\n\t... 1 more\n","Accumulables":[]}}
{"Event":"SparkListenerBlockManagerRemoved","Block Manager ID":{"Executor ID":"5","Host":"192.168.1.11","Port":36316},"Timestamp":1467148519566}
{"Event":"SparkListenerStageSubmitted","Stage Info":{"Stage ID":2,"Stage Attempt ID":1,"Stage Name":"sortByKey at /home/daniar/documents/SPARK/spark-1.6.1/sort.py:12","Number of Tasks":1,"RDD Info":[{"RDD ID":5,"Name":"PairwiseRDD","Callsite":"sortByKey at /home/daniar/documents/SPARK/spark-1.6.1/sort.py:12","Parent IDs":[4],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":4,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":4,"Name":"PythonRDD","Callsite":"sortByKey at /home/daniar/documents/SPARK/spark-1.6.1/sort.py:12","Parent IDs":[1],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":4,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":0,"Name":"../generated_file/list_int","Scope":"{\"id\":\"0\",\"name\":\"textFile\"}","Callsite":"textFile at NativeMethodAccessorImpl.java:-2","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":4,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":1,"Name":"../generated_file/list_int","Scope":"{\"id\":\"0\",\"name\":\"textFile\"}","Callsite":"textFile at NativeMethodAccessorImpl.java:-2","Parent IDs":[0],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":4,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.<init>(RDD.scala:98)\norg.apache.spark.api.python.PairwiseRDD.<init>(PythonRDD.scala:338)\nsun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\nsun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\nsun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\njava.lang.reflect.Constructor.newInstance(Constructor.java:423)\npy4j.reflection.MethodInvoker.invoke(MethodInvoker.java:234)\npy4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:381)\npy4j.Gateway.invoke(Gateway.java:214)\npy4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:79)\npy4j.commands.ConstructorCommand.execute(ConstructorCommand.java:68)\npy4j.GatewayConnection.run(GatewayConnection.java:209)\njava.lang.Thread.run(Thread.java:745)","Accumulables":[]},"Properties":{"spark.rdd.scope.noOverride":"true","spark.rdd.scope":"{\"id\":\"11\",\"name\":\"saveAsTextFile\"}"}}
{"Event":"SparkListenerTaskStart","Stage ID":2,"Stage Attempt ID":1,"Task Info":{"Task ID":16,"Index":0,"Attempt":0,"Launch Time":1467148519723,"Executor ID":"1","Host":"192.168.1.3","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":0,"Failed":false,"Accumulables":[]}}
{"Event":"SparkListenerTaskEnd","Stage ID":3,"Stage Attempt ID":0,"Task Type":"ResultTask","Task End Reason":{"Reason":"FetchFailed","Block Manager Address":{"Executor ID":"7","Host":"192.168.1.11","Port":50553},"Shuffle ID":0,"Map ID":1,"Reduce ID":3,"Message":"org.apache.spark.shuffle.FetchFailedException: Failed to connect to /192.168.1.11:50553\n\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.throwFetchFailedException(ShuffleBlockFetcherIterator.scala:323)\n\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:300)\n\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:51)\n\tat scala.collection.Iterator$$anon$11.next(Iterator.scala:328)\n\tat scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)\n\tat org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:32)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:39)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:727)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1157)\n\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:452)\n\tat org.apache.spark.api.python.PythonRunner$WriterThread$$anonfun$run$3.apply(PythonRDD.scala:280)\n\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1765)\n\tat org.apache.spark.api.python.PythonRunner$WriterThread.run(PythonRDD.scala:239)\nCaused by: java.io.IOException: Failed to connect to /192.168.1.11:50553\n\tat org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:216)\n\tat org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:167)\n\tat org.apache.spark.network.netty.NettyBlockTransferService$$anon$1.createAndStart(NettyBlockTransferService.scala:90)\n\tat org.apache.spark.network.shuffle.RetryingBlockFetcher.fetchAllOutstanding(RetryingBlockFetcher.java:140)\n\tat org.apache.spark.network.shuffle.RetryingBlockFetcher.access$200(RetryingBlockFetcher.java:43)\n\tat org.apache.spark.network.shuffle.RetryingBlockFetcher$1.run(RetryingBlockFetcher.java:170)\n\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n\tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n\tat java.lang.Thread.run(Thread.java:745)\nCaused by: java.net.NoRouteToHostException: No route to host: /192.168.1.11:50553\n\tat sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)\n\tat sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)\n\tat io.netty.channel.socket.nio.NioSocketChannel.doFinishConnect(NioSocketChannel.java:224)\n\tat io.netty.channel.nio.AbstractNioChannel$AbstractNioUnsafe.finishConnect(AbstractNioChannel.java:289)\n\tat io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:528)\n\tat io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:468)\n\tat io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:382)\n\tat io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:354)\n\tat io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:111)\n\t... 1 more\n"},"Task Info":{"Task ID":15,"Index":3,"Attempt":0,"Launch Time":1467148498023,"Executor ID":"6","Host":"192.168.1.3","Locality":"ANY","Speculative":false,"Getting Result Time":0,"Finish Time":1467148521275,"Failed":true,"Accumulables":[]}}
{"Event":"SparkListenerBlockManagerRemoved","Block Manager ID":{"Executor ID":"7","Host":"192.168.1.11","Port":50553},"Timestamp":1467148521288}
{"Event":"SparkListenerTaskEnd","Stage ID":3,"Stage Attempt ID":0,"Task Type":"ResultTask","Task End Reason":{"Reason":"FetchFailed","Block Manager Address":{"Executor ID":"7","Host":"192.168.1.11","Port":50553},"Shuffle ID":0,"Map ID":1,"Reduce ID":1,"Message":"org.apache.spark.shuffle.FetchFailedException: Failed to connect to /192.168.1.11:50553\n\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.throwFetchFailedException(ShuffleBlockFetcherIterator.scala:323)\n\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:300)\n\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:51)\n\tat scala.collection.Iterator$$anon$11.next(Iterator.scala:328)\n\tat scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)\n\tat org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:32)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:39)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:727)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1157)\n\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:452)\n\tat org.apache.spark.api.python.PythonRunner$WriterThread$$anonfun$run$3.apply(PythonRDD.scala:280)\n\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1765)\n\tat org.apache.spark.api.python.PythonRunner$WriterThread.run(PythonRDD.scala:239)\nCaused by: java.io.IOException: Failed to connect to /192.168.1.11:50553\n\tat org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:216)\n\tat org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:167)\n\tat org.apache.spark.network.netty.NettyBlockTransferService$$anon$1.createAndStart(NettyBlockTransferService.scala:90)\n\tat org.apache.spark.network.shuffle.RetryingBlockFetcher.fetchAllOutstanding(RetryingBlockFetcher.java:140)\n\tat org.apache.spark.network.shuffle.RetryingBlockFetcher.access$200(RetryingBlockFetcher.java:43)\n\tat org.apache.spark.network.shuffle.RetryingBlockFetcher$1.run(RetryingBlockFetcher.java:170)\n\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n\tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n\tat java.lang.Thread.run(Thread.java:745)\nCaused by: java.net.NoRouteToHostException: No route to host: /192.168.1.11:50553\n\tat sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)\n\tat sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)\n\tat io.netty.channel.socket.nio.NioSocketChannel.doFinishConnect(NioSocketChannel.java:224)\n\tat io.netty.channel.nio.AbstractNioChannel$AbstractNioUnsafe.finishConnect(AbstractNioChannel.java:289)\n\tat io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:528)\n\tat io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:468)\n\tat io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:382)\n\tat io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:354)\n\tat io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:111)\n\t... 1 more\n"},"Task Info":{"Task ID":13,"Index":1,"Attempt":0,"Launch Time":1467148498022,"Executor ID":"1","Host":"192.168.1.3","Locality":"ANY","Speculative":false,"Getting Result Time":0,"Finish Time":1467148521285,"Failed":true,"Accumulables":[]}}
{"Event":"SparkListenerTaskEnd","Stage ID":3,"Stage Attempt ID":0,"Task Type":"ResultTask","Task End Reason":{"Reason":"FetchFailed","Block Manager Address":{"Executor ID":"5","Host":"192.168.1.11","Port":36316},"Shuffle ID":0,"Map ID":3,"Reduce ID":0,"Message":"org.apache.spark.shuffle.FetchFailedException: Failed to connect to /192.168.1.11:36316\n\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.throwFetchFailedException(ShuffleBlockFetcherIterator.scala:323)\n\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:300)\n\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:51)\n\tat scala.collection.Iterator$$anon$11.next(Iterator.scala:328)\n\tat scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)\n\tat org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:32)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:39)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:727)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1157)\n\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:452)\n\tat org.apache.spark.api.python.PythonRunner$WriterThread$$anonfun$run$3.apply(PythonRDD.scala:280)\n\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1765)\n\tat org.apache.spark.api.python.PythonRunner$WriterThread.run(PythonRDD.scala:239)\nCaused by: java.io.IOException: Failed to connect to /192.168.1.11:36316\n\tat org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:216)\n\tat org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:167)\n\tat org.apache.spark.network.netty.NettyBlockTransferService$$anon$1.createAndStart(NettyBlockTransferService.scala:90)\n\tat org.apache.spark.network.shuffle.RetryingBlockFetcher.fetchAllOutstanding(RetryingBlockFetcher.java:140)\n\tat org.apache.spark.network.shuffle.RetryingBlockFetcher.access$200(RetryingBlockFetcher.java:43)\n\tat org.apache.spark.network.shuffle.RetryingBlockFetcher$1.run(RetryingBlockFetcher.java:170)\n\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n\tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n\tat java.lang.Thread.run(Thread.java:745)\nCaused by: java.net.NoRouteToHostException: No route to host: /192.168.1.11:36316\n\tat sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)\n\tat sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)\n\tat io.netty.channel.socket.nio.NioSocketChannel.doFinishConnect(NioSocketChannel.java:224)\n\tat io.netty.channel.nio.AbstractNioChannel$AbstractNioUnsafe.finishConnect(AbstractNioChannel.java:289)\n\tat io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:528)\n\tat io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:468)\n\tat io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:382)\n\tat io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:354)\n\tat io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:111)\n\t... 1 more\n"},"Task Info":{"Task ID":12,"Index":0,"Attempt":0,"Launch Time":1467148498021,"Executor ID":"7","Host":"192.168.1.3","Locality":"ANY","Speculative":false,"Getting Result Time":0,"Finish Time":1467148522873,"Failed":true,"Accumulables":[]}}
{"Event":"SparkListenerBlockManagerAdded","Block Manager ID":{"Executor ID":"7","Host":"192.168.1.11","Port":50553},"Maximum Memory":535953408,"Timestamp":1467148522962}
{"Event":"SparkListenerBlockManagerAdded","Block Manager ID":{"Executor ID":"5","Host":"192.168.1.11","Port":36316},"Maximum Memory":535953408,"Timestamp":1467148523202}
{"Event":"SparkListenerTaskEnd","Stage ID":2,"Stage Attempt ID":1,"Task Type":"ShuffleMapTask","Task End Reason":{"Reason":"Success"},"Task Info":{"Task ID":16,"Index":0,"Attempt":0,"Launch Time":1467148519723,"Executor ID":"1","Host":"192.168.1.3","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":1467148533438,"Failed":false,"Accumulables":[]},"Task Metrics":{"Host Name":"192.168.1.11","Executor Deserialize Time":24,"Executor Run Time":13687,"Result Size":2321,"JVM GC Time":56,"Result Serialization Time":0,"Memory Bytes Spilled":0,"Disk Bytes Spilled":0,"Shuffle Write Metrics":{"Shuffle Bytes Written":53536423,"Shuffle Write Time":625973784,"Shuffle Records Written":148},"Input Metrics":{"Data Read Method":"Hadoop","Bytes Read":65536,"Records Read":3484461}}}
{"Event":"SparkListenerStageCompleted","Stage Info":{"Stage ID":2,"Stage Attempt ID":1,"Stage Name":"sortByKey at /home/daniar/documents/SPARK/spark-1.6.1/sort.py:12","Number of Tasks":1,"RDD Info":[{"RDD ID":5,"Name":"PairwiseRDD","Callsite":"sortByKey at /home/daniar/documents/SPARK/spark-1.6.1/sort.py:12","Parent IDs":[4],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":4,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":4,"Name":"PythonRDD","Callsite":"sortByKey at /home/daniar/documents/SPARK/spark-1.6.1/sort.py:12","Parent IDs":[1],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":4,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":0,"Name":"../generated_file/list_int","Scope":"{\"id\":\"0\",\"name\":\"textFile\"}","Callsite":"textFile at NativeMethodAccessorImpl.java:-2","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":4,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":1,"Name":"../generated_file/list_int","Scope":"{\"id\":\"0\",\"name\":\"textFile\"}","Callsite":"textFile at NativeMethodAccessorImpl.java:-2","Parent IDs":[0],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":4,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.<init>(RDD.scala:98)\norg.apache.spark.api.python.PairwiseRDD.<init>(PythonRDD.scala:338)\nsun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\nsun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\nsun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\njava.lang.reflect.Constructor.newInstance(Constructor.java:423)\npy4j.reflection.MethodInvoker.invoke(MethodInvoker.java:234)\npy4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:381)\npy4j.Gateway.invoke(Gateway.java:214)\npy4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:79)\npy4j.commands.ConstructorCommand.execute(ConstructorCommand.java:68)\npy4j.GatewayConnection.run(GatewayConnection.java:209)\njava.lang.Thread.run(Thread.java:745)","Submission Time":1467148519722,"Completion Time":1467148533439,"Accumulables":[]}}
{"Event":"SparkListenerStageSubmitted","Stage Info":{"Stage ID":2,"Stage Attempt ID":2,"Stage Name":"sortByKey at /home/daniar/documents/SPARK/spark-1.6.1/sort.py:12","Number of Tasks":1,"RDD Info":[{"RDD ID":5,"Name":"PairwiseRDD","Callsite":"sortByKey at /home/daniar/documents/SPARK/spark-1.6.1/sort.py:12","Parent IDs":[4],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":4,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":4,"Name":"PythonRDD","Callsite":"sortByKey at /home/daniar/documents/SPARK/spark-1.6.1/sort.py:12","Parent IDs":[1],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":4,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":0,"Name":"../generated_file/list_int","Scope":"{\"id\":\"0\",\"name\":\"textFile\"}","Callsite":"textFile at NativeMethodAccessorImpl.java:-2","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":4,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":1,"Name":"../generated_file/list_int","Scope":"{\"id\":\"0\",\"name\":\"textFile\"}","Callsite":"textFile at NativeMethodAccessorImpl.java:-2","Parent IDs":[0],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":4,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.<init>(RDD.scala:98)\norg.apache.spark.api.python.PairwiseRDD.<init>(PythonRDD.scala:338)\nsun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\nsun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\nsun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\njava.lang.reflect.Constructor.newInstance(Constructor.java:423)\npy4j.reflection.MethodInvoker.invoke(MethodInvoker.java:234)\npy4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:381)\npy4j.Gateway.invoke(Gateway.java:214)\npy4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:79)\npy4j.commands.ConstructorCommand.execute(ConstructorCommand.java:68)\npy4j.GatewayConnection.run(GatewayConnection.java:209)\njava.lang.Thread.run(Thread.java:745)","Accumulables":[]},"Properties":{"spark.rdd.scope.noOverride":"true","spark.rdd.scope":"{\"id\":\"11\",\"name\":\"saveAsTextFile\"}"}}
{"Event":"SparkListenerTaskStart","Stage ID":2,"Stage Attempt ID":2,"Task Info":{"Task ID":17,"Index":0,"Attempt":0,"Launch Time":1467148533503,"Executor ID":"1","Host":"192.168.1.3","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":0,"Failed":false,"Accumulables":[]}}
{"Event":"SparkListenerTaskEnd","Stage ID":2,"Stage Attempt ID":2,"Task Type":"ShuffleMapTask","Task End Reason":{"Reason":"Success"},"Task Info":{"Task ID":17,"Index":0,"Attempt":0,"Launch Time":1467148533503,"Executor ID":"1","Host":"192.168.1.3","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":1467148548782,"Failed":false,"Accumulables":[]},"Task Metrics":{"Host Name":"192.168.1.11","Executor Deserialize Time":8,"Executor Run Time":15267,"Result Size":2321,"JVM GC Time":77,"Result Serialization Time":1,"Memory Bytes Spilled":0,"Disk Bytes Spilled":0,"Shuffle Write Metrics":{"Shuffle Bytes Written":65350616,"Shuffle Write Time":219909743,"Shuffle Records Written":164},"Input Metrics":{"Data Read Method":"Hadoop","Bytes Read":65536,"Records Read":4253268}}}
{"Event":"SparkListenerStageCompleted","Stage Info":{"Stage ID":2,"Stage Attempt ID":2,"Stage Name":"sortByKey at /home/daniar/documents/SPARK/spark-1.6.1/sort.py:12","Number of Tasks":1,"RDD Info":[{"RDD ID":5,"Name":"PairwiseRDD","Callsite":"sortByKey at /home/daniar/documents/SPARK/spark-1.6.1/sort.py:12","Parent IDs":[4],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":4,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":4,"Name":"PythonRDD","Callsite":"sortByKey at /home/daniar/documents/SPARK/spark-1.6.1/sort.py:12","Parent IDs":[1],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":4,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":0,"Name":"../generated_file/list_int","Scope":"{\"id\":\"0\",\"name\":\"textFile\"}","Callsite":"textFile at NativeMethodAccessorImpl.java:-2","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":4,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":1,"Name":"../generated_file/list_int","Scope":"{\"id\":\"0\",\"name\":\"textFile\"}","Callsite":"textFile at NativeMethodAccessorImpl.java:-2","Parent IDs":[0],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":4,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.<init>(RDD.scala:98)\norg.apache.spark.api.python.PairwiseRDD.<init>(PythonRDD.scala:338)\nsun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\nsun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\nsun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\njava.lang.reflect.Constructor.newInstance(Constructor.java:423)\npy4j.reflection.MethodInvoker.invoke(MethodInvoker.java:234)\npy4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:381)\npy4j.Gateway.invoke(Gateway.java:214)\npy4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:79)\npy4j.commands.ConstructorCommand.execute(ConstructorCommand.java:68)\npy4j.GatewayConnection.run(GatewayConnection.java:209)\njava.lang.Thread.run(Thread.java:745)","Submission Time":1467148533503,"Completion Time":1467148548782,"Accumulables":[]}}
{"Event":"SparkListenerStageSubmitted","Stage Info":{"Stage ID":3,"Stage Attempt ID":1,"Stage Name":"saveAsTextFile at NativeMethodAccessorImpl.java:-2","Number of Tasks":4,"RDD Info":[{"RDD ID":10,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"11\",\"name\":\"saveAsTextFile\"}","Callsite":"saveAsTextFile at NativeMethodAccessorImpl.java:-2","Parent IDs":[9],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":4,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":7,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"9\",\"name\":\"mapPartitions\"}","Callsite":"mapPartitions at PythonRDD.scala:374","Parent IDs":[6],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":4,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":6,"Name":"ShuffledRDD","Scope":"{\"id\":\"8\",\"name\":\"partitionBy\"}","Callsite":"partitionBy at NativeMethodAccessorImpl.java:-2","Parent IDs":[5],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":4,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":8,"Name":"PythonRDD","Callsite":"RDD at PythonRDD.scala:43","Parent IDs":[7],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":4,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":9,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"10\",\"name\":\"map\"}","Callsite":"map at NativeMethodAccessorImpl.java:-2","Parent IDs":[8],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":4,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[2],"Details":"org.apache.spark.api.java.AbstractJavaRDDLike.saveAsTextFile(JavaRDDLike.scala:46)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:498)\npy4j.reflection.MethodInvoker.invoke(MethodInvoker.java:231)\npy4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:381)\npy4j.Gateway.invoke(Gateway.java:259)\npy4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:133)\npy4j.commands.CallCommand.execute(CallCommand.java:79)\npy4j.GatewayConnection.run(GatewayConnection.java:209)\njava.lang.Thread.run(Thread.java:745)","Accumulables":[]},"Properties":{"spark.rdd.scope.noOverride":"true","spark.rdd.scope":"{\"id\":\"11\",\"name\":\"saveAsTextFile\"}"}}
{"Event":"SparkListenerTaskStart","Stage ID":3,"Stage Attempt ID":1,"Task Info":{"Task ID":18,"Index":0,"Attempt":0,"Launch Time":1467148548815,"Executor ID":"6","Host":"192.168.1.3","Locality":"ANY","Speculative":false,"Getting Result Time":0,"Finish Time":0,"Failed":false,"Accumulables":[]}}
{"Event":"SparkListenerTaskStart","Stage ID":3,"Stage Attempt ID":1,"Task Info":{"Task ID":19,"Index":1,"Attempt":0,"Launch Time":1467148548815,"Executor ID":"3","Host":"192.168.1.3","Locality":"ANY","Speculative":false,"Getting Result Time":0,"Finish Time":0,"Failed":false,"Accumulables":[]}}
{"Event":"SparkListenerTaskStart","Stage ID":3,"Stage Attempt ID":1,"Task Info":{"Task ID":20,"Index":2,"Attempt":0,"Launch Time":1467148548815,"Executor ID":"5","Host":"192.168.1.3","Locality":"ANY","Speculative":false,"Getting Result Time":0,"Finish Time":0,"Failed":false,"Accumulables":[]}}
{"Event":"SparkListenerTaskStart","Stage ID":3,"Stage Attempt ID":1,"Task Info":{"Task ID":21,"Index":3,"Attempt":0,"Launch Time":1467148548815,"Executor ID":"1","Host":"192.168.1.3","Locality":"ANY","Speculative":false,"Getting Result Time":0,"Finish Time":0,"Failed":false,"Accumulables":[]}}
{"Event":"SparkListenerTaskEnd","Stage ID":3,"Stage Attempt ID":1,"Task Type":"ResultTask","Task End Reason":{"Reason":"FetchFailed","Block Manager Address":{"Executor ID":"6","Host":"192.168.1.11","Port":38720},"Shuffle ID":0,"Map ID":0,"Reduce ID":1,"Message":"org.apache.spark.shuffle.FetchFailedException: Failed to connect to /192.168.1.11:38720\n\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.throwFetchFailedException(ShuffleBlockFetcherIterator.scala:323)\n\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:300)\n\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:51)\n\tat scala.collection.Iterator$$anon$11.next(Iterator.scala:328)\n\tat scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)\n\tat org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:32)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:39)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:727)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1157)\n\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:452)\n\tat org.apache.spark.api.python.PythonRunner$WriterThread$$anonfun$run$3.apply(PythonRDD.scala:280)\n\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1765)\n\tat org.apache.spark.api.python.PythonRunner$WriterThread.run(PythonRDD.scala:239)\nCaused by: java.io.IOException: Failed to connect to /192.168.1.11:38720\n\tat org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:216)\n\tat org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:167)\n\tat org.apache.spark.network.netty.NettyBlockTransferService$$anon$1.createAndStart(NettyBlockTransferService.scala:90)\n\tat org.apache.spark.network.shuffle.RetryingBlockFetcher.fetchAllOutstanding(RetryingBlockFetcher.java:140)\n\tat org.apache.spark.network.shuffle.RetryingBlockFetcher.access$200(RetryingBlockFetcher.java:43)\n\tat org.apache.spark.network.shuffle.RetryingBlockFetcher$1.run(RetryingBlockFetcher.java:170)\n\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n\tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n\tat java.lang.Thread.run(Thread.java:745)\nCaused by: java.net.NoRouteToHostException: No route to host: /192.168.1.11:38720\n\tat sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)\n\tat sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)\n\tat io.netty.channel.socket.nio.NioSocketChannel.doFinishConnect(NioSocketChannel.java:224)\n\tat io.netty.channel.nio.AbstractNioChannel$AbstractNioUnsafe.finishConnect(AbstractNioChannel.java:289)\n\tat io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:528)\n\tat io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:468)\n\tat io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:382)\n\tat io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:354)\n\tat io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:111)\n\t... 1 more\n"},"Task Info":{"Task ID":19,"Index":1,"Attempt":0,"Launch Time":1467148548815,"Executor ID":"3","Host":"192.168.1.3","Locality":"ANY","Speculative":false,"Getting Result Time":0,"Finish Time":1467148572897,"Failed":true,"Accumulables":[]}}
{"Event":"SparkListenerStageCompleted","Stage Info":{"Stage ID":3,"Stage Attempt ID":1,"Stage Name":"saveAsTextFile at NativeMethodAccessorImpl.java:-2","Number of Tasks":4,"RDD Info":[{"RDD ID":10,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"11\",\"name\":\"saveAsTextFile\"}","Callsite":"saveAsTextFile at NativeMethodAccessorImpl.java:-2","Parent IDs":[9],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":4,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":7,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"9\",\"name\":\"mapPartitions\"}","Callsite":"mapPartitions at PythonRDD.scala:374","Parent IDs":[6],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":4,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":6,"Name":"ShuffledRDD","Scope":"{\"id\":\"8\",\"name\":\"partitionBy\"}","Callsite":"partitionBy at NativeMethodAccessorImpl.java:-2","Parent IDs":[5],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":4,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":8,"Name":"PythonRDD","Callsite":"RDD at PythonRDD.scala:43","Parent IDs":[7],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":4,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":9,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"10\",\"name\":\"map\"}","Callsite":"map at NativeMethodAccessorImpl.java:-2","Parent IDs":[8],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":4,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[2],"Details":"org.apache.spark.api.java.AbstractJavaRDDLike.saveAsTextFile(JavaRDDLike.scala:46)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:498)\npy4j.reflection.MethodInvoker.invoke(MethodInvoker.java:231)\npy4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:381)\npy4j.Gateway.invoke(Gateway.java:259)\npy4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:133)\npy4j.commands.CallCommand.execute(CallCommand.java:79)\npy4j.GatewayConnection.run(GatewayConnection.java:209)\njava.lang.Thread.run(Thread.java:745)","Submission Time":1467148548815,"Completion Time":1467148572898,"Failure Reason":"org.apache.spark.shuffle.FetchFailedException: Failed to connect to /192.168.1.11:38720\n\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.throwFetchFailedException(ShuffleBlockFetcherIterator.scala:323)\n\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:300)\n\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:51)\n\tat scala.collection.Iterator$$anon$11.next(Iterator.scala:328)\n\tat scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)\n\tat org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:32)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:39)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:727)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1157)\n\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:452)\n\tat org.apache.spark.api.python.PythonRunner$WriterThread$$anonfun$run$3.apply(PythonRDD.scala:280)\n\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1765)\n\tat org.apache.spark.api.python.PythonRunner$WriterThread.run(PythonRDD.scala:239)\nCaused by: java.io.IOException: Failed to connect to /192.168.1.11:38720\n\tat org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:216)\n\tat org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:167)\n\tat org.apache.spark.network.netty.NettyBlockTransferService$$anon$1.createAndStart(NettyBlockTransferService.scala:90)\n\tat org.apache.spark.network.shuffle.RetryingBlockFetcher.fetchAllOutstanding(RetryingBlockFetcher.java:140)\n\tat org.apache.spark.network.shuffle.RetryingBlockFetcher.access$200(RetryingBlockFetcher.java:43)\n\tat org.apache.spark.network.shuffle.RetryingBlockFetcher$1.run(RetryingBlockFetcher.java:170)\n\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n\tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n\tat java.lang.Thread.run(Thread.java:745)\nCaused by: java.net.NoRouteToHostException: No route to host: /192.168.1.11:38720\n\tat sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)\n\tat sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)\n\tat io.netty.channel.socket.nio.NioSocketChannel.doFinishConnect(NioSocketChannel.java:224)\n\tat io.netty.channel.nio.AbstractNioChannel$AbstractNioUnsafe.finishConnect(AbstractNioChannel.java:289)\n\tat io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:528)\n\tat io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:468)\n\tat io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:382)\n\tat io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:354)\n\tat io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:111)\n\t... 1 more\n","Accumulables":[]}}
{"Event":"SparkListenerBlockManagerRemoved","Block Manager ID":{"Executor ID":"6","Host":"192.168.1.11","Port":38720},"Timestamp":1467148572899}
{"Event":"SparkListenerTaskEnd","Stage ID":3,"Stage Attempt ID":1,"Task Type":"ResultTask","Task End Reason":{"Reason":"FetchFailed","Block Manager Address":{"Executor ID":"1","Host":"192.168.1.11","Port":50530},"Shuffle ID":0,"Map ID":2,"Reduce ID":0,"Message":"org.apache.spark.shuffle.FetchFailedException: Failed to connect to /192.168.1.11:50530\n\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.throwFetchFailedException(ShuffleBlockFetcherIterator.scala:323)\n\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:300)\n\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:51)\n\tat scala.collection.Iterator$$anon$11.next(Iterator.scala:328)\n\tat scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)\n\tat org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:32)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:39)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:727)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1157)\n\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:452)\n\tat org.apache.spark.api.python.PythonRunner$WriterThread$$anonfun$run$3.apply(PythonRDD.scala:280)\n\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1765)\n\tat org.apache.spark.api.python.PythonRunner$WriterThread.run(PythonRDD.scala:239)\nCaused by: java.io.IOException: Failed to connect to /192.168.1.11:50530\n\tat org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:216)\n\tat org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:167)\n\tat org.apache.spark.network.netty.NettyBlockTransferService$$anon$1.createAndStart(NettyBlockTransferService.scala:90)\n\tat org.apache.spark.network.shuffle.RetryingBlockFetcher.fetchAllOutstanding(RetryingBlockFetcher.java:140)\n\tat org.apache.spark.network.shuffle.RetryingBlockFetcher.access$200(RetryingBlockFetcher.java:43)\n\tat org.apache.spark.network.shuffle.RetryingBlockFetcher$1.run(RetryingBlockFetcher.java:170)\n\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n\tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n\tat java.lang.Thread.run(Thread.java:745)\nCaused by: java.net.NoRouteToHostException: No route to host: /192.168.1.11:50530\n\tat sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)\n\tat sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)\n\tat io.netty.channel.socket.nio.NioSocketChannel.doFinishConnect(NioSocketChannel.java:224)\n\tat io.netty.channel.nio.AbstractNioChannel$AbstractNioUnsafe.finishConnect(AbstractNioChannel.java:289)\n\tat io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:528)\n\tat io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:468)\n\tat io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:382)\n\tat io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:354)\n\tat io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:111)\n\t... 1 more\n"},"Task Info":{"Task ID":18,"Index":0,"Attempt":0,"Launch Time":1467148548815,"Executor ID":"6","Host":"192.168.1.3","Locality":"ANY","Speculative":false,"Getting Result Time":0,"Finish Time":1467148572973,"Failed":true,"Accumulables":[]}}
{"Event":"SparkListenerBlockManagerRemoved","Block Manager ID":{"Executor ID":"1","Host":"192.168.1.11","Port":50530},"Timestamp":1467148572974}
{"Event":"SparkListenerStageSubmitted","Stage Info":{"Stage ID":2,"Stage Attempt ID":3,"Stage Name":"sortByKey at /home/daniar/documents/SPARK/spark-1.6.1/sort.py:12","Number of Tasks":4,"RDD Info":[{"RDD ID":5,"Name":"PairwiseRDD","Callsite":"sortByKey at /home/daniar/documents/SPARK/spark-1.6.1/sort.py:12","Parent IDs":[4],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":4,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":4,"Name":"PythonRDD","Callsite":"sortByKey at /home/daniar/documents/SPARK/spark-1.6.1/sort.py:12","Parent IDs":[1],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":4,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":0,"Name":"../generated_file/list_int","Scope":"{\"id\":\"0\",\"name\":\"textFile\"}","Callsite":"textFile at NativeMethodAccessorImpl.java:-2","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":4,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":1,"Name":"../generated_file/list_int","Scope":"{\"id\":\"0\",\"name\":\"textFile\"}","Callsite":"textFile at NativeMethodAccessorImpl.java:-2","Parent IDs":[0],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":4,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.<init>(RDD.scala:98)\norg.apache.spark.api.python.PairwiseRDD.<init>(PythonRDD.scala:338)\nsun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\nsun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\nsun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\njava.lang.reflect.Constructor.newInstance(Constructor.java:423)\npy4j.reflection.MethodInvoker.invoke(MethodInvoker.java:234)\npy4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:381)\npy4j.Gateway.invoke(Gateway.java:214)\npy4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:79)\npy4j.commands.ConstructorCommand.execute(ConstructorCommand.java:68)\npy4j.GatewayConnection.run(GatewayConnection.java:209)\njava.lang.Thread.run(Thread.java:745)","Accumulables":[]},"Properties":{"spark.rdd.scope.noOverride":"true","spark.rdd.scope":"{\"id\":\"11\",\"name\":\"saveAsTextFile\"}"}}
{"Event":"SparkListenerTaskStart","Stage ID":2,"Stage Attempt ID":3,"Task Info":{"Task ID":22,"Index":0,"Attempt":0,"Launch Time":1467148573104,"Executor ID":"5","Host":"192.168.1.3","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":0,"Failed":false,"Accumulables":[]}}
{"Event":"SparkListenerTaskStart","Stage ID":2,"Stage Attempt ID":3,"Task Info":{"Task ID":23,"Index":1,"Attempt":0,"Launch Time":1467148573104,"Executor ID":"1","Host":"192.168.1.3","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":0,"Failed":false,"Accumulables":[]}}
{"Event":"SparkListenerTaskStart","Stage ID":2,"Stage Attempt ID":3,"Task Info":{"Task ID":24,"Index":2,"Attempt":0,"Launch Time":1467148573105,"Executor ID":"4","Host":"192.168.1.3","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":0,"Failed":false,"Accumulables":[]}}
{"Event":"SparkListenerTaskStart","Stage ID":2,"Stage Attempt ID":3,"Task Info":{"Task ID":25,"Index":3,"Attempt":0,"Launch Time":1467148573105,"Executor ID":"0","Host":"192.168.1.3","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":0,"Failed":false,"Accumulables":[]}}
{"Event":"SparkListenerBlockManagerAdded","Block Manager ID":{"Executor ID":"1","Host":"192.168.1.11","Port":50530},"Maximum Memory":535953408,"Timestamp":1467148573204}
{"Event":"SparkListenerTaskEnd","Stage ID":3,"Stage Attempt ID":1,"Task Type":"ResultTask","Task End Reason":{"Reason":"FetchFailed","Block Manager Address":{"Executor ID":"6","Host":"192.168.1.11","Port":38720},"Shuffle ID":0,"Map ID":0,"Reduce ID":3,"Message":"org.apache.spark.shuffle.FetchFailedException: Failed to connect to /192.168.1.11:38720\n\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.throwFetchFailedException(ShuffleBlockFetcherIterator.scala:323)\n\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:300)\n\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:51)\n\tat scala.collection.Iterator$$anon$11.next(Iterator.scala:328)\n\tat scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)\n\tat org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:32)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:39)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:727)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1157)\n\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:452)\n\tat org.apache.spark.api.python.PythonRunner$WriterThread$$anonfun$run$3.apply(PythonRDD.scala:280)\n\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1765)\n\tat org.apache.spark.api.python.PythonRunner$WriterThread.run(PythonRDD.scala:239)\nCaused by: java.io.IOException: Failed to connect to /192.168.1.11:38720\n\tat org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:216)\n\tat org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:167)\n\tat org.apache.spark.network.netty.NettyBlockTransferService$$anon$1.createAndStart(NettyBlockTransferService.scala:90)\n\tat org.apache.spark.network.shuffle.RetryingBlockFetcher.fetchAllOutstanding(RetryingBlockFetcher.java:140)\n\tat org.apache.spark.network.shuffle.RetryingBlockFetcher.access$200(RetryingBlockFetcher.java:43)\n\tat org.apache.spark.network.shuffle.RetryingBlockFetcher$1.run(RetryingBlockFetcher.java:170)\n\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n\tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n\tat java.lang.Thread.run(Thread.java:745)\nCaused by: java.net.NoRouteToHostException: No route to host: /192.168.1.11:38720\n\tat sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)\n\tat sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)\n\tat io.netty.channel.socket.nio.NioSocketChannel.doFinishConnect(NioSocketChannel.java:224)\n\tat io.netty.channel.nio.AbstractNioChannel$AbstractNioUnsafe.finishConnect(AbstractNioChannel.java:289)\n\tat io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:528)\n\tat io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:468)\n\tat io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:382)\n\tat io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:354)\n\tat io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:111)\n\t... 1 more\n"},"Task Info":{"Task ID":21,"Index":3,"Attempt":0,"Launch Time":1467148548815,"Executor ID":"1","Host":"192.168.1.3","Locality":"ANY","Speculative":false,"Getting Result Time":0,"Finish Time":1467148574076,"Failed":true,"Accumulables":[]}}
{"Event":"SparkListenerBlockManagerAdded","Block Manager ID":{"Executor ID":"6","Host":"192.168.1.11","Port":38720},"Maximum Memory":535953408,"Timestamp":1467148577654}
{"Event":"SparkListenerTaskEnd","Stage ID":2,"Stage Attempt ID":3,"Task Type":"ShuffleMapTask","Task End Reason":{"Reason":"Success"},"Task Info":{"Task ID":22,"Index":0,"Attempt":0,"Launch Time":1467148573104,"Executor ID":"5","Host":"192.168.1.3","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":1467148594918,"Failed":false,"Accumulables":[]},"Task Metrics":{"Host Name":"192.168.1.11","Executor Deserialize Time":27,"Executor Run Time":21781,"Result Size":2321,"JVM GC Time":141,"Result Serialization Time":0,"Memory Bytes Spilled":0,"Disk Bytes Spilled":0,"Shuffle Write Metrics":{"Shuffle Bytes Written":65356545,"Shuffle Write Time":522723033,"Shuffle Records Written":164},"Input Metrics":{"Data Read Method":"Hadoop","Bytes Read":0,"Records Read":4253453}}}
{"Event":"SparkListenerTaskEnd","Stage ID":3,"Stage Attempt ID":1,"Task Type":"ResultTask","Task End Reason":{"Reason":"FetchFailed","Block Manager Address":{"Executor ID":"6","Host":"192.168.1.11","Port":38720},"Shuffle ID":0,"Map ID":0,"Reduce ID":2,"Message":"org.apache.spark.shuffle.FetchFailedException: Failed to connect to /192.168.1.11:38720\n\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.throwFetchFailedException(ShuffleBlockFetcherIterator.scala:323)\n\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:300)\n\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:51)\n\tat scala.collection.Iterator$$anon$11.next(Iterator.scala:328)\n\tat scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)\n\tat org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:32)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:39)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:727)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1157)\n\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:452)\n\tat org.apache.spark.api.python.PythonRunner$WriterThread$$anonfun$run$3.apply(PythonRDD.scala:280)\n\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1765)\n\tat org.apache.spark.api.python.PythonRunner$WriterThread.run(PythonRDD.scala:239)\nCaused by: java.io.IOException: Failed to connect to /192.168.1.11:38720\n\tat org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:216)\n\tat org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:167)\n\tat org.apache.spark.network.netty.NettyBlockTransferService$$anon$1.createAndStart(NettyBlockTransferService.scala:90)\n\tat org.apache.spark.network.shuffle.RetryingBlockFetcher.fetchAllOutstanding(RetryingBlockFetcher.java:140)\n\tat org.apache.spark.network.shuffle.RetryingBlockFetcher.access$200(RetryingBlockFetcher.java:43)\n\tat org.apache.spark.network.shuffle.RetryingBlockFetcher$1.run(RetryingBlockFetcher.java:170)\n\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n\tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n\tat java.lang.Thread.run(Thread.java:745)\nCaused by: java.net.NoRouteToHostException: No route to host: /192.168.1.11:38720\n\tat sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)\n\tat sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)\n\tat io.netty.channel.socket.nio.NioSocketChannel.doFinishConnect(NioSocketChannel.java:224)\n\tat io.netty.channel.nio.AbstractNioChannel$AbstractNioUnsafe.finishConnect(AbstractNioChannel.java:289)\n\tat io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:528)\n\tat io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:468)\n\tat io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:382)\n\tat io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:354)\n\tat io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:111)\n\t... 1 more\n"},"Task Info":{"Task ID":20,"Index":2,"Attempt":0,"Launch Time":1467148548815,"Executor ID":"5","Host":"192.168.1.3","Locality":"ANY","Speculative":false,"Getting Result Time":0,"Finish Time":1467148595867,"Failed":true,"Accumulables":[]}}
{"Event":"SparkListenerTaskEnd","Stage ID":2,"Stage Attempt ID":3,"Task Type":"ShuffleMapTask","Task End Reason":{"Reason":"Success"},"Task Info":{"Task ID":23,"Index":1,"Attempt":0,"Launch Time":1467148573104,"Executor ID":"1","Host":"192.168.1.3","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":1467148595963,"Failed":false,"Accumulables":[]},"Task Metrics":{"Host Name":"192.168.1.11","Executor Deserialize Time":88,"Executor Run Time":22760,"Result Size":2321,"JVM GC Time":92,"Result Serialization Time":0,"Memory Bytes Spilled":0,"Disk Bytes Spilled":0,"Shuffle Write Metrics":{"Shuffle Bytes Written":65350616,"Shuffle Write Time":1471451105,"Shuffle Records Written":164},"Input Metrics":{"Data Read Method":"Hadoop","Bytes Read":65536,"Records Read":4253268}}}
{"Event":"SparkListenerTaskEnd","Stage ID":2,"Stage Attempt ID":3,"Task Type":"ShuffleMapTask","Task End Reason":{"Reason":"Success"},"Task Info":{"Task ID":25,"Index":3,"Attempt":0,"Launch Time":1467148573105,"Executor ID":"0","Host":"192.168.1.3","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":1467148613625,"Failed":false,"Accumulables":[]},"Task Metrics":{"Host Name":"192.168.1.11","Executor Deserialize Time":23201,"Executor Run Time":17313,"Result Size":2321,"JVM GC Time":41,"Result Serialization Time":1,"Memory Bytes Spilled":0,"Disk Bytes Spilled":0,"Shuffle Write Metrics":{"Shuffle Bytes Written":53536423,"Shuffle Write Time":901790840,"Shuffle Records Written":148},"Input Metrics":{"Data Read Method":"Hadoop","Bytes Read":65536,"Records Read":3484461}}}
{"Event":"SparkListenerTaskEnd","Stage ID":2,"Stage Attempt ID":3,"Task Type":"ShuffleMapTask","Task End Reason":{"Reason":"Success"},"Task Info":{"Task ID":24,"Index":2,"Attempt":0,"Launch Time":1467148573105,"Executor ID":"4","Host":"192.168.1.3","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":1467148807875,"Failed":false,"Accumulables":[]},"Task Metrics":{"Host Name":"192.168.1.11","Executor Deserialize Time":54932,"Executor Run Time":178777,"Result Size":2321,"JVM GC Time":201,"Result Serialization Time":5,"Memory Bytes Spilled":0,"Disk Bytes Spilled":0,"Shuffle Write Metrics":{"Shuffle Bytes Written":65352156,"Shuffle Write Time":755005509,"Shuffle Records Written":164},"Input Metrics":{"Data Read Method":"Hadoop","Bytes Read":65536,"Records Read":4253236}}}
{"Event":"SparkListenerStageCompleted","Stage Info":{"Stage ID":2,"Stage Attempt ID":3,"Stage Name":"sortByKey at /home/daniar/documents/SPARK/spark-1.6.1/sort.py:12","Number of Tasks":4,"RDD Info":[{"RDD ID":5,"Name":"PairwiseRDD","Callsite":"sortByKey at /home/daniar/documents/SPARK/spark-1.6.1/sort.py:12","Parent IDs":[4],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":4,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":4,"Name":"PythonRDD","Callsite":"sortByKey at /home/daniar/documents/SPARK/spark-1.6.1/sort.py:12","Parent IDs":[1],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":4,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":0,"Name":"../generated_file/list_int","Scope":"{\"id\":\"0\",\"name\":\"textFile\"}","Callsite":"textFile at NativeMethodAccessorImpl.java:-2","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":4,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":1,"Name":"../generated_file/list_int","Scope":"{\"id\":\"0\",\"name\":\"textFile\"}","Callsite":"textFile at NativeMethodAccessorImpl.java:-2","Parent IDs":[0],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":4,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.<init>(RDD.scala:98)\norg.apache.spark.api.python.PairwiseRDD.<init>(PythonRDD.scala:338)\nsun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\nsun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\nsun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\njava.lang.reflect.Constructor.newInstance(Constructor.java:423)\npy4j.reflection.MethodInvoker.invoke(MethodInvoker.java:234)\npy4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:381)\npy4j.Gateway.invoke(Gateway.java:214)\npy4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:79)\npy4j.commands.ConstructorCommand.execute(ConstructorCommand.java:68)\npy4j.GatewayConnection.run(GatewayConnection.java:209)\njava.lang.Thread.run(Thread.java:745)","Submission Time":1467148573104,"Completion Time":1467148807876,"Accumulables":[]}}
{"Event":"SparkListenerStageSubmitted","Stage Info":{"Stage ID":3,"Stage Attempt ID":2,"Stage Name":"saveAsTextFile at NativeMethodAccessorImpl.java:-2","Number of Tasks":4,"RDD Info":[{"RDD ID":10,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"11\",\"name\":\"saveAsTextFile\"}","Callsite":"saveAsTextFile at NativeMethodAccessorImpl.java:-2","Parent IDs":[9],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":4,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":7,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"9\",\"name\":\"mapPartitions\"}","Callsite":"mapPartitions at PythonRDD.scala:374","Parent IDs":[6],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":4,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":6,"Name":"ShuffledRDD","Scope":"{\"id\":\"8\",\"name\":\"partitionBy\"}","Callsite":"partitionBy at NativeMethodAccessorImpl.java:-2","Parent IDs":[5],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":4,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":8,"Name":"PythonRDD","Callsite":"RDD at PythonRDD.scala:43","Parent IDs":[7],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":4,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":9,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"10\",\"name\":\"map\"}","Callsite":"map at NativeMethodAccessorImpl.java:-2","Parent IDs":[8],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":4,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[2],"Details":"org.apache.spark.api.java.AbstractJavaRDDLike.saveAsTextFile(JavaRDDLike.scala:46)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:498)\npy4j.reflection.MethodInvoker.invoke(MethodInvoker.java:231)\npy4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:381)\npy4j.Gateway.invoke(Gateway.java:259)\npy4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:133)\npy4j.commands.CallCommand.execute(CallCommand.java:79)\npy4j.GatewayConnection.run(GatewayConnection.java:209)\njava.lang.Thread.run(Thread.java:745)","Accumulables":[]},"Properties":{"spark.rdd.scope.noOverride":"true","spark.rdd.scope":"{\"id\":\"11\",\"name\":\"saveAsTextFile\"}"}}
{"Event":"SparkListenerTaskStart","Stage ID":3,"Stage Attempt ID":2,"Task Info":{"Task ID":26,"Index":0,"Attempt":0,"Launch Time":1467148807903,"Executor ID":"4","Host":"192.168.1.3","Locality":"ANY","Speculative":false,"Getting Result Time":0,"Finish Time":0,"Failed":false,"Accumulables":[]}}
{"Event":"SparkListenerTaskStart","Stage ID":3,"Stage Attempt ID":2,"Task Info":{"Task ID":27,"Index":1,"Attempt":0,"Launch Time":1467148807904,"Executor ID":"0","Host":"192.168.1.3","Locality":"ANY","Speculative":false,"Getting Result Time":0,"Finish Time":0,"Failed":false,"Accumulables":[]}}
{"Event":"SparkListenerTaskStart","Stage ID":3,"Stage Attempt ID":2,"Task Info":{"Task ID":28,"Index":2,"Attempt":0,"Launch Time":1467148807904,"Executor ID":"5","Host":"192.168.1.3","Locality":"ANY","Speculative":false,"Getting Result Time":0,"Finish Time":0,"Failed":false,"Accumulables":[]}}
{"Event":"SparkListenerTaskStart","Stage ID":3,"Stage Attempt ID":2,"Task Info":{"Task ID":29,"Index":3,"Attempt":0,"Launch Time":1467148807904,"Executor ID":"3","Host":"192.168.1.3","Locality":"ANY","Speculative":false,"Getting Result Time":0,"Finish Time":0,"Failed":false,"Accumulables":[]}}
{"Event":"SparkListenerTaskEnd","Stage ID":3,"Stage Attempt ID":2,"Task Type":"ResultTask","Task End Reason":{"Reason":"FetchFailed","Block Manager Address":{"Executor ID":"0","Host":"192.168.1.11","Port":43480},"Shuffle ID":0,"Map ID":3,"Reduce ID":3,"Message":"org.apache.spark.shuffle.FetchFailedException: Failed to connect to /192.168.1.11:43480\n\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.throwFetchFailedException(ShuffleBlockFetcherIterator.scala:323)\n\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:300)\n\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:51)\n\tat scala.collection.Iterator$$anon$11.next(Iterator.scala:328)\n\tat scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)\n\tat org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:32)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:39)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:727)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1157)\n\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:452)\n\tat org.apache.spark.api.python.PythonRunner$WriterThread$$anonfun$run$3.apply(PythonRDD.scala:280)\n\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1765)\n\tat org.apache.spark.api.python.PythonRunner$WriterThread.run(PythonRDD.scala:239)\nCaused by: java.io.IOException: Failed to connect to /192.168.1.11:43480\n\tat org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:216)\n\tat org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:167)\n\tat org.apache.spark.network.netty.NettyBlockTransferService$$anon$1.createAndStart(NettyBlockTransferService.scala:90)\n\tat org.apache.spark.network.shuffle.RetryingBlockFetcher.fetchAllOutstanding(RetryingBlockFetcher.java:140)\n\tat org.apache.spark.network.shuffle.RetryingBlockFetcher.access$200(RetryingBlockFetcher.java:43)\n\tat org.apache.spark.network.shuffle.RetryingBlockFetcher$1.run(RetryingBlockFetcher.java:170)\n\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n\tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n\tat java.lang.Thread.run(Thread.java:745)\nCaused by: java.net.NoRouteToHostException: No route to host: /192.168.1.11:43480\n\tat sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)\n\tat sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)\n\tat io.netty.channel.socket.nio.NioSocketChannel.doFinishConnect(NioSocketChannel.java:224)\n\tat io.netty.channel.nio.AbstractNioChannel$AbstractNioUnsafe.finishConnect(AbstractNioChannel.java:289)\n\tat io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:528)\n\tat io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:468)\n\tat io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:382)\n\tat io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:354)\n\tat io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:111)\n\t... 1 more\n"},"Task Info":{"Task ID":29,"Index":3,"Attempt":0,"Launch Time":1467148807904,"Executor ID":"3","Host":"192.168.1.3","Locality":"ANY","Speculative":false,"Getting Result Time":0,"Finish Time":1467148828996,"Failed":true,"Accumulables":[]}}
{"Event":"SparkListenerStageCompleted","Stage Info":{"Stage ID":3,"Stage Attempt ID":2,"Stage Name":"saveAsTextFile at NativeMethodAccessorImpl.java:-2","Number of Tasks":4,"RDD Info":[{"RDD ID":10,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"11\",\"name\":\"saveAsTextFile\"}","Callsite":"saveAsTextFile at NativeMethodAccessorImpl.java:-2","Parent IDs":[9],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":4,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":7,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"9\",\"name\":\"mapPartitions\"}","Callsite":"mapPartitions at PythonRDD.scala:374","Parent IDs":[6],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":4,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":6,"Name":"ShuffledRDD","Scope":"{\"id\":\"8\",\"name\":\"partitionBy\"}","Callsite":"partitionBy at NativeMethodAccessorImpl.java:-2","Parent IDs":[5],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":4,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":8,"Name":"PythonRDD","Callsite":"RDD at PythonRDD.scala:43","Parent IDs":[7],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":4,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":9,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"10\",\"name\":\"map\"}","Callsite":"map at NativeMethodAccessorImpl.java:-2","Parent IDs":[8],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":4,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[2],"Details":"org.apache.spark.api.java.AbstractJavaRDDLike.saveAsTextFile(JavaRDDLike.scala:46)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:498)\npy4j.reflection.MethodInvoker.invoke(MethodInvoker.java:231)\npy4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:381)\npy4j.Gateway.invoke(Gateway.java:259)\npy4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:133)\npy4j.commands.CallCommand.execute(CallCommand.java:79)\npy4j.GatewayConnection.run(GatewayConnection.java:209)\njava.lang.Thread.run(Thread.java:745)","Submission Time":1467148807903,"Completion Time":1467148828997,"Failure Reason":"org.apache.spark.shuffle.FetchFailedException: Failed to connect to /192.168.1.11:43480\n\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.throwFetchFailedException(ShuffleBlockFetcherIterator.scala:323)\n\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:300)\n\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:51)\n\tat scala.collection.Iterator$$anon$11.next(Iterator.scala:328)\n\tat scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)\n\tat org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:32)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:39)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:727)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1157)\n\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:452)\n\tat org.apache.spark.api.python.PythonRunner$WriterThread$$anonfun$run$3.apply(PythonRDD.scala:280)\n\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1765)\n\tat org.apache.spark.api.python.PythonRunner$WriterThread.run(PythonRDD.scala:239)\nCaused by: java.io.IOException: Failed to connect to /192.168.1.11:43480\n\tat org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:216)\n\tat org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:167)\n\tat org.apache.spark.network.netty.NettyBlockTransferService$$anon$1.createAndStart(NettyBlockTransferService.scala:90)\n\tat org.apache.spark.network.shuffle.RetryingBlockFetcher.fetchAllOutstanding(RetryingBlockFetcher.java:140)\n\tat org.apache.spark.network.shuffle.RetryingBlockFetcher.access$200(RetryingBlockFetcher.java:43)\n\tat org.apache.spark.network.shuffle.RetryingBlockFetcher$1.run(RetryingBlockFetcher.java:170)\n\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n\tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n\tat java.lang.Thread.run(Thread.java:745)\nCaused by: java.net.NoRouteToHostException: No route to host: /192.168.1.11:43480\n\tat sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)\n\tat sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)\n\tat io.netty.channel.socket.nio.NioSocketChannel.doFinishConnect(NioSocketChannel.java:224)\n\tat io.netty.channel.nio.AbstractNioChannel$AbstractNioUnsafe.finishConnect(AbstractNioChannel.java:289)\n\tat io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:528)\n\tat io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:468)\n\tat io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:382)\n\tat io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:354)\n\tat io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:111)\n\t... 1 more\n","Accumulables":[]}}
{"Event":"SparkListenerBlockManagerRemoved","Block Manager ID":{"Executor ID":"0","Host":"192.168.1.11","Port":43480},"Timestamp":1467148828998}
{"Event":"SparkListenerStageSubmitted","Stage Info":{"Stage ID":2,"Stage Attempt ID":4,"Stage Name":"sortByKey at /home/daniar/documents/SPARK/spark-1.6.1/sort.py:12","Number of Tasks":1,"RDD Info":[{"RDD ID":5,"Name":"PairwiseRDD","Callsite":"sortByKey at /home/daniar/documents/SPARK/spark-1.6.1/sort.py:12","Parent IDs":[4],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":4,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":4,"Name":"PythonRDD","Callsite":"sortByKey at /home/daniar/documents/SPARK/spark-1.6.1/sort.py:12","Parent IDs":[1],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":4,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":0,"Name":"../generated_file/list_int","Scope":"{\"id\":\"0\",\"name\":\"textFile\"}","Callsite":"textFile at NativeMethodAccessorImpl.java:-2","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":4,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":1,"Name":"../generated_file/list_int","Scope":"{\"id\":\"0\",\"name\":\"textFile\"}","Callsite":"textFile at NativeMethodAccessorImpl.java:-2","Parent IDs":[0],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":4,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.<init>(RDD.scala:98)\norg.apache.spark.api.python.PairwiseRDD.<init>(PythonRDD.scala:338)\nsun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\nsun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\nsun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\njava.lang.reflect.Constructor.newInstance(Constructor.java:423)\npy4j.reflection.MethodInvoker.invoke(MethodInvoker.java:234)\npy4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:381)\npy4j.Gateway.invoke(Gateway.java:214)\npy4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:79)\npy4j.commands.ConstructorCommand.execute(ConstructorCommand.java:68)\npy4j.GatewayConnection.run(GatewayConnection.java:209)\njava.lang.Thread.run(Thread.java:745)","Accumulables":[]},"Properties":{"spark.rdd.scope.noOverride":"true","spark.rdd.scope":"{\"id\":\"11\",\"name\":\"saveAsTextFile\"}"}}
{"Event":"SparkListenerTaskStart","Stage ID":2,"Stage Attempt ID":4,"Task Info":{"Task ID":30,"Index":0,"Attempt":0,"Launch Time":1467148829830,"Executor ID":"7","Host":"192.168.1.3","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":0,"Failed":false,"Accumulables":[]}}
{"Event":"SparkListenerTaskEnd","Stage ID":3,"Stage Attempt ID":2,"Task Type":"ResultTask","Task End Reason":{"Reason":"FetchFailed","Block Manager Address":{"Executor ID":"0","Host":"192.168.1.11","Port":43480},"Shuffle ID":0,"Map ID":3,"Reduce ID":2,"Message":"org.apache.spark.shuffle.FetchFailedException: Failed to connect to /192.168.1.11:43480\n\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.throwFetchFailedException(ShuffleBlockFetcherIterator.scala:323)\n\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:300)\n\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:51)\n\tat scala.collection.Iterator$$anon$11.next(Iterator.scala:328)\n\tat scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)\n\tat org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:32)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:39)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:727)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1157)\n\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:452)\n\tat org.apache.spark.api.python.PythonRunner$WriterThread$$anonfun$run$3.apply(PythonRDD.scala:280)\n\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1765)\n\tat org.apache.spark.api.python.PythonRunner$WriterThread.run(PythonRDD.scala:239)\nCaused by: java.io.IOException: Failed to connect to /192.168.1.11:43480\n\tat org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:216)\n\tat org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:167)\n\tat org.apache.spark.network.netty.NettyBlockTransferService$$anon$1.createAndStart(NettyBlockTransferService.scala:90)\n\tat org.apache.spark.network.shuffle.RetryingBlockFetcher.fetchAllOutstanding(RetryingBlockFetcher.java:140)\n\tat org.apache.spark.network.shuffle.RetryingBlockFetcher.access$200(RetryingBlockFetcher.java:43)\n\tat org.apache.spark.network.shuffle.RetryingBlockFetcher$1.run(RetryingBlockFetcher.java:170)\n\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n\tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n\tat java.lang.Thread.run(Thread.java:745)\nCaused by: java.net.NoRouteToHostException: No route to host: /192.168.1.11:43480\n\tat sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)\n\tat sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)\n\tat io.netty.channel.socket.nio.NioSocketChannel.doFinishConnect(NioSocketChannel.java:224)\n\tat io.netty.channel.nio.AbstractNioChannel$AbstractNioUnsafe.finishConnect(AbstractNioChannel.java:289)\n\tat io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:528)\n\tat io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:468)\n\tat io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:382)\n\tat io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:354)\n\tat io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:111)\n\t... 1 more\n"},"Task Info":{"Task ID":28,"Index":2,"Attempt":0,"Launch Time":1467148807904,"Executor ID":"5","Host":"192.168.1.3","Locality":"ANY","Speculative":false,"Getting Result Time":0,"Finish Time":1467148830874,"Failed":true,"Accumulables":[]}}
{"Event":"SparkListenerTaskEnd","Stage ID":3,"Stage Attempt ID":2,"Task Type":"ResultTask","Task End Reason":{"Reason":"FetchFailed","Block Manager Address":{"Executor ID":"5","Host":"192.168.1.11","Port":36316},"Shuffle ID":0,"Map ID":0,"Reduce ID":1,"Message":"org.apache.spark.shuffle.FetchFailedException: Failed to connect to /192.168.1.11:36316\n\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.throwFetchFailedException(ShuffleBlockFetcherIterator.scala:323)\n\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:300)\n\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:51)\n\tat scala.collection.Iterator$$anon$11.next(Iterator.scala:328)\n\tat scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)\n\tat org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:32)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:39)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:727)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1157)\n\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:452)\n\tat org.apache.spark.api.python.PythonRunner$WriterThread$$anonfun$run$3.apply(PythonRDD.scala:280)\n\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1765)\n\tat org.apache.spark.api.python.PythonRunner$WriterThread.run(PythonRDD.scala:239)\nCaused by: java.io.IOException: Failed to connect to /192.168.1.11:36316\n\tat org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:216)\n\tat org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:167)\n\tat org.apache.spark.network.netty.NettyBlockTransferService$$anon$1.createAndStart(NettyBlockTransferService.scala:90)\n\tat org.apache.spark.network.shuffle.RetryingBlockFetcher.fetchAllOutstanding(RetryingBlockFetcher.java:140)\n\tat org.apache.spark.network.shuffle.RetryingBlockFetcher.access$200(RetryingBlockFetcher.java:43)\n\tat org.apache.spark.network.shuffle.RetryingBlockFetcher$1.run(RetryingBlockFetcher.java:170)\n\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n\tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n\tat java.lang.Thread.run(Thread.java:745)\nCaused by: java.net.NoRouteToHostException: No route to host: /192.168.1.11:36316\n\tat sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)\n\tat sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)\n\tat io.netty.channel.socket.nio.NioSocketChannel.doFinishConnect(NioSocketChannel.java:224)\n\tat io.netty.channel.nio.AbstractNioChannel$AbstractNioUnsafe.finishConnect(AbstractNioChannel.java:289)\n\tat io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:528)\n\tat io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:468)\n\tat io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:382)\n\tat io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:354)\n\tat io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:111)\n\t... 1 more\n"},"Task Info":{"Task ID":27,"Index":1,"Attempt":0,"Launch Time":1467148807904,"Executor ID":"0","Host":"192.168.1.3","Locality":"ANY","Speculative":false,"Getting Result Time":0,"Finish Time":1467148830881,"Failed":true,"Accumulables":[]}}
{"Event":"SparkListenerBlockManagerRemoved","Block Manager ID":{"Executor ID":"5","Host":"192.168.1.11","Port":36316},"Timestamp":1467148830884}
{"Event":"SparkListenerTaskEnd","Stage ID":3,"Stage Attempt ID":2,"Task Type":"ResultTask","Task End Reason":{"Reason":"FetchFailed","Block Manager Address":{"Executor ID":"5","Host":"192.168.1.11","Port":36316},"Shuffle ID":0,"Map ID":0,"Reduce ID":0,"Message":"org.apache.spark.shuffle.FetchFailedException: Failed to connect to /192.168.1.11:36316\n\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.throwFetchFailedException(ShuffleBlockFetcherIterator.scala:323)\n\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:300)\n\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:51)\n\tat scala.collection.Iterator$$anon$11.next(Iterator.scala:328)\n\tat scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)\n\tat org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:32)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:39)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:727)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1157)\n\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:452)\n\tat org.apache.spark.api.python.PythonRunner$WriterThread$$anonfun$run$3.apply(PythonRDD.scala:280)\n\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1765)\n\tat org.apache.spark.api.python.PythonRunner$WriterThread.run(PythonRDD.scala:239)\nCaused by: java.io.IOException: Failed to connect to /192.168.1.11:36316\n\tat org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:216)\n\tat org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:167)\n\tat org.apache.spark.network.netty.NettyBlockTransferService$$anon$1.createAndStart(NettyBlockTransferService.scala:90)\n\tat org.apache.spark.network.shuffle.RetryingBlockFetcher.fetchAllOutstanding(RetryingBlockFetcher.java:140)\n\tat org.apache.spark.network.shuffle.RetryingBlockFetcher.access$200(RetryingBlockFetcher.java:43)\n\tat org.apache.spark.network.shuffle.RetryingBlockFetcher$1.run(RetryingBlockFetcher.java:170)\n\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n\tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n\tat java.lang.Thread.run(Thread.java:745)\nCaused by: java.net.NoRouteToHostException: No route to host: /192.168.1.11:36316\n\tat sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)\n\tat sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)\n\tat io.netty.channel.socket.nio.NioSocketChannel.doFinishConnect(NioSocketChannel.java:224)\n\tat io.netty.channel.nio.AbstractNioChannel$AbstractNioUnsafe.finishConnect(AbstractNioChannel.java:289)\n\tat io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:528)\n\tat io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:468)\n\tat io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:382)\n\tat io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:354)\n\tat io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:111)\n\t... 1 more\n"},"Task Info":{"Task ID":26,"Index":0,"Attempt":0,"Launch Time":1467148807903,"Executor ID":"4","Host":"192.168.1.3","Locality":"ANY","Speculative":false,"Getting Result Time":0,"Finish Time":1467148832317,"Failed":true,"Accumulables":[]}}
{"Event":"SparkListenerBlockManagerAdded","Block Manager ID":{"Executor ID":"0","Host":"192.168.1.11","Port":43480},"Maximum Memory":535953408,"Timestamp":1467148833079}
{"Event":"SparkListenerBlockManagerAdded","Block Manager ID":{"Executor ID":"5","Host":"192.168.1.11","Port":36316},"Maximum Memory":535953408,"Timestamp":1467148833201}
{"Event":"SparkListenerTaskEnd","Stage ID":2,"Stage Attempt ID":4,"Task Type":"ShuffleMapTask","Task End Reason":{"Reason":"Success"},"Task Info":{"Task ID":30,"Index":0,"Attempt":0,"Launch Time":1467148829830,"Executor ID":"7","Host":"192.168.1.3","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":1467148843203,"Failed":false,"Accumulables":[]},"Task Metrics":{"Host Name":"192.168.1.11","Executor Deserialize Time":69,"Executor Run Time":13296,"Result Size":2321,"JVM GC Time":117,"Result Serialization Time":0,"Memory Bytes Spilled":0,"Disk Bytes Spilled":0,"Shuffle Write Metrics":{"Shuffle Bytes Written":53536423,"Shuffle Write Time":273501210,"Shuffle Records Written":148},"Input Metrics":{"Data Read Method":"Hadoop","Bytes Read":65536,"Records Read":3484461}}}
{"Event":"SparkListenerStageCompleted","Stage Info":{"Stage ID":2,"Stage Attempt ID":4,"Stage Name":"sortByKey at /home/daniar/documents/SPARK/spark-1.6.1/sort.py:12","Number of Tasks":1,"RDD Info":[{"RDD ID":5,"Name":"PairwiseRDD","Callsite":"sortByKey at /home/daniar/documents/SPARK/spark-1.6.1/sort.py:12","Parent IDs":[4],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":4,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":4,"Name":"PythonRDD","Callsite":"sortByKey at /home/daniar/documents/SPARK/spark-1.6.1/sort.py:12","Parent IDs":[1],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":4,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":0,"Name":"../generated_file/list_int","Scope":"{\"id\":\"0\",\"name\":\"textFile\"}","Callsite":"textFile at NativeMethodAccessorImpl.java:-2","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":4,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":1,"Name":"../generated_file/list_int","Scope":"{\"id\":\"0\",\"name\":\"textFile\"}","Callsite":"textFile at NativeMethodAccessorImpl.java:-2","Parent IDs":[0],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":4,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.<init>(RDD.scala:98)\norg.apache.spark.api.python.PairwiseRDD.<init>(PythonRDD.scala:338)\nsun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\nsun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\nsun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\njava.lang.reflect.Constructor.newInstance(Constructor.java:423)\npy4j.reflection.MethodInvoker.invoke(MethodInvoker.java:234)\npy4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:381)\npy4j.Gateway.invoke(Gateway.java:214)\npy4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:79)\npy4j.commands.ConstructorCommand.execute(ConstructorCommand.java:68)\npy4j.GatewayConnection.run(GatewayConnection.java:209)\njava.lang.Thread.run(Thread.java:745)","Submission Time":1467148829830,"Completion Time":1467148843203,"Accumulables":[]}}
{"Event":"SparkListenerStageSubmitted","Stage Info":{"Stage ID":2,"Stage Attempt ID":5,"Stage Name":"sortByKey at /home/daniar/documents/SPARK/spark-1.6.1/sort.py:12","Number of Tasks":1,"RDD Info":[{"RDD ID":5,"Name":"PairwiseRDD","Callsite":"sortByKey at /home/daniar/documents/SPARK/spark-1.6.1/sort.py:12","Parent IDs":[4],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":4,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":4,"Name":"PythonRDD","Callsite":"sortByKey at /home/daniar/documents/SPARK/spark-1.6.1/sort.py:12","Parent IDs":[1],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":4,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":0,"Name":"../generated_file/list_int","Scope":"{\"id\":\"0\",\"name\":\"textFile\"}","Callsite":"textFile at NativeMethodAccessorImpl.java:-2","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":4,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":1,"Name":"../generated_file/list_int","Scope":"{\"id\":\"0\",\"name\":\"textFile\"}","Callsite":"textFile at NativeMethodAccessorImpl.java:-2","Parent IDs":[0],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":4,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.<init>(RDD.scala:98)\norg.apache.spark.api.python.PairwiseRDD.<init>(PythonRDD.scala:338)\nsun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\nsun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\nsun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\njava.lang.reflect.Constructor.newInstance(Constructor.java:423)\npy4j.reflection.MethodInvoker.invoke(MethodInvoker.java:234)\npy4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:381)\npy4j.Gateway.invoke(Gateway.java:214)\npy4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:79)\npy4j.commands.ConstructorCommand.execute(ConstructorCommand.java:68)\npy4j.GatewayConnection.run(GatewayConnection.java:209)\njava.lang.Thread.run(Thread.java:745)","Accumulables":[]},"Properties":{"spark.rdd.scope.noOverride":"true","spark.rdd.scope":"{\"id\":\"11\",\"name\":\"saveAsTextFile\"}"}}
{"Event":"SparkListenerTaskStart","Stage ID":2,"Stage Attempt ID":5,"Task Info":{"Task ID":31,"Index":0,"Attempt":0,"Launch Time":1467148843207,"Executor ID":"6","Host":"192.168.1.3","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":0,"Failed":false,"Accumulables":[]}}
{"Event":"SparkListenerTaskEnd","Stage ID":2,"Stage Attempt ID":5,"Task Type":"ShuffleMapTask","Task End Reason":{"Reason":"Success"},"Task Info":{"Task ID":31,"Index":0,"Attempt":0,"Launch Time":1467148843207,"Executor ID":"6","Host":"192.168.1.3","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":1467148860418,"Failed":false,"Accumulables":[]},"Task Metrics":{"Host Name":"192.168.1.11","Executor Deserialize Time":436,"Executor Run Time":16771,"Result Size":2321,"JVM GC Time":88,"Result Serialization Time":0,"Memory Bytes Spilled":0,"Disk Bytes Spilled":0,"Shuffle Write Metrics":{"Shuffle Bytes Written":65356545,"Shuffle Write Time":592928692,"Shuffle Records Written":164},"Input Metrics":{"Data Read Method":"Hadoop","Bytes Read":0,"Records Read":4253453}}}
{"Event":"SparkListenerStageCompleted","Stage Info":{"Stage ID":2,"Stage Attempt ID":5,"Stage Name":"sortByKey at /home/daniar/documents/SPARK/spark-1.6.1/sort.py:12","Number of Tasks":1,"RDD Info":[{"RDD ID":5,"Name":"PairwiseRDD","Callsite":"sortByKey at /home/daniar/documents/SPARK/spark-1.6.1/sort.py:12","Parent IDs":[4],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":4,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":4,"Name":"PythonRDD","Callsite":"sortByKey at /home/daniar/documents/SPARK/spark-1.6.1/sort.py:12","Parent IDs":[1],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":4,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":0,"Name":"../generated_file/list_int","Scope":"{\"id\":\"0\",\"name\":\"textFile\"}","Callsite":"textFile at NativeMethodAccessorImpl.java:-2","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":4,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":1,"Name":"../generated_file/list_int","Scope":"{\"id\":\"0\",\"name\":\"textFile\"}","Callsite":"textFile at NativeMethodAccessorImpl.java:-2","Parent IDs":[0],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":4,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.<init>(RDD.scala:98)\norg.apache.spark.api.python.PairwiseRDD.<init>(PythonRDD.scala:338)\nsun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\nsun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\nsun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\njava.lang.reflect.Constructor.newInstance(Constructor.java:423)\npy4j.reflection.MethodInvoker.invoke(MethodInvoker.java:234)\npy4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:381)\npy4j.Gateway.invoke(Gateway.java:214)\npy4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:79)\npy4j.commands.ConstructorCommand.execute(ConstructorCommand.java:68)\npy4j.GatewayConnection.run(GatewayConnection.java:209)\njava.lang.Thread.run(Thread.java:745)","Submission Time":1467148843207,"Completion Time":1467148860421,"Accumulables":[]}}
{"Event":"SparkListenerStageSubmitted","Stage Info":{"Stage ID":3,"Stage Attempt ID":3,"Stage Name":"saveAsTextFile at NativeMethodAccessorImpl.java:-2","Number of Tasks":4,"RDD Info":[{"RDD ID":10,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"11\",\"name\":\"saveAsTextFile\"}","Callsite":"saveAsTextFile at NativeMethodAccessorImpl.java:-2","Parent IDs":[9],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":4,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":7,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"9\",\"name\":\"mapPartitions\"}","Callsite":"mapPartitions at PythonRDD.scala:374","Parent IDs":[6],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":4,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":6,"Name":"ShuffledRDD","Scope":"{\"id\":\"8\",\"name\":\"partitionBy\"}","Callsite":"partitionBy at NativeMethodAccessorImpl.java:-2","Parent IDs":[5],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":4,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":8,"Name":"PythonRDD","Callsite":"RDD at PythonRDD.scala:43","Parent IDs":[7],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":4,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":9,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"10\",\"name\":\"map\"}","Callsite":"map at NativeMethodAccessorImpl.java:-2","Parent IDs":[8],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":4,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[2],"Details":"org.apache.spark.api.java.AbstractJavaRDDLike.saveAsTextFile(JavaRDDLike.scala:46)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:498)\npy4j.reflection.MethodInvoker.invoke(MethodInvoker.java:231)\npy4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:381)\npy4j.Gateway.invoke(Gateway.java:259)\npy4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:133)\npy4j.commands.CallCommand.execute(CallCommand.java:79)\npy4j.GatewayConnection.run(GatewayConnection.java:209)\njava.lang.Thread.run(Thread.java:745)","Accumulables":[]},"Properties":{"spark.rdd.scope.noOverride":"true","spark.rdd.scope":"{\"id\":\"11\",\"name\":\"saveAsTextFile\"}"}}
{"Event":"SparkListenerTaskStart","Stage ID":3,"Stage Attempt ID":3,"Task Info":{"Task ID":32,"Index":0,"Attempt":0,"Launch Time":1467148860449,"Executor ID":"1","Host":"192.168.1.3","Locality":"ANY","Speculative":false,"Getting Result Time":0,"Finish Time":0,"Failed":false,"Accumulables":[]}}
{"Event":"SparkListenerTaskStart","Stage ID":3,"Stage Attempt ID":3,"Task Info":{"Task ID":33,"Index":1,"Attempt":0,"Launch Time":1467148860449,"Executor ID":"6","Host":"192.168.1.3","Locality":"ANY","Speculative":false,"Getting Result Time":0,"Finish Time":0,"Failed":false,"Accumulables":[]}}
{"Event":"SparkListenerTaskStart","Stage ID":3,"Stage Attempt ID":3,"Task Info":{"Task ID":34,"Index":2,"Attempt":0,"Launch Time":1467148860450,"Executor ID":"2","Host":"192.168.1.3","Locality":"ANY","Speculative":false,"Getting Result Time":0,"Finish Time":0,"Failed":false,"Accumulables":[]}}
{"Event":"SparkListenerTaskStart","Stage ID":3,"Stage Attempt ID":3,"Task Info":{"Task ID":35,"Index":3,"Attempt":0,"Launch Time":1467148860450,"Executor ID":"4","Host":"192.168.1.3","Locality":"ANY","Speculative":false,"Getting Result Time":0,"Finish Time":0,"Failed":false,"Accumulables":[]}}
{"Event":"SparkListenerTaskEnd","Stage ID":3,"Stage Attempt ID":3,"Task Type":"ResultTask","Task End Reason":{"Reason":"FetchFailed","Block Manager Address":{"Executor ID":"7","Host":"192.168.1.11","Port":50553},"Shuffle ID":0,"Map ID":3,"Reduce ID":3,"Message":"org.apache.spark.shuffle.FetchFailedException: Failed to connect to /192.168.1.11:50553\n\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.throwFetchFailedException(ShuffleBlockFetcherIterator.scala:323)\n\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:300)\n\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:51)\n\tat scala.collection.Iterator$$anon$11.next(Iterator.scala:328)\n\tat scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)\n\tat org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:32)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:39)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:727)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1157)\n\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:452)\n\tat org.apache.spark.api.python.PythonRunner$WriterThread$$anonfun$run$3.apply(PythonRDD.scala:280)\n\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1765)\n\tat org.apache.spark.api.python.PythonRunner$WriterThread.run(PythonRDD.scala:239)\nCaused by: java.io.IOException: Failed to connect to /192.168.1.11:50553\n\tat org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:216)\n\tat org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:167)\n\tat org.apache.spark.network.netty.NettyBlockTransferService$$anon$1.createAndStart(NettyBlockTransferService.scala:90)\n\tat org.apache.spark.network.shuffle.RetryingBlockFetcher.fetchAllOutstanding(RetryingBlockFetcher.java:140)\n\tat org.apache.spark.network.shuffle.RetryingBlockFetcher.access$200(RetryingBlockFetcher.java:43)\n\tat org.apache.spark.network.shuffle.RetryingBlockFetcher$1.run(RetryingBlockFetcher.java:170)\n\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n\tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n\tat java.lang.Thread.run(Thread.java:745)\nCaused by: java.net.NoRouteToHostException: No route to host: /192.168.1.11:50553\n\tat sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)\n\tat sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)\n\tat io.netty.channel.socket.nio.NioSocketChannel.doFinishConnect(NioSocketChannel.java:224)\n\tat io.netty.channel.nio.AbstractNioChannel$AbstractNioUnsafe.finishConnect(AbstractNioChannel.java:289)\n\tat io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:528)\n\tat io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:468)\n\tat io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:382)\n\tat io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:354)\n\tat io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:111)\n\t... 1 more\n"},"Task Info":{"Task ID":35,"Index":3,"Attempt":0,"Launch Time":1467148860450,"Executor ID":"4","Host":"192.168.1.3","Locality":"ANY","Speculative":false,"Getting Result Time":0,"Finish Time":1467148882791,"Failed":true,"Accumulables":[]}}
{"Event":"SparkListenerStageCompleted","Stage Info":{"Stage ID":3,"Stage Attempt ID":3,"Stage Name":"saveAsTextFile at NativeMethodAccessorImpl.java:-2","Number of Tasks":4,"RDD Info":[{"RDD ID":10,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"11\",\"name\":\"saveAsTextFile\"}","Callsite":"saveAsTextFile at NativeMethodAccessorImpl.java:-2","Parent IDs":[9],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":4,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":7,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"9\",\"name\":\"mapPartitions\"}","Callsite":"mapPartitions at PythonRDD.scala:374","Parent IDs":[6],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":4,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":6,"Name":"ShuffledRDD","Scope":"{\"id\":\"8\",\"name\":\"partitionBy\"}","Callsite":"partitionBy at NativeMethodAccessorImpl.java:-2","Parent IDs":[5],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":4,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":8,"Name":"PythonRDD","Callsite":"RDD at PythonRDD.scala:43","Parent IDs":[7],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":4,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":9,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"10\",\"name\":\"map\"}","Callsite":"map at NativeMethodAccessorImpl.java:-2","Parent IDs":[8],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":4,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[2],"Details":"org.apache.spark.api.java.AbstractJavaRDDLike.saveAsTextFile(JavaRDDLike.scala:46)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:498)\npy4j.reflection.MethodInvoker.invoke(MethodInvoker.java:231)\npy4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:381)\npy4j.Gateway.invoke(Gateway.java:259)\npy4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:133)\npy4j.commands.CallCommand.execute(CallCommand.java:79)\npy4j.GatewayConnection.run(GatewayConnection.java:209)\njava.lang.Thread.run(Thread.java:745)","Submission Time":1467148860449,"Completion Time":1467148882792,"Failure Reason":"org.apache.spark.shuffle.FetchFailedException: Failed to connect to /192.168.1.11:50553\n\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.throwFetchFailedException(ShuffleBlockFetcherIterator.scala:323)\n\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:300)\n\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:51)\n\tat scala.collection.Iterator$$anon$11.next(Iterator.scala:328)\n\tat scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)\n\tat org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:32)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:39)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:727)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1157)\n\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:452)\n\tat org.apache.spark.api.python.PythonRunner$WriterThread$$anonfun$run$3.apply(PythonRDD.scala:280)\n\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1765)\n\tat org.apache.spark.api.python.PythonRunner$WriterThread.run(PythonRDD.scala:239)\nCaused by: java.io.IOException: Failed to connect to /192.168.1.11:50553\n\tat org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:216)\n\tat org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:167)\n\tat org.apache.spark.network.netty.NettyBlockTransferService$$anon$1.createAndStart(NettyBlockTransferService.scala:90)\n\tat org.apache.spark.network.shuffle.RetryingBlockFetcher.fetchAllOutstanding(RetryingBlockFetcher.java:140)\n\tat org.apache.spark.network.shuffle.RetryingBlockFetcher.access$200(RetryingBlockFetcher.java:43)\n\tat org.apache.spark.network.shuffle.RetryingBlockFetcher$1.run(RetryingBlockFetcher.java:170)\n\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n\tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n\tat java.lang.Thread.run(Thread.java:745)\nCaused by: java.net.NoRouteToHostException: No route to host: /192.168.1.11:50553\n\tat sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)\n\tat sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)\n\tat io.netty.channel.socket.nio.NioSocketChannel.doFinishConnect(NioSocketChannel.java:224)\n\tat io.netty.channel.nio.AbstractNioChannel$AbstractNioUnsafe.finishConnect(AbstractNioChannel.java:289)\n\tat io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:528)\n\tat io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:468)\n\tat io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:382)\n\tat io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:354)\n\tat io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:111)\n\t... 1 more\n","Accumulables":[]}}
{"Event":"SparkListenerJobEnd","Job ID":2,"Completion Time":1467148882879,"Job Result":{"Result":"JobFailed","Exception":{"Message":"Job aborted due to stage failure: ResultStage 3 (saveAsTextFile at NativeMethodAccessorImpl.java:-2) has failed the maximum allowable number of times: 4. Most recent failure reason: org.apache.spark.shuffle.FetchFailedException: Failed to connect to /192.168.1.11:50553\n\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.throwFetchFailedException(ShuffleBlockFetcherIterator.scala:323)\n\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:300)\n\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:51)\n\tat scala.collection.Iterator$$anon$11.next(Iterator.scala:328)\n\tat scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)\n\tat org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:32)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:39)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:727)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1157)\n\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:452)\n\tat org.apache.spark.api.python.PythonRunner$WriterThread$$anonfun$run$3.apply(PythonRDD.scala:280)\n\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1765)\n\tat org.apache.spark.api.python.PythonRunner$WriterThread.run(PythonRDD.scala:239)\nCaused by: java.io.IOException: Failed to connect to /192.168.1.11:50553\n\tat org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:216)\n\tat org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:167)\n\tat org.apache.spark.network.netty.NettyBlockTransferService$$anon$1.createAndStart(NettyBlockTransferService.scala:90)\n\tat org.apache.spark.network.shuffle.RetryingBlockFetcher.fetchAllOutstanding(RetryingBlockFetcher.java:140)\n\tat org.apache.spark.network.shuffle.RetryingBlockFetcher.access$200(RetryingBlockFetcher.java:43)\n\tat org.apache.spark.network.shuffle.RetryingBlockFetcher$1.run(RetryingBlockFetcher.java:170)\n\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n\tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n\tat java.lang.Thread.run(Thread.java:745)\nCaused by: java.net.NoRouteToHostException: No route to host: /192.168.1.11:50553\n\tat sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)\n\tat sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)\n\tat io.netty.channel.socket.nio.NioSocketChannel.doFinishConnect(NioSocketChannel.java:224)\n\tat io.netty.channel.nio.AbstractNioChannel$AbstractNioUnsafe.finishConnect(AbstractNioChannel.java:289)\n\tat io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:528)\n\tat io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:468)\n\tat io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:382)\n\tat io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:354)\n\tat io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:111)\n\t... 1 more\n","Stack Trace":[{"Declaring Class":"org.apache.spark.scheduler.DAGScheduler","Method Name":"org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages","File Name":"DAGScheduler.scala","Line Number":1431},{"Declaring Class":"org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1","Method Name":"apply","File Name":"DAGScheduler.scala","Line Number":1419},{"Declaring Class":"org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1","Method Name":"apply","File Name":"DAGScheduler.scala","Line Number":1418},{"Declaring Class":"scala.collection.mutable.ResizableArray$class","Method Name":"foreach","File Name":"ResizableArray.scala","Line Number":59},{"Declaring Class":"scala.collection.mutable.ArrayBuffer","Method Name":"foreach","File Name":"ArrayBuffer.scala","Line Number":47},{"Declaring Class":"org.apache.spark.scheduler.DAGScheduler","Method Name":"abortStage","File Name":"DAGScheduler.scala","Line Number":1418},{"Declaring Class":"org.apache.spark.scheduler.DAGScheduler","Method Name":"handleTaskCompletion","File Name":"DAGScheduler.scala","Line Number":1258},{"Declaring Class":"org.apache.spark.scheduler.DAGSchedulerEventProcessLoop","Method Name":"doOnReceive","File Name":"DAGScheduler.scala","Line Number":1637},{"Declaring Class":"org.apache.spark.scheduler.DAGSchedulerEventProcessLoop","Method Name":"onReceive","File Name":"DAGScheduler.scala","Line Number":1599},{"Declaring Class":"org.apache.spark.scheduler.DAGSchedulerEventProcessLoop","Method Name":"onReceive","File Name":"DAGScheduler.scala","Line Number":1588},{"Declaring Class":"org.apache.spark.util.EventLoop$$anon$1","Method Name":"run","File Name":"EventLoop.scala","Line Number":48}]}}}
{"Event":"SparkListenerBlockManagerRemoved","Block Manager ID":{"Executor ID":"7","Host":"192.168.1.11","Port":50553},"Timestamp":1467148882880}
{"Event":"SparkListenerBlockManagerAdded","Block Manager ID":{"Executor ID":"7","Host":"192.168.1.11","Port":50553},"Maximum Memory":535953408,"Timestamp":1467148882930}
{"Event":"SparkListenerTaskEnd","Stage ID":3,"Stage Attempt ID":3,"Task Type":"ResultTask","Task End Reason":{"Reason":"FetchFailed","Block Manager Address":{"Executor ID":"7","Host":"192.168.1.11","Port":50553},"Shuffle ID":0,"Map ID":3,"Reduce ID":1,"Message":"org.apache.spark.shuffle.FetchFailedException: Failed to connect to /192.168.1.11:50553\n\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.throwFetchFailedException(ShuffleBlockFetcherIterator.scala:323)\n\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:300)\n\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:51)\n\tat scala.collection.Iterator$$anon$11.next(Iterator.scala:328)\n\tat scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)\n\tat org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:32)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:39)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:727)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1157)\n\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:452)\n\tat org.apache.spark.api.python.PythonRunner$WriterThread$$anonfun$run$3.apply(PythonRDD.scala:280)\n\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1765)\n\tat org.apache.spark.api.python.PythonRunner$WriterThread.run(PythonRDD.scala:239)\nCaused by: java.io.IOException: Failed to connect to /192.168.1.11:50553\n\tat org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:216)\n\tat org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:167)\n\tat org.apache.spark.network.netty.NettyBlockTransferService$$anon$1.createAndStart(NettyBlockTransferService.scala:90)\n\tat org.apache.spark.network.shuffle.RetryingBlockFetcher.fetchAllOutstanding(RetryingBlockFetcher.java:140)\n\tat org.apache.spark.network.shuffle.RetryingBlockFetcher.access$200(RetryingBlockFetcher.java:43)\n\tat org.apache.spark.network.shuffle.RetryingBlockFetcher$1.run(RetryingBlockFetcher.java:170)\n\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n\tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n\tat java.lang.Thread.run(Thread.java:745)\nCaused by: java.net.NoRouteToHostException: No route to host: /192.168.1.11:50553\n\tat sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)\n\tat sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)\n\tat io.netty.channel.socket.nio.NioSocketChannel.doFinishConnect(NioSocketChannel.java:224)\n\tat io.netty.channel.nio.AbstractNioChannel$AbstractNioUnsafe.finishConnect(AbstractNioChannel.java:289)\n\tat io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:528)\n\tat io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:468)\n\tat io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:382)\n\tat io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:354)\n\tat io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:111)\n\t... 1 more\n"},"Task Info":{"Task ID":33,"Index":1,"Attempt":0,"Launch Time":1467148860449,"Executor ID":"6","Host":"192.168.1.3","Locality":"ANY","Speculative":false,"Getting Result Time":0,"Finish Time":1467148883062,"Failed":true,"Accumulables":[]}}
{"Event":"SparkListenerApplicationEnd","Timestamp":1467148883323}
