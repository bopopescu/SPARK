{"Event":"SparkListenerLogStart","Spark Version":"1.6.1"}
{"Event":"SparkListenerBlockManagerAdded","Block Manager ID":{"Executor ID":"driver","Host":"192.168.1.3","Port":36920},"Maximum Memory":535953408,"Timestamp":1467147004484}
{"Event":"SparkListenerEnvironmentUpdate","JVM Information":{"Java Home":"/usr/lib/jvm/java-8-oracle/jre","Java Version":"1.8.0_91 (Oracle Corporation)","Scala Version":"version 2.10.5"},"Spark Properties":{"spark.speculation":"true","spark.driver.host":"192.168.1.3","spark.serializer.objectStreamReset":"100","spark.eventLog.enabled":"true","spark.driver.maxResultSize":"1g","spark.driver.port":"50479","spark.rdd.compress":"True","spark.app.name":"Sorting","spark.scheduler.mode":"FIFO","spark.driver.memory":"1g","spark.executor.instances":"2","spark.files":"file:/home/daniar/documents/SPARK/spark-1.6.1/sort.py","spark.executor.id":"driver","spark.submit.deployMode":"client","spark.master":"spark://daniar-X450JF:7077","spark.executor.memory":"1g","spark.eventLog.dir":"../generated_driver_log/","spark.executor.cores":"2","spark.externalBlockStore.folderName":"spark-b0992821-1620-4eba-b61d-4e0d0cd2f97c","spark.app.id":"app-20160629035001-0002"},"System Properties":{"java.io.tmpdir":"/tmp","line.separator":"\n","path.separator":":","sun.management.compiler":"HotSpot 64-Bit Tiered Compilers","SPARK_SUBMIT":"true","sun.cpu.endian":"little","java.specification.version":"1.8","java.vm.specification.name":"Java Virtual Machine Specification","java.vendor":"Oracle Corporation","java.vm.specification.version":"1.8","user.home":"/home/daniar","file.encoding.pkg":"sun.io","sun.nio.ch.bugLevel":"","sun.arch.data.model":"64","sun.boot.library.path":"/usr/lib/jvm/java-8-oracle/jre/lib/amd64","user.dir":"/home/daniar/documents/SPARK/spark-1.6.1","java.library.path":"/usr/java/packages/lib/amd64:/usr/lib64:/lib64:/lib:/usr/lib","sun.cpu.isalist":"","sun.desktop":"gnome","os.arch":"amd64","java.vm.version":"25.91-b14","java.endorsed.dirs":"/usr/lib/jvm/java-8-oracle/jre/lib/endorsed","java.runtime.version":"1.8.0_91-b14","java.vm.info":"mixed mode","java.ext.dirs":"/usr/lib/jvm/java-8-oracle/jre/lib/ext:/usr/java/packages/lib/ext","java.runtime.name":"Java(TM) SE Runtime Environment","file.separator":"/","java.class.version":"52.0","java.specification.name":"Java Platform API Specification","sun.boot.class.path":"/usr/lib/jvm/java-8-oracle/jre/lib/resources.jar:/usr/lib/jvm/java-8-oracle/jre/lib/rt.jar:/usr/lib/jvm/java-8-oracle/jre/lib/sunrsasign.jar:/usr/lib/jvm/java-8-oracle/jre/lib/jsse.jar:/usr/lib/jvm/java-8-oracle/jre/lib/jce.jar:/usr/lib/jvm/java-8-oracle/jre/lib/charsets.jar:/usr/lib/jvm/java-8-oracle/jre/lib/jfr.jar:/usr/lib/jvm/java-8-oracle/jre/classes","file.encoding":"UTF-8","user.timezone":"Asia/Jakarta","java.specification.vendor":"Oracle Corporation","sun.java.launcher":"SUN_STANDARD","os.version":"3.19.0-51-generic","sun.os.patch.level":"unknown","java.vm.specification.vendor":"Oracle Corporation","user.country":"US","sun.jnu.encoding":"UTF-8","user.language":"en","java.vendor.url":"http://java.oracle.com/","java.awt.printerjob":"sun.print.PSPrinterJob","java.awt.graphicsenv":"sun.awt.X11GraphicsEnvironment","awt.toolkit":"sun.awt.X11.XToolkit","os.name":"Linux","java.vm.vendor":"Oracle Corporation","java.vendor.url.bug":"http://bugreport.sun.com/bugreport/","user.name":"daniar","java.vm.name":"Java HotSpot(TM) 64-Bit Server VM","sun.java.command":"org.apache.spark.deploy.SparkSubmit sort.py --master spark://daniar-X450JF:7077 --deploy-mode cluster","java.home":"/usr/lib/jvm/java-8-oracle/jre","java.version":"1.8.0_91","sun.io.unicode.encoding":"UnicodeLittle"},"Classpath Entries":{"/home/daniar/documents/SPARK/spark-1.6.1/tools/target/scala-2.10/classes/":"System Classpath","/home/daniar/documents/SPARK/spark-1.6.1/launcher/target/scala-2.10/classes/":"System Classpath","/home/daniar/documents/SPARK/spark-1.6.1/lib_managed/jars/datanucleus-rdbms-3.2.9.jar":"System Classpath","/home/daniar/documents/SPARK/spark-1.6.1/network/shuffle/target/scala-2.10/classes/":"System Classpath","/home/daniar/documents/SPARK/spark-1.6.1/core/target/scala-2.10/classes/":"System Classpath","/home/daniar/documents/SPARK/spark-1.6.1/core/target/jars/*":"System Classpath","/home/daniar/documents/SPARK/spark-1.6.1/streaming/target/scala-2.10/classes/":"System Classpath","/home/daniar/documents/SPARK/spark-1.6.1/assembly/target/scala-2.10/spark-assembly-1.6.1-hadoop2.7.2.jar":"System Classpath","http://192.168.1.3:36925/files/sort.py":"Added By User","/home/daniar/documents/SPARK/spark-1.6.1/graphx/target/scala-2.10/classes/":"System Classpath","/home/daniar/documents/SPARK/spark-1.6.1/bagel/target/scala-2.10/classes/":"System Classpath","/home/daniar/documents/SPARK/spark-1.6.1/mllib/target/scala-2.10/classes/":"System Classpath","/home/daniar/documents/SPARK/spark-1.6.1/sql/hive-thriftserver/target/scala-2.10/classes":"System Classpath","/home/daniar/documents/SPARK/spark-1.6.1/lib_managed/jars/datanucleus-core-3.2.10.jar":"System Classpath","/home/daniar/documents/SPARK/spark-1.6.1/yarn/target/scala-2.10/classes/":"System Classpath","/home/daniar/documents/SPARK/spark-1.6.1/network/common/target/scala-2.10/classes/":"System Classpath","/home/daniar/documents/SPARK/spark-1.6.1/network/yarn/target/scala-2.10/classes/":"System Classpath","/home/daniar/documents/SPARK/spark-1.6.1/repl/target/scala-2.10/classes/":"System Classpath","/home/daniar/documents/SPARK/spark-1.6.1/sql/core/target/scala-2.10/classes/":"System Classpath","/home/daniar/documents/SPARK/spark-1.6.1/lib_managed/jars/datanucleus-api-jdo-3.2.6.jar":"System Classpath","/home/daniar/documents/SPARK/spark-1.6.1/conf/":"System Classpath","/home/daniar/documents/SPARK/spark-1.6.1/sql/hive/target/scala-2.10/classes/":"System Classpath","/home/daniar/documents/SPARK/spark-1.6.1/sql/catalyst/target/scala-2.10/classes/":"System Classpath"}}
{"Event":"SparkListenerApplicationStart","App Name":"Sorting","App ID":"app-20160629035001-0002","Timestamp":1467146966570,"User":"daniar"}
{"Event":"SparkListenerExecutorAdded","Timestamp":1467147013803,"Executor ID":"7","Executor Info":{"Host":"192.168.1.3","Total Cores":2,"Log Urls":{"stdout":"http://192.168.1.3:8081/logPage/?appId=app-20160629035001-0002&executorId=7&logType=stdout","stderr":"http://192.168.1.3:8081/logPage/?appId=app-20160629035001-0002&executorId=7&logType=stderr"}}}
{"Event":"SparkListenerBlockManagerAdded","Block Manager ID":{"Executor ID":"7","Host":"192.168.1.11","Port":49459},"Maximum Memory":535953408,"Timestamp":1467147014413}
{"Event":"SparkListenerExecutorAdded","Timestamp":1467147014528,"Executor ID":"4","Executor Info":{"Host":"192.168.1.3","Total Cores":2,"Log Urls":{"stdout":"http://192.168.1.3:8081/logPage/?appId=app-20160629035001-0002&executorId=4&logType=stdout","stderr":"http://192.168.1.3:8081/logPage/?appId=app-20160629035001-0002&executorId=4&logType=stderr"}}}
{"Event":"SparkListenerExecutorAdded","Timestamp":1467147014654,"Executor ID":"5","Executor Info":{"Host":"192.168.1.3","Total Cores":2,"Log Urls":{"stdout":"http://192.168.1.3:8081/logPage/?appId=app-20160629035001-0002&executorId=5&logType=stdout","stderr":"http://192.168.1.3:8081/logPage/?appId=app-20160629035001-0002&executorId=5&logType=stderr"}}}
{"Event":"SparkListenerBlockManagerAdded","Block Manager ID":{"Executor ID":"4","Host":"192.168.1.11","Port":37967},"Maximum Memory":535953408,"Timestamp":1467147014814}
{"Event":"SparkListenerBlockManagerAdded","Block Manager ID":{"Executor ID":"5","Host":"192.168.1.11","Port":40021},"Maximum Memory":535953408,"Timestamp":1467147014887}
{"Event":"SparkListenerExecutorAdded","Timestamp":1467147015735,"Executor ID":"6","Executor Info":{"Host":"192.168.1.3","Total Cores":2,"Log Urls":{"stdout":"http://192.168.1.3:8081/logPage/?appId=app-20160629035001-0002&executorId=6&logType=stdout","stderr":"http://192.168.1.3:8081/logPage/?appId=app-20160629035001-0002&executorId=6&logType=stderr"}}}
{"Event":"SparkListenerBlockManagerAdded","Block Manager ID":{"Executor ID":"6","Host":"192.168.1.11","Port":56770},"Maximum Memory":535953408,"Timestamp":1467147016171}
{"Event":"SparkListenerJobStart","Job ID":0,"Submission Time":1467147017092,"Stage Infos":[{"Stage ID":0,"Stage Attempt ID":0,"Stage Name":"sortByKey at /home/daniar/documents/SPARK/spark-1.6.1/sort.py:12","Number of Tasks":4,"RDD Info":[{"RDD ID":2,"Name":"PythonRDD","Callsite":"sortByKey at /home/daniar/documents/SPARK/spark-1.6.1/sort.py:12","Parent IDs":[1],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":4,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":0,"Name":"../generated_file/list_int","Scope":"{\"id\":\"0\",\"name\":\"textFile\"}","Callsite":"textFile at NativeMethodAccessorImpl.java:-2","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":4,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":1,"Name":"../generated_file/list_int","Scope":"{\"id\":\"0\",\"name\":\"textFile\"}","Callsite":"textFile at NativeMethodAccessorImpl.java:-2","Parent IDs":[0],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":4,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.collect(RDD.scala:926)\norg.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:405)\norg.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:498)\npy4j.reflection.MethodInvoker.invoke(MethodInvoker.java:231)\npy4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:381)\npy4j.Gateway.invoke(Gateway.java:259)\npy4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:133)\npy4j.commands.CallCommand.execute(CallCommand.java:79)\npy4j.GatewayConnection.run(GatewayConnection.java:209)\njava.lang.Thread.run(Thread.java:745)","Accumulables":[]}],"Stage IDs":[0],"Properties":{"spark.rdd.scope.noOverride":"true","spark.rdd.scope":"{\"id\":\"1\",\"name\":\"collect\"}","callSite.short":"sortByKey at /home/daniar/documents/SPARK/spark-1.6.1/sort.py:12"}}
{"Event":"SparkListenerStageSubmitted","Stage Info":{"Stage ID":0,"Stage Attempt ID":0,"Stage Name":"sortByKey at /home/daniar/documents/SPARK/spark-1.6.1/sort.py:12","Number of Tasks":4,"RDD Info":[{"RDD ID":2,"Name":"PythonRDD","Callsite":"sortByKey at /home/daniar/documents/SPARK/spark-1.6.1/sort.py:12","Parent IDs":[1],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":4,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":0,"Name":"../generated_file/list_int","Scope":"{\"id\":\"0\",\"name\":\"textFile\"}","Callsite":"textFile at NativeMethodAccessorImpl.java:-2","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":4,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":1,"Name":"../generated_file/list_int","Scope":"{\"id\":\"0\",\"name\":\"textFile\"}","Callsite":"textFile at NativeMethodAccessorImpl.java:-2","Parent IDs":[0],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":4,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.collect(RDD.scala:926)\norg.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:405)\norg.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:498)\npy4j.reflection.MethodInvoker.invoke(MethodInvoker.java:231)\npy4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:381)\npy4j.Gateway.invoke(Gateway.java:259)\npy4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:133)\npy4j.commands.CallCommand.execute(CallCommand.java:79)\npy4j.GatewayConnection.run(GatewayConnection.java:209)\njava.lang.Thread.run(Thread.java:745)","Submission Time":1467147018303,"Accumulables":[]},"Properties":{"spark.rdd.scope.noOverride":"true","spark.rdd.scope":"{\"id\":\"1\",\"name\":\"collect\"}","callSite.short":"sortByKey at /home/daniar/documents/SPARK/spark-1.6.1/sort.py:12"}}
{"Event":"SparkListenerExecutorAdded","Timestamp":1467147017840,"Executor ID":"0","Executor Info":{"Host":"192.168.1.3","Total Cores":2,"Log Urls":{"stdout":"http://192.168.1.3:8082/logPage/?appId=app-20160629035001-0002&executorId=0&logType=stdout","stderr":"http://192.168.1.3:8082/logPage/?appId=app-20160629035001-0002&executorId=0&logType=stderr"}}}
{"Event":"SparkListenerExecutorAdded","Timestamp":1467147017880,"Executor ID":"1","Executor Info":{"Host":"192.168.1.3","Total Cores":2,"Log Urls":{"stdout":"http://192.168.1.3:8082/logPage/?appId=app-20160629035001-0002&executorId=1&logType=stdout","stderr":"http://192.168.1.3:8082/logPage/?appId=app-20160629035001-0002&executorId=1&logType=stderr"}}}
{"Event":"SparkListenerBlockManagerAdded","Block Manager ID":{"Executor ID":"1","Host":"192.168.1.11","Port":39473},"Maximum Memory":535953408,"Timestamp":1467147018119}
{"Event":"SparkListenerBlockManagerAdded","Block Manager ID":{"Executor ID":"0","Host":"192.168.1.11","Port":41011},"Maximum Memory":535953408,"Timestamp":1467147018217}
{"Event":"SparkListenerTaskStart","Stage ID":0,"Stage Attempt ID":0,"Task Info":{"Task ID":0,"Index":0,"Attempt":0,"Launch Time":1467147018362,"Executor ID":"6","Host":"192.168.1.3","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":0,"Failed":false,"Accumulables":[]}}
{"Event":"SparkListenerTaskStart","Stage ID":0,"Stage Attempt ID":0,"Task Info":{"Task ID":1,"Index":1,"Attempt":0,"Launch Time":1467147018714,"Executor ID":"7","Host":"192.168.1.3","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":0,"Failed":false,"Accumulables":[]}}
{"Event":"SparkListenerTaskStart","Stage ID":0,"Stage Attempt ID":0,"Task Info":{"Task ID":2,"Index":2,"Attempt":0,"Launch Time":1467147018715,"Executor ID":"5","Host":"192.168.1.3","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":0,"Failed":false,"Accumulables":[]}}
{"Event":"SparkListenerTaskStart","Stage ID":0,"Stage Attempt ID":0,"Task Info":{"Task ID":3,"Index":3,"Attempt":0,"Launch Time":1467147018716,"Executor ID":"0","Host":"192.168.1.3","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":0,"Failed":false,"Accumulables":[]}}
{"Event":"SparkListenerExecutorAdded","Timestamp":1467147019086,"Executor ID":"3","Executor Info":{"Host":"192.168.1.3","Total Cores":2,"Log Urls":{"stdout":"http://192.168.1.3:8082/logPage/?appId=app-20160629035001-0002&executorId=3&logType=stdout","stderr":"http://192.168.1.3:8082/logPage/?appId=app-20160629035001-0002&executorId=3&logType=stderr"}}}
{"Event":"SparkListenerExecutorAdded","Timestamp":1467147019167,"Executor ID":"2","Executor Info":{"Host":"192.168.1.3","Total Cores":2,"Log Urls":{"stdout":"http://192.168.1.3:8082/logPage/?appId=app-20160629035001-0002&executorId=2&logType=stdout","stderr":"http://192.168.1.3:8082/logPage/?appId=app-20160629035001-0002&executorId=2&logType=stderr"}}}
{"Event":"SparkListenerBlockManagerAdded","Block Manager ID":{"Executor ID":"3","Host":"192.168.1.11","Port":60009},"Maximum Memory":535953408,"Timestamp":1467147019351}
{"Event":"SparkListenerBlockManagerAdded","Block Manager ID":{"Executor ID":"2","Host":"192.168.1.11","Port":35327},"Maximum Memory":535953408,"Timestamp":1467147019483}
{"Event":"SparkListenerTaskEnd","Stage ID":0,"Stage Attempt ID":0,"Task Type":"ResultTask","Task End Reason":{"Reason":"Success"},"Task Info":{"Task ID":3,"Index":3,"Attempt":0,"Launch Time":1467147018716,"Executor ID":"0","Host":"192.168.1.3","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":1467147045579,"Failed":false,"Accumulables":[]},"Task Metrics":{"Host Name":"192.168.1.11","Executor Deserialize Time":3165,"Executor Run Time":23145,"Result Size":2130,"JVM GC Time":431,"Result Serialization Time":1,"Memory Bytes Spilled":0,"Disk Bytes Spilled":0,"Input Metrics":{"Data Read Method":"Hadoop","Bytes Read":65536,"Records Read":3484461}}}
{"Event":"SparkListenerTaskEnd","Stage ID":0,"Stage Attempt ID":0,"Task Type":"ResultTask","Task End Reason":{"Reason":"Success"},"Task Info":{"Task ID":0,"Index":0,"Attempt":0,"Launch Time":1467147018362,"Executor ID":"6","Host":"192.168.1.3","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":1467147046218,"Failed":false,"Accumulables":[]},"Task Metrics":{"Host Name":"192.168.1.11","Executor Deserialize Time":3144,"Executor Run Time":24112,"Result Size":2130,"JVM GC Time":478,"Result Serialization Time":0,"Memory Bytes Spilled":0,"Disk Bytes Spilled":0,"Input Metrics":{"Data Read Method":"Hadoop","Bytes Read":0,"Records Read":4253453}}}
{"Event":"SparkListenerTaskEnd","Stage ID":0,"Stage Attempt ID":0,"Task Type":"ResultTask","Task End Reason":{"Reason":"Success"},"Task Info":{"Task ID":1,"Index":1,"Attempt":0,"Launch Time":1467147018714,"Executor ID":"7","Host":"192.168.1.3","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":1467147047351,"Failed":false,"Accumulables":[]},"Task Metrics":{"Host Name":"192.168.1.11","Executor Deserialize Time":3135,"Executor Run Time":25244,"Result Size":2130,"JVM GC Time":488,"Result Serialization Time":1,"Memory Bytes Spilled":0,"Disk Bytes Spilled":0,"Input Metrics":{"Data Read Method":"Hadoop","Bytes Read":65536,"Records Read":4253268}}}
{"Event":"SparkListenerTaskEnd","Stage ID":0,"Stage Attempt ID":0,"Task Type":"ResultTask","Task End Reason":{"Reason":"Success"},"Task Info":{"Task ID":2,"Index":2,"Attempt":0,"Launch Time":1467147018715,"Executor ID":"5","Host":"192.168.1.3","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":1467147047748,"Failed":false,"Accumulables":[]},"Task Metrics":{"Host Name":"192.168.1.11","Executor Deserialize Time":3166,"Executor Run Time":25622,"Result Size":2130,"JVM GC Time":331,"Result Serialization Time":0,"Memory Bytes Spilled":0,"Disk Bytes Spilled":0,"Input Metrics":{"Data Read Method":"Hadoop","Bytes Read":65536,"Records Read":4253236}}}
{"Event":"SparkListenerStageCompleted","Stage Info":{"Stage ID":0,"Stage Attempt ID":0,"Stage Name":"sortByKey at /home/daniar/documents/SPARK/spark-1.6.1/sort.py:12","Number of Tasks":4,"RDD Info":[{"RDD ID":2,"Name":"PythonRDD","Callsite":"sortByKey at /home/daniar/documents/SPARK/spark-1.6.1/sort.py:12","Parent IDs":[1],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":4,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":0,"Name":"../generated_file/list_int","Scope":"{\"id\":\"0\",\"name\":\"textFile\"}","Callsite":"textFile at NativeMethodAccessorImpl.java:-2","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":4,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":1,"Name":"../generated_file/list_int","Scope":"{\"id\":\"0\",\"name\":\"textFile\"}","Callsite":"textFile at NativeMethodAccessorImpl.java:-2","Parent IDs":[0],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":4,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.collect(RDD.scala:926)\norg.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:405)\norg.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:498)\npy4j.reflection.MethodInvoker.invoke(MethodInvoker.java:231)\npy4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:381)\npy4j.Gateway.invoke(Gateway.java:259)\npy4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:133)\npy4j.commands.CallCommand.execute(CallCommand.java:79)\npy4j.GatewayConnection.run(GatewayConnection.java:209)\njava.lang.Thread.run(Thread.java:745)","Submission Time":1467147018303,"Completion Time":1467147047775,"Accumulables":[]}}
{"Event":"SparkListenerJobEnd","Job ID":0,"Completion Time":1467147047815,"Job Result":{"Result":"JobSucceeded"}}
{"Event":"SparkListenerJobStart","Job ID":1,"Submission Time":1467147048066,"Stage Infos":[{"Stage ID":1,"Stage Attempt ID":0,"Stage Name":"sortByKey at /home/daniar/documents/SPARK/spark-1.6.1/sort.py:12","Number of Tasks":4,"RDD Info":[{"RDD ID":3,"Name":"PythonRDD","Callsite":"sortByKey at /home/daniar/documents/SPARK/spark-1.6.1/sort.py:12","Parent IDs":[1],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":4,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":0,"Name":"../generated_file/list_int","Scope":"{\"id\":\"0\",\"name\":\"textFile\"}","Callsite":"textFile at NativeMethodAccessorImpl.java:-2","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":4,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":1,"Name":"../generated_file/list_int","Scope":"{\"id\":\"0\",\"name\":\"textFile\"}","Callsite":"textFile at NativeMethodAccessorImpl.java:-2","Parent IDs":[0],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":4,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.collect(RDD.scala:926)\norg.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:405)\norg.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:498)\npy4j.reflection.MethodInvoker.invoke(MethodInvoker.java:231)\npy4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:381)\npy4j.Gateway.invoke(Gateway.java:259)\npy4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:133)\npy4j.commands.CallCommand.execute(CallCommand.java:79)\npy4j.GatewayConnection.run(GatewayConnection.java:209)\njava.lang.Thread.run(Thread.java:745)","Accumulables":[]}],"Stage IDs":[1],"Properties":{"spark.rdd.scope.noOverride":"true","spark.rdd.scope":"{\"id\":\"5\",\"name\":\"collect\"}","callSite.short":"sortByKey at /home/daniar/documents/SPARK/spark-1.6.1/sort.py:12"}}
{"Event":"SparkListenerStageSubmitted","Stage Info":{"Stage ID":1,"Stage Attempt ID":0,"Stage Name":"sortByKey at /home/daniar/documents/SPARK/spark-1.6.1/sort.py:12","Number of Tasks":4,"RDD Info":[{"RDD ID":3,"Name":"PythonRDD","Callsite":"sortByKey at /home/daniar/documents/SPARK/spark-1.6.1/sort.py:12","Parent IDs":[1],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":4,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":0,"Name":"../generated_file/list_int","Scope":"{\"id\":\"0\",\"name\":\"textFile\"}","Callsite":"textFile at NativeMethodAccessorImpl.java:-2","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":4,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":1,"Name":"../generated_file/list_int","Scope":"{\"id\":\"0\",\"name\":\"textFile\"}","Callsite":"textFile at NativeMethodAccessorImpl.java:-2","Parent IDs":[0],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":4,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.collect(RDD.scala:926)\norg.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:405)\norg.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:498)\npy4j.reflection.MethodInvoker.invoke(MethodInvoker.java:231)\npy4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:381)\npy4j.Gateway.invoke(Gateway.java:259)\npy4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:133)\npy4j.commands.CallCommand.execute(CallCommand.java:79)\npy4j.GatewayConnection.run(GatewayConnection.java:209)\njava.lang.Thread.run(Thread.java:745)","Submission Time":1467147048074,"Accumulables":[]},"Properties":{"spark.rdd.scope.noOverride":"true","spark.rdd.scope":"{\"id\":\"5\",\"name\":\"collect\"}","callSite.short":"sortByKey at /home/daniar/documents/SPARK/spark-1.6.1/sort.py:12"}}
{"Event":"SparkListenerTaskStart","Stage ID":1,"Stage Attempt ID":0,"Task Info":{"Task ID":4,"Index":0,"Attempt":0,"Launch Time":1467147048075,"Executor ID":"5","Host":"192.168.1.3","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":0,"Failed":false,"Accumulables":[]}}
{"Event":"SparkListenerTaskStart","Stage ID":1,"Stage Attempt ID":0,"Task Info":{"Task ID":5,"Index":1,"Attempt":0,"Launch Time":1467147048076,"Executor ID":"6","Host":"192.168.1.3","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":0,"Failed":false,"Accumulables":[]}}
{"Event":"SparkListenerTaskStart","Stage ID":1,"Stage Attempt ID":0,"Task Info":{"Task ID":6,"Index":2,"Attempt":0,"Launch Time":1467147048076,"Executor ID":"7","Host":"192.168.1.3","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":0,"Failed":false,"Accumulables":[]}}
{"Event":"SparkListenerTaskStart","Stage ID":1,"Stage Attempt ID":0,"Task Info":{"Task ID":7,"Index":3,"Attempt":0,"Launch Time":1467147048077,"Executor ID":"0","Host":"192.168.1.3","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":0,"Failed":false,"Accumulables":[]}}
{"Event":"SparkListenerTaskEnd","Stage ID":1,"Stage Attempt ID":0,"Task Type":"ResultTask","Task End Reason":{"Reason":"Success"},"Task Info":{"Task ID":7,"Index":3,"Attempt":0,"Launch Time":1467147048077,"Executor ID":"0","Host":"192.168.1.3","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":1467147067123,"Failed":false,"Accumulables":[]},"Task Metrics":{"Host Name":"192.168.1.11","Executor Deserialize Time":47,"Executor Run Time":18985,"Result Size":2292,"JVM GC Time":27,"Result Serialization Time":0,"Memory Bytes Spilled":0,"Disk Bytes Spilled":0,"Input Metrics":{"Data Read Method":"Hadoop","Bytes Read":65536,"Records Read":3484461}}}
{"Event":"SparkListenerTaskEnd","Stage ID":1,"Stage Attempt ID":0,"Task Type":"ResultTask","Task End Reason":{"Reason":"Success"},"Task Info":{"Task ID":5,"Index":1,"Attempt":0,"Launch Time":1467147048076,"Executor ID":"6","Host":"192.168.1.3","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":1467147072053,"Failed":false,"Accumulables":[]},"Task Metrics":{"Host Name":"192.168.1.11","Executor Deserialize Time":35,"Executor Run Time":23924,"Result Size":2320,"JVM GC Time":78,"Result Serialization Time":0,"Memory Bytes Spilled":0,"Disk Bytes Spilled":0,"Input Metrics":{"Data Read Method":"Hadoop","Bytes Read":65536,"Records Read":4253268}}}
{"Event":"SparkListenerTaskEnd","Stage ID":1,"Stage Attempt ID":0,"Task Type":"ResultTask","Task End Reason":{"Reason":"Success"},"Task Info":{"Task ID":6,"Index":2,"Attempt":0,"Launch Time":1467147048076,"Executor ID":"7","Host":"192.168.1.3","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":1467147073364,"Failed":false,"Accumulables":[]},"Task Metrics":{"Host Name":"192.168.1.11","Executor Deserialize Time":36,"Executor Run Time":25240,"Result Size":2254,"JVM GC Time":68,"Result Serialization Time":0,"Memory Bytes Spilled":0,"Disk Bytes Spilled":0,"Input Metrics":{"Data Read Method":"Hadoop","Bytes Read":65536,"Records Read":4253236}}}
{"Event":"SparkListenerTaskEnd","Stage ID":1,"Stage Attempt ID":0,"Task Type":"ResultTask","Task End Reason":{"Reason":"Success"},"Task Info":{"Task ID":4,"Index":0,"Attempt":0,"Launch Time":1467147048075,"Executor ID":"5","Host":"192.168.1.3","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":1467147074042,"Failed":false,"Accumulables":[]},"Task Metrics":{"Host Name":"192.168.1.11","Executor Deserialize Time":37,"Executor Run Time":25920,"Result Size":2287,"JVM GC Time":65,"Result Serialization Time":0,"Memory Bytes Spilled":0,"Disk Bytes Spilled":0,"Input Metrics":{"Data Read Method":"Hadoop","Bytes Read":0,"Records Read":4253453}}}
{"Event":"SparkListenerStageCompleted","Stage Info":{"Stage ID":1,"Stage Attempt ID":0,"Stage Name":"sortByKey at /home/daniar/documents/SPARK/spark-1.6.1/sort.py:12","Number of Tasks":4,"RDD Info":[{"RDD ID":3,"Name":"PythonRDD","Callsite":"sortByKey at /home/daniar/documents/SPARK/spark-1.6.1/sort.py:12","Parent IDs":[1],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":4,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":0,"Name":"../generated_file/list_int","Scope":"{\"id\":\"0\",\"name\":\"textFile\"}","Callsite":"textFile at NativeMethodAccessorImpl.java:-2","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":4,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":1,"Name":"../generated_file/list_int","Scope":"{\"id\":\"0\",\"name\":\"textFile\"}","Callsite":"textFile at NativeMethodAccessorImpl.java:-2","Parent IDs":[0],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":4,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.collect(RDD.scala:926)\norg.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:405)\norg.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:498)\npy4j.reflection.MethodInvoker.invoke(MethodInvoker.java:231)\npy4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:381)\npy4j.Gateway.invoke(Gateway.java:259)\npy4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:133)\npy4j.commands.CallCommand.execute(CallCommand.java:79)\npy4j.GatewayConnection.run(GatewayConnection.java:209)\njava.lang.Thread.run(Thread.java:745)","Submission Time":1467147048074,"Completion Time":1467147074043,"Accumulables":[]}}
{"Event":"SparkListenerJobEnd","Job ID":1,"Completion Time":1467147074043,"Job Result":{"Result":"JobSucceeded"}}
{"Event":"SparkListenerJobStart","Job ID":2,"Submission Time":1467147076306,"Stage Infos":[{"Stage ID":2,"Stage Attempt ID":0,"Stage Name":"sortByKey at /home/daniar/documents/SPARK/spark-1.6.1/sort.py:12","Number of Tasks":4,"RDD Info":[{"RDD ID":5,"Name":"PairwiseRDD","Callsite":"sortByKey at /home/daniar/documents/SPARK/spark-1.6.1/sort.py:12","Parent IDs":[4],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":4,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":4,"Name":"PythonRDD","Callsite":"sortByKey at /home/daniar/documents/SPARK/spark-1.6.1/sort.py:12","Parent IDs":[1],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":4,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":0,"Name":"../generated_file/list_int","Scope":"{\"id\":\"0\",\"name\":\"textFile\"}","Callsite":"textFile at NativeMethodAccessorImpl.java:-2","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":4,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":1,"Name":"../generated_file/list_int","Scope":"{\"id\":\"0\",\"name\":\"textFile\"}","Callsite":"textFile at NativeMethodAccessorImpl.java:-2","Parent IDs":[0],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":4,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.<init>(RDD.scala:98)\norg.apache.spark.api.python.PairwiseRDD.<init>(PythonRDD.scala:338)\nsun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\nsun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\nsun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\njava.lang.reflect.Constructor.newInstance(Constructor.java:423)\npy4j.reflection.MethodInvoker.invoke(MethodInvoker.java:234)\npy4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:381)\npy4j.Gateway.invoke(Gateway.java:214)\npy4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:79)\npy4j.commands.ConstructorCommand.execute(ConstructorCommand.java:68)\npy4j.GatewayConnection.run(GatewayConnection.java:209)\njava.lang.Thread.run(Thread.java:745)","Accumulables":[]},{"Stage ID":3,"Stage Attempt ID":0,"Stage Name":"saveAsTextFile at NativeMethodAccessorImpl.java:-2","Number of Tasks":4,"RDD Info":[{"RDD ID":10,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"11\",\"name\":\"saveAsTextFile\"}","Callsite":"saveAsTextFile at NativeMethodAccessorImpl.java:-2","Parent IDs":[9],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":4,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":6,"Name":"ShuffledRDD","Scope":"{\"id\":\"8\",\"name\":\"partitionBy\"}","Callsite":"partitionBy at NativeMethodAccessorImpl.java:-2","Parent IDs":[5],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":4,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":9,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"10\",\"name\":\"map\"}","Callsite":"map at NativeMethodAccessorImpl.java:-2","Parent IDs":[8],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":4,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":8,"Name":"PythonRDD","Callsite":"RDD at PythonRDD.scala:43","Parent IDs":[7],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":4,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":7,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"9\",\"name\":\"mapPartitions\"}","Callsite":"mapPartitions at PythonRDD.scala:374","Parent IDs":[6],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":4,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[2],"Details":"org.apache.spark.api.java.AbstractJavaRDDLike.saveAsTextFile(JavaRDDLike.scala:46)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:498)\npy4j.reflection.MethodInvoker.invoke(MethodInvoker.java:231)\npy4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:381)\npy4j.Gateway.invoke(Gateway.java:259)\npy4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:133)\npy4j.commands.CallCommand.execute(CallCommand.java:79)\npy4j.GatewayConnection.run(GatewayConnection.java:209)\njava.lang.Thread.run(Thread.java:745)","Accumulables":[]}],"Stage IDs":[2,3],"Properties":{"spark.rdd.scope.noOverride":"true","spark.rdd.scope":"{\"id\":\"11\",\"name\":\"saveAsTextFile\"}"}}
{"Event":"SparkListenerStageSubmitted","Stage Info":{"Stage ID":2,"Stage Attempt ID":0,"Stage Name":"sortByKey at /home/daniar/documents/SPARK/spark-1.6.1/sort.py:12","Number of Tasks":4,"RDD Info":[{"RDD ID":5,"Name":"PairwiseRDD","Callsite":"sortByKey at /home/daniar/documents/SPARK/spark-1.6.1/sort.py:12","Parent IDs":[4],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":4,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":4,"Name":"PythonRDD","Callsite":"sortByKey at /home/daniar/documents/SPARK/spark-1.6.1/sort.py:12","Parent IDs":[1],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":4,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":0,"Name":"../generated_file/list_int","Scope":"{\"id\":\"0\",\"name\":\"textFile\"}","Callsite":"textFile at NativeMethodAccessorImpl.java:-2","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":4,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":1,"Name":"../generated_file/list_int","Scope":"{\"id\":\"0\",\"name\":\"textFile\"}","Callsite":"textFile at NativeMethodAccessorImpl.java:-2","Parent IDs":[0],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":4,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.<init>(RDD.scala:98)\norg.apache.spark.api.python.PairwiseRDD.<init>(PythonRDD.scala:338)\nsun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\nsun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\nsun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\njava.lang.reflect.Constructor.newInstance(Constructor.java:423)\npy4j.reflection.MethodInvoker.invoke(MethodInvoker.java:234)\npy4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:381)\npy4j.Gateway.invoke(Gateway.java:214)\npy4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:79)\npy4j.commands.ConstructorCommand.execute(ConstructorCommand.java:68)\npy4j.GatewayConnection.run(GatewayConnection.java:209)\njava.lang.Thread.run(Thread.java:745)","Accumulables":[]},"Properties":{"spark.rdd.scope.noOverride":"true","spark.rdd.scope":"{\"id\":\"11\",\"name\":\"saveAsTextFile\"}"}}
{"Event":"SparkListenerTaskStart","Stage ID":2,"Stage Attempt ID":0,"Task Info":{"Task ID":8,"Index":0,"Attempt":0,"Launch Time":1467147076435,"Executor ID":"1","Host":"192.168.1.3","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":0,"Failed":false,"Accumulables":[]}}
{"Event":"SparkListenerTaskStart","Stage ID":2,"Stage Attempt ID":0,"Task Info":{"Task ID":9,"Index":1,"Attempt":0,"Launch Time":1467147076438,"Executor ID":"0","Host":"192.168.1.3","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":0,"Failed":false,"Accumulables":[]}}
{"Event":"SparkListenerTaskStart","Stage ID":2,"Stage Attempt ID":0,"Task Info":{"Task ID":10,"Index":2,"Attempt":0,"Launch Time":1467147076439,"Executor ID":"6","Host":"192.168.1.3","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":0,"Failed":false,"Accumulables":[]}}
{"Event":"SparkListenerTaskStart","Stage ID":2,"Stage Attempt ID":0,"Task Info":{"Task ID":11,"Index":3,"Attempt":0,"Launch Time":1467147076439,"Executor ID":"2","Host":"192.168.1.3","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":0,"Failed":false,"Accumulables":[]}}
{"Event":"SparkListenerTaskEnd","Stage ID":2,"Stage Attempt ID":0,"Task Type":"ShuffleMapTask","Task End Reason":{"Reason":"Success"},"Task Info":{"Task ID":10,"Index":2,"Attempt":0,"Launch Time":1467147076439,"Executor ID":"6","Host":"192.168.1.3","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":1467147113769,"Failed":false,"Accumulables":[]},"Task Metrics":{"Host Name":"192.168.1.11","Executor Deserialize Time":86,"Executor Run Time":37221,"Result Size":2321,"JVM GC Time":139,"Result Serialization Time":12,"Memory Bytes Spilled":0,"Disk Bytes Spilled":0,"Shuffle Write Metrics":{"Shuffle Bytes Written":65352156,"Shuffle Write Time":6845204977,"Shuffle Records Written":164},"Input Metrics":{"Data Read Method":"Hadoop","Bytes Read":65536,"Records Read":4253236}}}
{"Event":"SparkListenerTaskEnd","Stage ID":2,"Stage Attempt ID":0,"Task Type":"ShuffleMapTask","Task End Reason":{"Reason":"Success"},"Task Info":{"Task ID":9,"Index":1,"Attempt":0,"Launch Time":1467147076438,"Executor ID":"0","Host":"192.168.1.3","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":1467147113769,"Failed":false,"Accumulables":[]},"Task Metrics":{"Host Name":"192.168.1.11","Executor Deserialize Time":54,"Executor Run Time":37253,"Result Size":2321,"JVM GC Time":412,"Result Serialization Time":12,"Memory Bytes Spilled":0,"Disk Bytes Spilled":0,"Shuffle Write Metrics":{"Shuffle Bytes Written":65350616,"Shuffle Write Time":7118959450,"Shuffle Records Written":164},"Input Metrics":{"Data Read Method":"Hadoop","Bytes Read":65536,"Records Read":4253268}}}
{"Event":"SparkListenerTaskEnd","Stage ID":2,"Stage Attempt ID":0,"Task Type":"ShuffleMapTask","Task End Reason":{"Reason":"Success"},"Task Info":{"Task ID":11,"Index":3,"Attempt":0,"Launch Time":1467147076439,"Executor ID":"2","Host":"192.168.1.3","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":1467147182678,"Failed":false,"Accumulables":[]},"Task Metrics":{"Host Name":"192.168.1.11","Executor Deserialize Time":44398,"Executor Run Time":61112,"Result Size":2321,"JVM GC Time":176,"Result Serialization Time":127,"Memory Bytes Spilled":0,"Disk Bytes Spilled":0,"Shuffle Write Metrics":{"Shuffle Bytes Written":53536423,"Shuffle Write Time":830024126,"Shuffle Records Written":148},"Input Metrics":{"Data Read Method":"Hadoop","Bytes Read":65536,"Records Read":3484461}}}
{"Event":"SparkListenerTaskEnd","Stage ID":2,"Stage Attempt ID":0,"Task Type":"ShuffleMapTask","Task End Reason":{"Reason":"Success"},"Task Info":{"Task ID":8,"Index":0,"Attempt":0,"Launch Time":1467147076435,"Executor ID":"1","Host":"192.168.1.3","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":1467147261659,"Failed":false,"Accumulables":[]},"Task Metrics":{"Host Name":"192.168.1.11","Executor Deserialize Time":44404,"Executor Run Time":140059,"Result Size":2321,"JVM GC Time":238,"Result Serialization Time":16,"Memory Bytes Spilled":0,"Disk Bytes Spilled":0,"Shuffle Write Metrics":{"Shuffle Bytes Written":65356545,"Shuffle Write Time":4173384779,"Shuffle Records Written":164},"Input Metrics":{"Data Read Method":"Hadoop","Bytes Read":0,"Records Read":4253453}}}
{"Event":"SparkListenerStageCompleted","Stage Info":{"Stage ID":2,"Stage Attempt ID":0,"Stage Name":"sortByKey at /home/daniar/documents/SPARK/spark-1.6.1/sort.py:12","Number of Tasks":4,"RDD Info":[{"RDD ID":5,"Name":"PairwiseRDD","Callsite":"sortByKey at /home/daniar/documents/SPARK/spark-1.6.1/sort.py:12","Parent IDs":[4],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":4,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":4,"Name":"PythonRDD","Callsite":"sortByKey at /home/daniar/documents/SPARK/spark-1.6.1/sort.py:12","Parent IDs":[1],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":4,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":0,"Name":"../generated_file/list_int","Scope":"{\"id\":\"0\",\"name\":\"textFile\"}","Callsite":"textFile at NativeMethodAccessorImpl.java:-2","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":4,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":1,"Name":"../generated_file/list_int","Scope":"{\"id\":\"0\",\"name\":\"textFile\"}","Callsite":"textFile at NativeMethodAccessorImpl.java:-2","Parent IDs":[0],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":4,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.<init>(RDD.scala:98)\norg.apache.spark.api.python.PairwiseRDD.<init>(PythonRDD.scala:338)\nsun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\nsun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\nsun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\njava.lang.reflect.Constructor.newInstance(Constructor.java:423)\npy4j.reflection.MethodInvoker.invoke(MethodInvoker.java:234)\npy4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:381)\npy4j.Gateway.invoke(Gateway.java:214)\npy4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:79)\npy4j.commands.ConstructorCommand.execute(ConstructorCommand.java:68)\npy4j.GatewayConnection.run(GatewayConnection.java:209)\njava.lang.Thread.run(Thread.java:745)","Submission Time":1467147076442,"Completion Time":1467147261659,"Accumulables":[]}}
{"Event":"SparkListenerStageSubmitted","Stage Info":{"Stage ID":3,"Stage Attempt ID":0,"Stage Name":"saveAsTextFile at NativeMethodAccessorImpl.java:-2","Number of Tasks":4,"RDD Info":[{"RDD ID":10,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"11\",\"name\":\"saveAsTextFile\"}","Callsite":"saveAsTextFile at NativeMethodAccessorImpl.java:-2","Parent IDs":[9],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":4,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":6,"Name":"ShuffledRDD","Scope":"{\"id\":\"8\",\"name\":\"partitionBy\"}","Callsite":"partitionBy at NativeMethodAccessorImpl.java:-2","Parent IDs":[5],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":4,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":9,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"10\",\"name\":\"map\"}","Callsite":"map at NativeMethodAccessorImpl.java:-2","Parent IDs":[8],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":4,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":8,"Name":"PythonRDD","Callsite":"RDD at PythonRDD.scala:43","Parent IDs":[7],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":4,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":7,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"9\",\"name\":\"mapPartitions\"}","Callsite":"mapPartitions at PythonRDD.scala:374","Parent IDs":[6],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":4,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[2],"Details":"org.apache.spark.api.java.AbstractJavaRDDLike.saveAsTextFile(JavaRDDLike.scala:46)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:498)\npy4j.reflection.MethodInvoker.invoke(MethodInvoker.java:231)\npy4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:381)\npy4j.Gateway.invoke(Gateway.java:259)\npy4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:133)\npy4j.commands.CallCommand.execute(CallCommand.java:79)\npy4j.GatewayConnection.run(GatewayConnection.java:209)\njava.lang.Thread.run(Thread.java:745)","Accumulables":[]},"Properties":{"spark.rdd.scope.noOverride":"true","spark.rdd.scope":"{\"id\":\"11\",\"name\":\"saveAsTextFile\"}"}}
{"Event":"SparkListenerTaskStart","Stage ID":3,"Stage Attempt ID":0,"Task Info":{"Task ID":12,"Index":0,"Attempt":0,"Launch Time":1467147262227,"Executor ID":"0","Host":"192.168.1.3","Locality":"ANY","Speculative":false,"Getting Result Time":0,"Finish Time":0,"Failed":false,"Accumulables":[]}}
{"Event":"SparkListenerTaskStart","Stage ID":3,"Stage Attempt ID":0,"Task Info":{"Task ID":13,"Index":1,"Attempt":0,"Launch Time":1467147262228,"Executor ID":"4","Host":"192.168.1.3","Locality":"ANY","Speculative":false,"Getting Result Time":0,"Finish Time":0,"Failed":false,"Accumulables":[]}}
{"Event":"SparkListenerTaskStart","Stage ID":3,"Stage Attempt ID":0,"Task Info":{"Task ID":14,"Index":2,"Attempt":0,"Launch Time":1467147262228,"Executor ID":"7","Host":"192.168.1.3","Locality":"ANY","Speculative":false,"Getting Result Time":0,"Finish Time":0,"Failed":false,"Accumulables":[]}}
{"Event":"SparkListenerTaskStart","Stage ID":3,"Stage Attempt ID":0,"Task Info":{"Task ID":15,"Index":3,"Attempt":0,"Launch Time":1467147262229,"Executor ID":"6","Host":"192.168.1.3","Locality":"ANY","Speculative":false,"Getting Result Time":0,"Finish Time":0,"Failed":false,"Accumulables":[]}}
{"Event":"SparkListenerTaskEnd","Stage ID":3,"Stage Attempt ID":0,"Task Type":"ResultTask","Task End Reason":{"Reason":"FetchFailed","Block Manager Address":{"Executor ID":"6","Host":"192.168.1.11","Port":56770},"Shuffle ID":0,"Map ID":2,"Reduce ID":2,"Message":"org.apache.spark.shuffle.FetchFailedException: Failed to connect to /192.168.1.11:56770\n\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.throwFetchFailedException(ShuffleBlockFetcherIterator.scala:323)\n\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:300)\n\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:51)\n\tat scala.collection.Iterator$$anon$11.next(Iterator.scala:328)\n\tat scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)\n\tat org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:32)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:39)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:727)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1157)\n\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:452)\n\tat org.apache.spark.api.python.PythonRunner$WriterThread$$anonfun$run$3.apply(PythonRDD.scala:280)\n\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1765)\n\tat org.apache.spark.api.python.PythonRunner$WriterThread.run(PythonRDD.scala:239)\nCaused by: java.io.IOException: Failed to connect to /192.168.1.11:56770\n\tat org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:216)\n\tat org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:167)\n\tat org.apache.spark.network.netty.NettyBlockTransferService$$anon$1.createAndStart(NettyBlockTransferService.scala:90)\n\tat org.apache.spark.network.shuffle.RetryingBlockFetcher.fetchAllOutstanding(RetryingBlockFetcher.java:140)\n\tat org.apache.spark.network.shuffle.RetryingBlockFetcher.access$200(RetryingBlockFetcher.java:43)\n\tat org.apache.spark.network.shuffle.RetryingBlockFetcher$1.run(RetryingBlockFetcher.java:170)\n\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n\tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n\tat java.lang.Thread.run(Thread.java:745)\nCaused by: java.net.NoRouteToHostException: No route to host: /192.168.1.11:56770\n\tat sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)\n\tat sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)\n\tat io.netty.channel.socket.nio.NioSocketChannel.doFinishConnect(NioSocketChannel.java:224)\n\tat io.netty.channel.nio.AbstractNioChannel$AbstractNioUnsafe.finishConnect(AbstractNioChannel.java:289)\n\tat io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:528)\n\tat io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:468)\n\tat io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:382)\n\tat io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:354)\n\tat io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:111)\n\t... 1 more\n"},"Task Info":{"Task ID":14,"Index":2,"Attempt":0,"Launch Time":1467147262228,"Executor ID":"7","Host":"192.168.1.3","Locality":"ANY","Speculative":false,"Getting Result Time":0,"Finish Time":1467147287082,"Failed":true,"Accumulables":[]}}
{"Event":"SparkListenerStageCompleted","Stage Info":{"Stage ID":3,"Stage Attempt ID":0,"Stage Name":"saveAsTextFile at NativeMethodAccessorImpl.java:-2","Number of Tasks":4,"RDD Info":[{"RDD ID":10,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"11\",\"name\":\"saveAsTextFile\"}","Callsite":"saveAsTextFile at NativeMethodAccessorImpl.java:-2","Parent IDs":[9],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":4,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":6,"Name":"ShuffledRDD","Scope":"{\"id\":\"8\",\"name\":\"partitionBy\"}","Callsite":"partitionBy at NativeMethodAccessorImpl.java:-2","Parent IDs":[5],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":4,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":9,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"10\",\"name\":\"map\"}","Callsite":"map at NativeMethodAccessorImpl.java:-2","Parent IDs":[8],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":4,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":8,"Name":"PythonRDD","Callsite":"RDD at PythonRDD.scala:43","Parent IDs":[7],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":4,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":7,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"9\",\"name\":\"mapPartitions\"}","Callsite":"mapPartitions at PythonRDD.scala:374","Parent IDs":[6],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":4,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[2],"Details":"org.apache.spark.api.java.AbstractJavaRDDLike.saveAsTextFile(JavaRDDLike.scala:46)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:498)\npy4j.reflection.MethodInvoker.invoke(MethodInvoker.java:231)\npy4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:381)\npy4j.Gateway.invoke(Gateway.java:259)\npy4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:133)\npy4j.commands.CallCommand.execute(CallCommand.java:79)\npy4j.GatewayConnection.run(GatewayConnection.java:209)\njava.lang.Thread.run(Thread.java:745)","Submission Time":1467147262227,"Completion Time":1467147287742,"Failure Reason":"org.apache.spark.shuffle.FetchFailedException: Failed to connect to /192.168.1.11:56770\n\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.throwFetchFailedException(ShuffleBlockFetcherIterator.scala:323)\n\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:300)\n\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:51)\n\tat scala.collection.Iterator$$anon$11.next(Iterator.scala:328)\n\tat scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)\n\tat org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:32)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:39)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:727)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1157)\n\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:452)\n\tat org.apache.spark.api.python.PythonRunner$WriterThread$$anonfun$run$3.apply(PythonRDD.scala:280)\n\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1765)\n\tat org.apache.spark.api.python.PythonRunner$WriterThread.run(PythonRDD.scala:239)\nCaused by: java.io.IOException: Failed to connect to /192.168.1.11:56770\n\tat org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:216)\n\tat org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:167)\n\tat org.apache.spark.network.netty.NettyBlockTransferService$$anon$1.createAndStart(NettyBlockTransferService.scala:90)\n\tat org.apache.spark.network.shuffle.RetryingBlockFetcher.fetchAllOutstanding(RetryingBlockFetcher.java:140)\n\tat org.apache.spark.network.shuffle.RetryingBlockFetcher.access$200(RetryingBlockFetcher.java:43)\n\tat org.apache.spark.network.shuffle.RetryingBlockFetcher$1.run(RetryingBlockFetcher.java:170)\n\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n\tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n\tat java.lang.Thread.run(Thread.java:745)\nCaused by: java.net.NoRouteToHostException: No route to host: /192.168.1.11:56770\n\tat sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)\n\tat sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)\n\tat io.netty.channel.socket.nio.NioSocketChannel.doFinishConnect(NioSocketChannel.java:224)\n\tat io.netty.channel.nio.AbstractNioChannel$AbstractNioUnsafe.finishConnect(AbstractNioChannel.java:289)\n\tat io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:528)\n\tat io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:468)\n\tat io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:382)\n\tat io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:354)\n\tat io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:111)\n\t... 1 more\n","Accumulables":[]}}
{"Event":"SparkListenerBlockManagerRemoved","Block Manager ID":{"Executor ID":"6","Host":"192.168.1.11","Port":56770},"Timestamp":1467147288803}
{"Event":"SparkListenerStageSubmitted","Stage Info":{"Stage ID":2,"Stage Attempt ID":1,"Stage Name":"sortByKey at /home/daniar/documents/SPARK/spark-1.6.1/sort.py:12","Number of Tasks":1,"RDD Info":[{"RDD ID":5,"Name":"PairwiseRDD","Callsite":"sortByKey at /home/daniar/documents/SPARK/spark-1.6.1/sort.py:12","Parent IDs":[4],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":4,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":4,"Name":"PythonRDD","Callsite":"sortByKey at /home/daniar/documents/SPARK/spark-1.6.1/sort.py:12","Parent IDs":[1],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":4,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":0,"Name":"../generated_file/list_int","Scope":"{\"id\":\"0\",\"name\":\"textFile\"}","Callsite":"textFile at NativeMethodAccessorImpl.java:-2","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":4,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":1,"Name":"../generated_file/list_int","Scope":"{\"id\":\"0\",\"name\":\"textFile\"}","Callsite":"textFile at NativeMethodAccessorImpl.java:-2","Parent IDs":[0],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":4,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.<init>(RDD.scala:98)\norg.apache.spark.api.python.PairwiseRDD.<init>(PythonRDD.scala:338)\nsun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\nsun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\nsun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\njava.lang.reflect.Constructor.newInstance(Constructor.java:423)\npy4j.reflection.MethodInvoker.invoke(MethodInvoker.java:234)\npy4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:381)\npy4j.Gateway.invoke(Gateway.java:214)\npy4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:79)\npy4j.commands.ConstructorCommand.execute(ConstructorCommand.java:68)\npy4j.GatewayConnection.run(GatewayConnection.java:209)\njava.lang.Thread.run(Thread.java:745)","Accumulables":[]},"Properties":{"spark.rdd.scope.noOverride":"true","spark.rdd.scope":"{\"id\":\"11\",\"name\":\"saveAsTextFile\"}"}}
{"Event":"SparkListenerTaskStart","Stage ID":2,"Stage Attempt ID":1,"Task Info":{"Task ID":16,"Index":0,"Attempt":0,"Launch Time":1467147289326,"Executor ID":"4","Host":"192.168.1.3","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":0,"Failed":false,"Accumulables":[]}}
{"Event":"SparkListenerBlockManagerAdded","Block Manager ID":{"Executor ID":"6","Host":"192.168.1.11","Port":56770},"Maximum Memory":535953408,"Timestamp":1467147289684}
{"Event":"SparkListenerTaskEnd","Stage ID":3,"Stage Attempt ID":0,"Task Type":"ResultTask","Task End Reason":{"Reason":"FetchFailed","Block Manager Address":{"Executor ID":"0","Host":"192.168.1.11","Port":41011},"Shuffle ID":0,"Map ID":1,"Reduce ID":3,"Message":"org.apache.spark.shuffle.FetchFailedException: Failed to connect to /192.168.1.11:41011\n\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.throwFetchFailedException(ShuffleBlockFetcherIterator.scala:323)\n\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:300)\n\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:51)\n\tat scala.collection.Iterator$$anon$11.next(Iterator.scala:328)\n\tat scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)\n\tat org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:32)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:39)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:727)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1157)\n\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:452)\n\tat org.apache.spark.api.python.PythonRunner$WriterThread$$anonfun$run$3.apply(PythonRDD.scala:280)\n\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1765)\n\tat org.apache.spark.api.python.PythonRunner$WriterThread.run(PythonRDD.scala:239)\nCaused by: java.io.IOException: Failed to connect to /192.168.1.11:41011\n\tat org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:216)\n\tat org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:167)\n\tat org.apache.spark.network.netty.NettyBlockTransferService$$anon$1.createAndStart(NettyBlockTransferService.scala:90)\n\tat org.apache.spark.network.shuffle.RetryingBlockFetcher.fetchAllOutstanding(RetryingBlockFetcher.java:140)\n\tat org.apache.spark.network.shuffle.RetryingBlockFetcher.access$200(RetryingBlockFetcher.java:43)\n\tat org.apache.spark.network.shuffle.RetryingBlockFetcher$1.run(RetryingBlockFetcher.java:170)\n\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n\tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n\tat java.lang.Thread.run(Thread.java:745)\nCaused by: java.net.NoRouteToHostException: No route to host: /192.168.1.11:41011\n\tat sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)\n\tat sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)\n\tat io.netty.channel.socket.nio.NioSocketChannel.doFinishConnect(NioSocketChannel.java:224)\n\tat io.netty.channel.nio.AbstractNioChannel$AbstractNioUnsafe.finishConnect(AbstractNioChannel.java:289)\n\tat io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:528)\n\tat io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:468)\n\tat io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:382)\n\tat io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:354)\n\tat io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:111)\n\t... 1 more\n"},"Task Info":{"Task ID":15,"Index":3,"Attempt":0,"Launch Time":1467147262229,"Executor ID":"6","Host":"192.168.1.3","Locality":"ANY","Speculative":false,"Getting Result Time":0,"Finish Time":1467147291017,"Failed":true,"Accumulables":[]}}
{"Event":"SparkListenerBlockManagerRemoved","Block Manager ID":{"Executor ID":"0","Host":"192.168.1.11","Port":41011},"Timestamp":1467147291099}
{"Event":"SparkListenerTaskEnd","Stage ID":3,"Stage Attempt ID":0,"Task Type":"ResultTask","Task End Reason":{"Reason":"FetchFailed","Block Manager Address":{"Executor ID":"2","Host":"192.168.1.11","Port":35327},"Shuffle ID":0,"Map ID":3,"Reduce ID":0,"Message":"org.apache.spark.shuffle.FetchFailedException: Failed to connect to /192.168.1.11:35327\n\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.throwFetchFailedException(ShuffleBlockFetcherIterator.scala:323)\n\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:300)\n\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:51)\n\tat scala.collection.Iterator$$anon$11.next(Iterator.scala:328)\n\tat scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)\n\tat org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:32)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:39)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:727)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1157)\n\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:452)\n\tat org.apache.spark.api.python.PythonRunner$WriterThread$$anonfun$run$3.apply(PythonRDD.scala:280)\n\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1765)\n\tat org.apache.spark.api.python.PythonRunner$WriterThread.run(PythonRDD.scala:239)\nCaused by: java.io.IOException: Failed to connect to /192.168.1.11:35327\n\tat org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:216)\n\tat org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:167)\n\tat org.apache.spark.network.netty.NettyBlockTransferService$$anon$1.createAndStart(NettyBlockTransferService.scala:90)\n\tat org.apache.spark.network.shuffle.RetryingBlockFetcher.fetchAllOutstanding(RetryingBlockFetcher.java:140)\n\tat org.apache.spark.network.shuffle.RetryingBlockFetcher.access$200(RetryingBlockFetcher.java:43)\n\tat org.apache.spark.network.shuffle.RetryingBlockFetcher$1.run(RetryingBlockFetcher.java:170)\n\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n\tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n\tat java.lang.Thread.run(Thread.java:745)\nCaused by: java.net.NoRouteToHostException: No route to host: /192.168.1.11:35327\n\tat sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)\n\tat sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)\n\tat io.netty.channel.socket.nio.NioSocketChannel.doFinishConnect(NioSocketChannel.java:224)\n\tat io.netty.channel.nio.AbstractNioChannel$AbstractNioUnsafe.finishConnect(AbstractNioChannel.java:289)\n\tat io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:528)\n\tat io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:468)\n\tat io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:382)\n\tat io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:354)\n\tat io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:111)\n\t... 1 more\n"},"Task Info":{"Task ID":12,"Index":0,"Attempt":0,"Launch Time":1467147262227,"Executor ID":"0","Host":"192.168.1.3","Locality":"ANY","Speculative":false,"Getting Result Time":0,"Finish Time":1467147291018,"Failed":true,"Accumulables":[]}}
{"Event":"SparkListenerBlockManagerRemoved","Block Manager ID":{"Executor ID":"2","Host":"192.168.1.11","Port":35327},"Timestamp":1467147291100}
{"Event":"SparkListenerBlockManagerAdded","Block Manager ID":{"Executor ID":"2","Host":"192.168.1.11","Port":35327},"Maximum Memory":535953408,"Timestamp":1467147295193}
{"Event":"SparkListenerBlockManagerAdded","Block Manager ID":{"Executor ID":"0","Host":"192.168.1.11","Port":41011},"Maximum Memory":535953408,"Timestamp":1467147296017}
{"Event":"SparkListenerTaskEnd","Stage ID":3,"Stage Attempt ID":0,"Task Type":"ResultTask","Task End Reason":{"Reason":"FetchFailed","Shuffle ID":0,"Map ID":-1,"Reduce ID":1,"Message":"org.apache.spark.shuffle.MetadataFetchFailedException: Missing an output location for shuffle 0\n\tat org.apache.spark.MapOutputTracker$$anonfun$org$apache$spark$MapOutputTracker$$convertMapStatuses$2.apply(MapOutputTracker.scala:542)\n\tat org.apache.spark.MapOutputTracker$$anonfun$org$apache$spark$MapOutputTracker$$convertMapStatuses$2.apply(MapOutputTracker.scala:538)\n\tat scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:772)\n\tat scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)\n\tat scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:108)\n\tat scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:771)\n\tat org.apache.spark.MapOutputTracker$.org$apache$spark$MapOutputTracker$$convertMapStatuses(MapOutputTracker.scala:538)\n\tat org.apache.spark.MapOutputTracker.getMapSizesByExecutorId(MapOutputTracker.scala:155)\n\tat org.apache.spark.shuffle.BlockStoreShuffleReader.read(BlockStoreShuffleReader.scala:47)\n\tat org.apache.spark.rdd.ShuffledRDD.compute(ShuffledRDD.scala:98)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:270)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:270)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:70)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:270)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:270)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:270)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:89)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:214)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n\tat java.lang.Thread.run(Thread.java:745)\n"},"Task Info":{"Task ID":13,"Index":1,"Attempt":0,"Launch Time":1467147262228,"Executor ID":"4","Host":"192.168.1.3","Locality":"ANY","Speculative":false,"Getting Result Time":0,"Finish Time":1467147323028,"Failed":true,"Accumulables":[]}}
{"Event":"SparkListenerApplicationEnd","Timestamp":1467147396598}
{"Event":"SparkListenerStageCompleted","Stage Info":{"Stage ID":2,"Stage Attempt ID":1,"Stage Name":"sortByKey at /home/daniar/documents/SPARK/spark-1.6.1/sort.py:12","Number of Tasks":1,"RDD Info":[{"RDD ID":5,"Name":"PairwiseRDD","Callsite":"sortByKey at /home/daniar/documents/SPARK/spark-1.6.1/sort.py:12","Parent IDs":[4],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":4,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":4,"Name":"PythonRDD","Callsite":"sortByKey at /home/daniar/documents/SPARK/spark-1.6.1/sort.py:12","Parent IDs":[1],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":4,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":0,"Name":"../generated_file/list_int","Scope":"{\"id\":\"0\",\"name\":\"textFile\"}","Callsite":"textFile at NativeMethodAccessorImpl.java:-2","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":4,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":1,"Name":"../generated_file/list_int","Scope":"{\"id\":\"0\",\"name\":\"textFile\"}","Callsite":"textFile at NativeMethodAccessorImpl.java:-2","Parent IDs":[0],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":4,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.<init>(RDD.scala:98)\norg.apache.spark.api.python.PairwiseRDD.<init>(PythonRDD.scala:338)\nsun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\nsun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\nsun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\njava.lang.reflect.Constructor.newInstance(Constructor.java:423)\npy4j.reflection.MethodInvoker.invoke(MethodInvoker.java:234)\npy4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:381)\npy4j.Gateway.invoke(Gateway.java:214)\npy4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:79)\npy4j.commands.ConstructorCommand.execute(ConstructorCommand.java:68)\npy4j.GatewayConnection.run(GatewayConnection.java:209)\njava.lang.Thread.run(Thread.java:745)","Submission Time":1467147289326,"Completion Time":1467147396663,"Failure Reason":"Job 2 cancelled as part of cancellation of all jobs","Accumulables":[]}}
{"Event":"SparkListenerJobEnd","Job ID":2,"Completion Time":1467147397125,"Job Result":{"Result":"JobFailed","Exception":{"Message":"Job 2 cancelled as part of cancellation of all jobs","Stack Trace":[{"Declaring Class":"org.apache.spark.scheduler.DAGScheduler","Method Name":"org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages","File Name":"DAGScheduler.scala","Line Number":1431},{"Declaring Class":"org.apache.spark.scheduler.DAGScheduler","Method Name":"handleJobCancellation","File Name":"DAGScheduler.scala","Line Number":1370},{"Declaring Class":"org.apache.spark.scheduler.DAGScheduler$$anonfun$doCancelAllJobs$1","Method Name":"apply$mcVI$sp","File Name":"DAGScheduler.scala","Line Number":713},{"Declaring Class":"org.apache.spark.scheduler.DAGScheduler$$anonfun$doCancelAllJobs$1","Method Name":"apply","File Name":"DAGScheduler.scala","Line Number":713},{"Declaring Class":"org.apache.spark.scheduler.DAGScheduler$$anonfun$doCancelAllJobs$1","Method Name":"apply","File Name":"DAGScheduler.scala","Line Number":713},{"Declaring Class":"scala.collection.mutable.HashSet","Method Name":"foreach","File Name":"HashSet.scala","Line Number":79},{"Declaring Class":"org.apache.spark.scheduler.DAGScheduler","Method Name":"doCancelAllJobs","File Name":"DAGScheduler.scala","Line Number":713},{"Declaring Class":"org.apache.spark.scheduler.DAGSchedulerEventProcessLoop","Method Name":"doOnReceive","File Name":"DAGScheduler.scala","Line Number":1622},{"Declaring Class":"org.apache.spark.scheduler.DAGSchedulerEventProcessLoop","Method Name":"onReceive","File Name":"DAGScheduler.scala","Line Number":1599},{"Declaring Class":"org.apache.spark.scheduler.DAGSchedulerEventProcessLoop","Method Name":"onReceive","File Name":"DAGScheduler.scala","Line Number":1588},{"Declaring Class":"org.apache.spark.util.EventLoop$$anon$1","Method Name":"run","File Name":"EventLoop.scala","Line Number":48},{"Declaring Class":"org.apache.spark.scheduler.DAGScheduler","Method Name":"runJob","File Name":"DAGScheduler.scala","Line Number":620},{"Declaring Class":"org.apache.spark.SparkContext","Method Name":"runJob","File Name":"SparkContext.scala","Line Number":1832},{"Declaring Class":"org.apache.spark.SparkContext","Method Name":"runJob","File Name":"SparkContext.scala","Line Number":1845},{"Declaring Class":"org.apache.spark.SparkContext","Method Name":"runJob","File Name":"SparkContext.scala","Line Number":1922},{"Declaring Class":"org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1","Method Name":"apply$mcV$sp","File Name":"PairRDDFunctions.scala","Line Number":1213},{"Declaring Class":"org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1","Method Name":"apply","File Name":"PairRDDFunctions.scala","Line Number":1156},{"Declaring Class":"org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1","Method Name":"apply","File Name":"PairRDDFunctions.scala","Line Number":1156},{"Declaring Class":"org.apache.spark.rdd.RDDOperationScope$","Method Name":"withScope","File Name":"RDDOperationScope.scala","Line Number":150},{"Declaring Class":"org.apache.spark.rdd.RDDOperationScope$","Method Name":"withScope","File Name":"RDDOperationScope.scala","Line Number":111},{"Declaring Class":"org.apache.spark.rdd.RDD","Method Name":"withScope","File Name":"RDD.scala","Line Number":316},{"Declaring Class":"org.apache.spark.rdd.PairRDDFunctions","Method Name":"saveAsHadoopDataset","File Name":"PairRDDFunctions.scala","Line Number":1156},{"Declaring Class":"org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$4","Method Name":"apply$mcV$sp","File Name":"PairRDDFunctions.scala","Line Number":1060},{"Declaring Class":"org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$4","Method Name":"apply","File Name":"PairRDDFunctions.scala","Line Number":1026},{"Declaring Class":"org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$4","Method Name":"apply","File Name":"PairRDDFunctions.scala","Line Number":1026},{"Declaring Class":"org.apache.spark.rdd.RDDOperationScope$","Method Name":"withScope","File Name":"RDDOperationScope.scala","Line Number":150},{"Declaring Class":"org.apache.spark.rdd.RDDOperationScope$","Method Name":"withScope","File Name":"RDDOperationScope.scala","Line Number":111},{"Declaring Class":"org.apache.spark.rdd.RDD","Method Name":"withScope","File Name":"RDD.scala","Line Number":316},{"Declaring Class":"org.apache.spark.rdd.PairRDDFunctions","Method Name":"saveAsHadoopFile","File Name":"PairRDDFunctions.scala","Line Number":1026},{"Declaring Class":"org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$1","Method Name":"apply$mcV$sp","File Name":"PairRDDFunctions.scala","Line Number":952},{"Declaring Class":"org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$1","Method Name":"apply","File Name":"PairRDDFunctions.scala","Line Number":952},{"Declaring Class":"org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$1","Method Name":"apply","File Name":"PairRDDFunctions.scala","Line Number":952},{"Declaring Class":"org.apache.spark.rdd.RDDOperationScope$","Method Name":"withScope","File Name":"RDDOperationScope.scala","Line Number":150},{"Declaring Class":"org.apache.spark.rdd.RDDOperationScope$","Method Name":"withScope","File Name":"RDDOperationScope.scala","Line Number":111},{"Declaring Class":"org.apache.spark.rdd.RDD","Method Name":"withScope","File Name":"RDD.scala","Line Number":316},{"Declaring Class":"org.apache.spark.rdd.PairRDDFunctions","Method Name":"saveAsHadoopFile","File Name":"PairRDDFunctions.scala","Line Number":951},{"Declaring Class":"org.apache.spark.rdd.RDD$$anonfun$saveAsTextFile$1","Method Name":"apply$mcV$sp","File Name":"RDD.scala","Line Number":1457},{"Declaring Class":"org.apache.spark.rdd.RDD$$anonfun$saveAsTextFile$1","Method Name":"apply","File Name":"RDD.scala","Line Number":1436},{"Declaring Class":"org.apache.spark.rdd.RDD$$anonfun$saveAsTextFile$1","Method Name":"apply","File Name":"RDD.scala","Line Number":1436},{"Declaring Class":"org.apache.spark.rdd.RDDOperationScope$","Method Name":"withScope","File Name":"RDDOperationScope.scala","Line Number":150},{"Declaring Class":"org.apache.spark.rdd.RDDOperationScope$","Method Name":"withScope","File Name":"RDDOperationScope.scala","Line Number":111},{"Declaring Class":"org.apache.spark.rdd.RDD","Method Name":"withScope","File Name":"RDD.scala","Line Number":316},{"Declaring Class":"org.apache.spark.rdd.RDD","Method Name":"saveAsTextFile","File Name":"RDD.scala","Line Number":1436},{"Declaring Class":"org.apache.spark.api.java.JavaRDDLike$class","Method Name":"saveAsTextFile","File Name":"JavaRDDLike.scala","Line Number":507},{"Declaring Class":"org.apache.spark.api.java.AbstractJavaRDDLike","Method Name":"saveAsTextFile","File Name":"JavaRDDLike.scala","Line Number":46},{"Declaring Class":"sun.reflect.NativeMethodAccessorImpl","Method Name":"invoke0","File Name":"NativeMethodAccessorImpl.java","Line Number":-2},{"Declaring Class":"sun.reflect.NativeMethodAccessorImpl","Method Name":"invoke","File Name":"NativeMethodAccessorImpl.java","Line Number":62},{"Declaring Class":"sun.reflect.DelegatingMethodAccessorImpl","Method Name":"invoke","File Name":"DelegatingMethodAccessorImpl.java","Line Number":43},{"Declaring Class":"java.lang.reflect.Method","Method Name":"invoke","File Name":"Method.java","Line Number":498},{"Declaring Class":"py4j.reflection.MethodInvoker","Method Name":"invoke","File Name":"MethodInvoker.java","Line Number":231},{"Declaring Class":"py4j.reflection.ReflectionEngine","Method Name":"invoke","File Name":"ReflectionEngine.java","Line Number":381},{"Declaring Class":"py4j.Gateway","Method Name":"invoke","File Name":"Gateway.java","Line Number":259},{"Declaring Class":"py4j.commands.AbstractCommand","Method Name":"invokeMethod","File Name":"AbstractCommand.java","Line Number":133},{"Declaring Class":"py4j.commands.CallCommand","Method Name":"execute","File Name":"CallCommand.java","Line Number":79},{"Declaring Class":"py4j.GatewayConnection","Method Name":"run","File Name":"GatewayConnection.java","Line Number":209},{"Declaring Class":"java.lang.Thread","Method Name":"run","File Name":"Thread.java","Line Number":745}]}}}
